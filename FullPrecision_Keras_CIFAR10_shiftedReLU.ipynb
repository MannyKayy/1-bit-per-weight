{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow.keras implementation of full precision shifted ReLU CNN for CIFAR 10\n",
    "##  https://arxiv.org/abs/1907.06916\n",
    "## Mark D. McDonnell, Hesham Mostafa, Runchun Wang, Andre van Schaik,\n",
    "## Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  1.13.1\n"
     ]
    }
   ],
   "source": [
    "# select a GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import savemat,loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import tensorflow\n",
    "print('Tensorflow version = ',tensorflow.__version__)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, History\n",
    "\n",
    "#from tensorflow.keras import backend as K\n",
    "\n",
    "from ResNetModel import resnet_srelu\n",
    "from Utils import cutout,LR_WarmRestart,GetDataGen,plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#params\n",
    "WhichDataSet = 'CIFAR10'\n",
    "#WhichDataSet = 'CIFAR100'\n",
    "init_lr = 0.1\n",
    "epochs = 300\n",
    "batch_size = 125\n",
    "My_wd=5e-4/2\n",
    "resnet_width = 10\n",
    "resnet_depth = 20\n",
    "UseBinary=False\n",
    "UseCutout=True\n",
    "Loss = 'categorical_crossentropy'\n",
    "Optimizer = SGD(lr=init_lr,decay=0.0, momentum=0.9, nesterov=False)\n",
    "Metrics = ['accuracy']\n",
    "ModelsPath = 'TrainedModels/Tensorflow.keras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare data\n",
    "if WhichDataSet == 'CIFAR10':\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()\n",
    "else:\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar100.load_data()\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "K_train = x_train.shape[0]\n",
    "input_shape = x_train.shape[1:]\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')#/255.0\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_entropy_logits_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        return tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 3)    12          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 160)  4320        batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 32, 32, 160)  0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 32, 32, 160)  0           lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 32, 32, 160)  0           re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 160)  230400      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 32, 32, 160)  0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 32, 32, 160)  0           lambda_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 32, 32, 160)  0           re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 160)  230400      lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 160)  0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 32, 32, 160)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 32, 32, 160)  0           lambda_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 32, 32, 160)  0           re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 160)  230400      lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 32, 32, 160)  0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 32, 32, 160)  0           lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 32, 32, 160)  0           re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 160)  230400      lambda_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 160)  0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 32, 32, 160)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 32, 32, 160)  0           lambda_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 32, 32, 160)  0           re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 160)  230400      lambda_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 32, 32, 160)  0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 32, 32, 160)  0           lambda_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 32, 32, 160)  0           re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 160)  230400      lambda_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 160)  0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 32, 32, 160)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 32, 32, 160)  0           lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 32, 32, 160)  0           re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 320)  460800      lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 16, 16, 320)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 16, 16, 320)  0           lambda_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 160)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 16, 16, 320)  0           re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 16, 16, 160)  0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 320)  921600      lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 320)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 320)  0           conv2d_28[0][0]                  \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 16, 16, 320)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 16, 16, 320)  0           lambda_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 16, 16, 320)  0           re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 320)  921600      lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 16, 16, 320)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 16, 16, 320)  0           lambda_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 16, 16, 320)  0           re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 320)  921600      lambda_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 320)  0           conv2d_30[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 16, 16, 320)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 16, 16, 320)  0           lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 16, 16, 320)  0           re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 320)  921600      lambda_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 16, 16, 320)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 16, 16, 320)  0           lambda_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 16, 16, 320)  0           re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 320)  921600      lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 320)  0           conv2d_32[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 16, 16, 320)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 16, 16, 320)  0           lambda_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 16, 16, 320)  0           re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 640)    1843200     lambda_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 8, 8, 640)    0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 8, 8, 640)    0           lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 8, 8, 320)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 8, 8, 640)    0           re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 8, 8, 320)    0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 640)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 lambda_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 640)    0           conv2d_34[0][0]                  \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 8, 8, 640)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 8, 8, 640)    0           lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 8, 8, 640)    0           re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 8, 8, 640)    0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 8, 8, 640)    0           lambda_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 8, 8, 640)    0           re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 640)    0           conv2d_36[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 8, 8, 640)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 8, 8, 640)    0           lambda_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 8, 8, 640)    0           re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 8, 8, 640)    0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 8, 8, 640)    0           lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 8, 8, 640)    0           re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 640)    0           conv2d_38[0][0]                  \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 8, 8, 640)    0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 8, 8, 640)    0           lambda_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 8, 8, 640)    0           re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 10)     6400        lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 8, 8, 10)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 10)           0           lambda_81[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,737,132\n",
      "Trainable params: 26,737,126\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define a datagen or generating training samples with flip and pad/crop augmentation, and if set to True, with cutout augmentation\n",
    "dataGenerator = GetDataGen(UseCutout)\n",
    "\n",
    "#define and compile the model\n",
    "Temperature=80.0\n",
    "model = resnet_srelu(Temperature,UseBinary,input_shape=input_shape, depth=resnet_depth, num_classes=num_classes,\n",
    "                     wd=My_wd,width=resnet_width)\n",
    "model.compile(loss=catcross_entropy_logits_loss() ,optimizer = Optimizer, metrics = Metrics)\n",
    "\n",
    "#print  the model\n",
    "model.summary()\n",
    "\n",
    "#define the learnng rate schedule\n",
    "steps_per_epoch = int(np.floor(K_train / batch_size))\n",
    "lr_scheduler = LR_WarmRestart(nbatch=steps_per_epoch,\n",
    "                              initial_lr=init_lr, min_lr=init_lr*1e-4,\n",
    "                              epochs_restart = [],\n",
    "                              Tmult=300.0) \n",
    "\n",
    "#define callbacks\n",
    "history = History()\n",
    "callbacks = [lr_scheduler,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 4.0340 - acc: 0.3872\n",
      "\n",
      " End of Epoch Learning Rate = 0.099997\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 4.9153 - acc: 0.2461 - val_loss: 4.0340 - val_acc: 0.3872\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099997\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.9530 - acc: 0.5267\n",
      "\n",
      " End of Epoch Learning Rate = 0.099989\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 3.5469 - acc: 0.4304 - val_loss: 2.9530 - val_acc: 0.5267\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099989\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 2.1806 - acc: 0.6481\n",
      "\n",
      " End of Epoch Learning Rate = 0.099975\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 2.6641 - acc: 0.5507 - val_loss: 2.1806 - val_acc: 0.6481\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099975\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 1.7955 - acc: 0.6960\n",
      "\n",
      " End of Epoch Learning Rate = 0.099956\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 2.0646 - acc: 0.6366 - val_loss: 1.7955 - val_acc: 0.6960\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099956\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.5418 - acc: 0.7390\n",
      "\n",
      " End of Epoch Learning Rate = 0.099931\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 1.6721 - acc: 0.6982 - val_loss: 1.5418 - val_acc: 0.7390\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099931\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 1.2470 - acc: 0.7765\n",
      "\n",
      " End of Epoch Learning Rate = 0.099901\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 1.4245 - acc: 0.7332 - val_loss: 1.2470 - val_acc: 0.7765\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099901\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 1.0464 - acc: 0.8197\n",
      "\n",
      " End of Epoch Learning Rate = 0.099866\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 1.2588 - acc: 0.7587 - val_loss: 1.0464 - val_acc: 0.8197\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099866\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.9999 - acc: 0.8252\n",
      "\n",
      " End of Epoch Learning Rate = 0.099825\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 1.1351 - acc: 0.7813 - val_loss: 0.9999 - val_acc: 0.8252\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099825\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.9472 - acc: 0.8370\n",
      "\n",
      " End of Epoch Learning Rate = 0.099778\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 1.0779 - acc: 0.7919 - val_loss: 0.9472 - val_acc: 0.8370\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099778\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 0.8806 - acc: 0.8574\n",
      "\n",
      " End of Epoch Learning Rate = 0.099726\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 1.0252 - acc: 0.8033 - val_loss: 0.8806 - val_acc: 0.8574\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099726\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.8623 - acc: 0.8580\n",
      "\n",
      " End of Epoch Learning Rate = 0.099669\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.9870 - acc: 0.8126 - val_loss: 0.8623 - val_acc: 0.8580\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099669\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.8467 - acc: 0.8646\n",
      "\n",
      " End of Epoch Learning Rate = 0.099606\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.9525 - acc: 0.8216 - val_loss: 0.8467 - val_acc: 0.8646\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099606\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.8720 - acc: 0.8568\n",
      "\n",
      " End of Epoch Learning Rate = 0.099537\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.9425 - acc: 0.8271 - val_loss: 0.8720 - val_acc: 0.8568\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099537\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.8847 - acc: 0.8537\n",
      "\n",
      " End of Epoch Learning Rate = 0.099464\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.9219 - acc: 0.8358 - val_loss: 0.8847 - val_acc: 0.8537\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099464\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.8429 - acc: 0.8644\n",
      "\n",
      " End of Epoch Learning Rate = 0.099384\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.9128 - acc: 0.8381 - val_loss: 0.8429 - val_acc: 0.8644\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099384\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.8362 - acc: 0.8653\n",
      "\n",
      " End of Epoch Learning Rate = 0.099300\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.9034 - acc: 0.8387 - val_loss: 0.8362 - val_acc: 0.8653\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099300\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.8163 - acc: 0.8833\n",
      "\n",
      " End of Epoch Learning Rate = 0.099210\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.9201 - acc: 0.8398 - val_loss: 0.8163 - val_acc: 0.8833\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099210\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.8236 - acc: 0.8804\n",
      "\n",
      " End of Epoch Learning Rate = 0.099114\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.9008 - acc: 0.8466 - val_loss: 0.8236 - val_acc: 0.8804\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099114\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.8299 - acc: 0.8753\n",
      "\n",
      " End of Epoch Learning Rate = 0.099014\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8912 - acc: 0.8516 - val_loss: 0.8299 - val_acc: 0.8753\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099014\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 0.9128 - acc: 0.8540\n",
      "\n",
      " End of Epoch Learning Rate = 0.098907\n",
      "400/400 [==============================] - 115s 289ms/step - loss: 0.8826 - acc: 0.8563 - val_loss: 0.9128 - val_acc: 0.8540\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098907\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.8005 - acc: 0.8898\n",
      "\n",
      " End of Epoch Learning Rate = 0.098796\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8907 - acc: 0.8544 - val_loss: 0.8005 - val_acc: 0.8898\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098796\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.8391 - acc: 0.8762\n",
      "\n",
      " End of Epoch Learning Rate = 0.098679\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8933 - acc: 0.8534 - val_loss: 0.8391 - val_acc: 0.8762\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098679\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.8284 - acc: 0.8796\n",
      "\n",
      " End of Epoch Learning Rate = 0.098557\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8838 - acc: 0.8583 - val_loss: 0.8284 - val_acc: 0.8796\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098557\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 0.8237 - acc: 0.8819\n",
      "\n",
      " End of Epoch Learning Rate = 0.098429\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8797 - acc: 0.8599 - val_loss: 0.8237 - val_acc: 0.8819\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098429\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.8821 - acc: 0.8707\n",
      "\n",
      " End of Epoch Learning Rate = 0.098296\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.8768 - acc: 0.8611 - val_loss: 0.8821 - val_acc: 0.8707\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098296\n",
      "Epoch 26/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.8569 - acc: 0.8749\n",
      "\n",
      " End of Epoch Learning Rate = 0.098158\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8887 - acc: 0.8606 - val_loss: 0.8569 - val_acc: 0.8749\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098158\n",
      "Epoch 27/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.8353 - acc: 0.8842\n",
      "\n",
      " End of Epoch Learning Rate = 0.098015\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.8792 - acc: 0.8635 - val_loss: 0.8353 - val_acc: 0.8842\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098015\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.8041 - acc: 0.8912\n",
      "\n",
      " End of Epoch Learning Rate = 0.097866\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8822 - acc: 0.8637 - val_loss: 0.8041 - val_acc: 0.8912\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097866\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8596 - acc: 0.8757\n",
      "\n",
      " End of Epoch Learning Rate = 0.097712\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8893 - acc: 0.8612 - val_loss: 0.8596 - val_acc: 0.8757\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097712\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.8144 - acc: 0.8910\n",
      "\n",
      " End of Epoch Learning Rate = 0.097553\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.8767 - acc: 0.8688 - val_loss: 0.8144 - val_acc: 0.8910\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097553\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 0.8601 - acc: 0.8841\n",
      "\n",
      " End of Epoch Learning Rate = 0.097389\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8733 - acc: 0.8691 - val_loss: 0.8601 - val_acc: 0.8841\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097389\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 0.8457 - acc: 0.8835\n",
      "\n",
      " End of Epoch Learning Rate = 0.097219\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.8816 - acc: 0.8676 - val_loss: 0.8457 - val_acc: 0.8835\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097219\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.9115 - acc: 0.8699\n",
      "\n",
      " End of Epoch Learning Rate = 0.097044\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8754 - acc: 0.8696 - val_loss: 0.9115 - val_acc: 0.8699\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097044\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.8200 - acc: 0.8931\n",
      "\n",
      " End of Epoch Learning Rate = 0.096864\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8775 - acc: 0.8685 - val_loss: 0.8200 - val_acc: 0.8931\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096864\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.8592 - acc: 0.8842\n",
      "\n",
      " End of Epoch Learning Rate = 0.096679\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8673 - acc: 0.8722 - val_loss: 0.8592 - val_acc: 0.8842\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096679\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.8224 - acc: 0.8895\n",
      "\n",
      " End of Epoch Learning Rate = 0.096489\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8785 - acc: 0.8687 - val_loss: 0.8224 - val_acc: 0.8895\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096489\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.8128 - acc: 0.8974\n",
      "\n",
      " End of Epoch Learning Rate = 0.096294\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8843 - acc: 0.8699 - val_loss: 0.8128 - val_acc: 0.8974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096294\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.7997 - acc: 0.9011\n",
      "\n",
      " End of Epoch Learning Rate = 0.096094\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8607 - acc: 0.8764 - val_loss: 0.7997 - val_acc: 0.9011\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096094\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.8715 - acc: 0.8821\n",
      "\n",
      " End of Epoch Learning Rate = 0.095888\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8645 - acc: 0.8736 - val_loss: 0.8715 - val_acc: 0.8821\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095888\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.8251 - acc: 0.8965\n",
      "\n",
      " End of Epoch Learning Rate = 0.095678\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8634 - acc: 0.8759 - val_loss: 0.8251 - val_acc: 0.8965\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095678\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.7920 - acc: 0.9062\n",
      "\n",
      " End of Epoch Learning Rate = 0.095462\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8651 - acc: 0.8747 - val_loss: 0.7920 - val_acc: 0.9062\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095462\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7912 - acc: 0.9045\n",
      "\n",
      " End of Epoch Learning Rate = 0.095242\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.8614 - acc: 0.8763 - val_loss: 0.7912 - val_acc: 0.9045\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095242\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.8919 - acc: 0.8819\n",
      "\n",
      " End of Epoch Learning Rate = 0.095016\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8624 - acc: 0.8781 - val_loss: 0.8919 - val_acc: 0.8819\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095016\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.8642 - acc: 0.8855\n",
      "\n",
      " End of Epoch Learning Rate = 0.094786\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8640 - acc: 0.8760 - val_loss: 0.8642 - val_acc: 0.8855\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094786\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 0.8563 - acc: 0.8896\n",
      "\n",
      " End of Epoch Learning Rate = 0.094551\n",
      "400/400 [==============================] - 115s 289ms/step - loss: 0.8647 - acc: 0.8767 - val_loss: 0.8563 - val_acc: 0.8896\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094551\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.8063 - acc: 0.9038\n",
      "\n",
      " End of Epoch Learning Rate = 0.094311\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.8596 - acc: 0.8780 - val_loss: 0.8063 - val_acc: 0.9038\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094311\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 7s 723us/sample - loss: 0.8600 - acc: 0.8841\n",
      "\n",
      " End of Epoch Learning Rate = 0.094066\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8498 - acc: 0.8805 - val_loss: 0.8600 - val_acc: 0.8841\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094066\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.8208 - acc: 0.8927\n",
      "\n",
      " End of Epoch Learning Rate = 0.093816\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.8586 - acc: 0.8785 - val_loss: 0.8208 - val_acc: 0.8927\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093816\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.8081 - acc: 0.9019\n",
      "\n",
      " End of Epoch Learning Rate = 0.093561\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8636 - acc: 0.8758 - val_loss: 0.8081 - val_acc: 0.9019\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093561\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.8492 - acc: 0.8906\n",
      "\n",
      " End of Epoch Learning Rate = 0.093302\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8551 - acc: 0.8793 - val_loss: 0.8492 - val_acc: 0.8906\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093302\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.7775 - acc: 0.9078\n",
      "\n",
      " End of Epoch Learning Rate = 0.093038\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8536 - acc: 0.8799 - val_loss: 0.7775 - val_acc: 0.9078\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093038\n",
      "Epoch 52/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.7763 - acc: 0.9112\n",
      "\n",
      " End of Epoch Learning Rate = 0.092769\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8572 - acc: 0.8800 - val_loss: 0.7763 - val_acc: 0.9112\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092769\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 717us/sample - loss: 0.7937 - acc: 0.9049\n",
      "\n",
      " End of Epoch Learning Rate = 0.092495\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8566 - acc: 0.8797 - val_loss: 0.7937 - val_acc: 0.9049\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092495\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.8261 - acc: 0.9000\n",
      "\n",
      " End of Epoch Learning Rate = 0.092217\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8429 - acc: 0.8831 - val_loss: 0.8261 - val_acc: 0.9000\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092217\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.8204 - acc: 0.8981\n",
      "\n",
      " End of Epoch Learning Rate = 0.091934\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8372 - acc: 0.8850 - val_loss: 0.8204 - val_acc: 0.8981\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091934\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.7841 - acc: 0.9061\n",
      "\n",
      " End of Epoch Learning Rate = 0.091647\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8399 - acc: 0.8829 - val_loss: 0.7841 - val_acc: 0.9061\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091647\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.7946 - acc: 0.9037\n",
      "\n",
      " End of Epoch Learning Rate = 0.091355\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8431 - acc: 0.8821 - val_loss: 0.7946 - val_acc: 0.9037\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091355\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.7940 - acc: 0.9059\n",
      "\n",
      " End of Epoch Learning Rate = 0.091058\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8374 - acc: 0.8834 - val_loss: 0.7940 - val_acc: 0.9059\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091058\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.8567 - acc: 0.8919\n",
      "\n",
      " End of Epoch Learning Rate = 0.090757\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.8371 - acc: 0.8841 - val_loss: 0.8567 - val_acc: 0.8919\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090757\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 7s 723us/sample - loss: 0.8111 - acc: 0.8956\n",
      "\n",
      " End of Epoch Learning Rate = 0.090452\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8260 - acc: 0.8874 - val_loss: 0.8111 - val_acc: 0.8956\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090452\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 0.8176 - acc: 0.8925\n",
      "\n",
      " End of Epoch Learning Rate = 0.090142\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8327 - acc: 0.8851 - val_loss: 0.8176 - val_acc: 0.8925\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090142\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 0.8128 - acc: 0.8956\n",
      "\n",
      " End of Epoch Learning Rate = 0.089828\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8275 - acc: 0.8862 - val_loss: 0.8128 - val_acc: 0.8956\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089828\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.8089 - acc: 0.8999\n",
      "\n",
      " End of Epoch Learning Rate = 0.089509\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.8266 - acc: 0.8854 - val_loss: 0.8089 - val_acc: 0.8999\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089509\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7882 - acc: 0.9086\n",
      "\n",
      " End of Epoch Learning Rate = 0.089186\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.8224 - acc: 0.8883 - val_loss: 0.7882 - val_acc: 0.9086\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089186\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.7809 - acc: 0.9049\n",
      "\n",
      " End of Epoch Learning Rate = 0.088858\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.8258 - acc: 0.8873 - val_loss: 0.7809 - val_acc: 0.9049\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088858\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 0.7820 - acc: 0.9064\n",
      "\n",
      " End of Epoch Learning Rate = 0.088527\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8172 - acc: 0.8891 - val_loss: 0.7820 - val_acc: 0.9064\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088527\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.8418 - acc: 0.8900\n",
      "\n",
      " End of Epoch Learning Rate = 0.088191\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8231 - acc: 0.8879 - val_loss: 0.8418 - val_acc: 0.8900\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088191\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 0.7997 - acc: 0.8995\n",
      "\n",
      " End of Epoch Learning Rate = 0.087851\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.8143 - acc: 0.8913 - val_loss: 0.7997 - val_acc: 0.8995\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087851\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.7909 - acc: 0.9047\n",
      "\n",
      " End of Epoch Learning Rate = 0.087507\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8145 - acc: 0.8905 - val_loss: 0.7909 - val_acc: 0.9047\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087507\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.8395 - acc: 0.8926\n",
      "\n",
      " End of Epoch Learning Rate = 0.087159\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8135 - acc: 0.8907 - val_loss: 0.8395 - val_acc: 0.8926\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087159\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 7s 723us/sample - loss: 0.8686 - acc: 0.8832\n",
      "\n",
      " End of Epoch Learning Rate = 0.086806\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8147 - acc: 0.8886 - val_loss: 0.8686 - val_acc: 0.8832\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086806\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.8254 - acc: 0.8881\n",
      "\n",
      " End of Epoch Learning Rate = 0.086450\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8143 - acc: 0.8898 - val_loss: 0.8254 - val_acc: 0.8881\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086450\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 0.7816 - acc: 0.9051\n",
      "\n",
      " End of Epoch Learning Rate = 0.086089\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8037 - acc: 0.8944 - val_loss: 0.7816 - val_acc: 0.9051\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086089\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 0.7510 - acc: 0.9148\n",
      "\n",
      " End of Epoch Learning Rate = 0.085725\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8082 - acc: 0.8892 - val_loss: 0.7510 - val_acc: 0.9148\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085725\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.7697 - acc: 0.9071\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8067 - acc: 0.8917 - val_loss: 0.7697 - val_acc: 0.9071\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.7603 - acc: 0.9104\n",
      "\n",
      " End of Epoch Learning Rate = 0.084985\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.8011 - acc: 0.8926 - val_loss: 0.7603 - val_acc: 0.9104\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084985\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.7933 - acc: 0.8993\n",
      "\n",
      " End of Epoch Learning Rate = 0.084609\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8074 - acc: 0.8907 - val_loss: 0.7933 - val_acc: 0.8993\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084609\n",
      "Epoch 78/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 0.8249 - acc: 0.8940\n",
      "\n",
      " End of Epoch Learning Rate = 0.084229\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7955 - acc: 0.8946 - val_loss: 0.8249 - val_acc: 0.8940\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084229\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.7818 - acc: 0.9022\n",
      "\n",
      " End of Epoch Learning Rate = 0.083845\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.8056 - acc: 0.8906 - val_loss: 0.7818 - val_acc: 0.9022\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083845\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 0.7479 - acc: 0.9182\n",
      "\n",
      " End of Epoch Learning Rate = 0.083458\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7973 - acc: 0.8958 - val_loss: 0.7479 - val_acc: 0.9182\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083458\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 0.7751 - acc: 0.9045\n",
      "\n",
      " End of Epoch Learning Rate = 0.083067\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7822 - acc: 0.8984 - val_loss: 0.7751 - val_acc: 0.9045\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083067\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.7625 - acc: 0.9110\n",
      "\n",
      " End of Epoch Learning Rate = 0.082673\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7899 - acc: 0.8947 - val_loss: 0.7625 - val_acc: 0.9110\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082673\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.7522 - acc: 0.9134\n",
      "\n",
      " End of Epoch Learning Rate = 0.082275\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7868 - acc: 0.8959 - val_loss: 0.7522 - val_acc: 0.9134\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082275\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.7766 - acc: 0.9070\n",
      "\n",
      " End of Epoch Learning Rate = 0.081873\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.7898 - acc: 0.8945 - val_loss: 0.7766 - val_acc: 0.9070\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081873\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.7962 - acc: 0.8990\n",
      "\n",
      " End of Epoch Learning Rate = 0.081468\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7680 - acc: 0.9012 - val_loss: 0.7962 - val_acc: 0.8990\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081468\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.7629 - acc: 0.9049\n",
      "\n",
      " End of Epoch Learning Rate = 0.081059\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7792 - acc: 0.8965 - val_loss: 0.7629 - val_acc: 0.9049\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081059\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7700 - acc: 0.9066\n",
      "\n",
      " End of Epoch Learning Rate = 0.080647\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7781 - acc: 0.8967 - val_loss: 0.7700 - val_acc: 0.9066\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080647\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.7601 - acc: 0.9090\n",
      "\n",
      " End of Epoch Learning Rate = 0.080232\n",
      "400/400 [==============================] - 115s 289ms/step - loss: 0.7765 - acc: 0.8972 - val_loss: 0.7601 - val_acc: 0.9090\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080232\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.7511 - acc: 0.9104\n",
      "\n",
      " End of Epoch Learning Rate = 0.079813\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.7774 - acc: 0.8962 - val_loss: 0.7511 - val_acc: 0.9104\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079813\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8090 - acc: 0.8967\n",
      "\n",
      " End of Epoch Learning Rate = 0.079391\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7786 - acc: 0.8967 - val_loss: 0.8090 - val_acc: 0.8967\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079391\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.7705 - acc: 0.9075\n",
      "\n",
      " End of Epoch Learning Rate = 0.078966\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.7666 - acc: 0.9001 - val_loss: 0.7705 - val_acc: 0.9075\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078966\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7692 - acc: 0.9032\n",
      "\n",
      " End of Epoch Learning Rate = 0.078538\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7632 - acc: 0.9003 - val_loss: 0.7692 - val_acc: 0.9032\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078538\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8163 - acc: 0.8942\n",
      "\n",
      " End of Epoch Learning Rate = 0.078106\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7680 - acc: 0.8991 - val_loss: 0.8163 - val_acc: 0.8942\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078106\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.7409 - acc: 0.9123\n",
      "\n",
      " End of Epoch Learning Rate = 0.077672\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7638 - acc: 0.9000 - val_loss: 0.7409 - val_acc: 0.9123\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077672\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 0.7526 - acc: 0.9100\n",
      "\n",
      " End of Epoch Learning Rate = 0.077234\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7669 - acc: 0.8993 - val_loss: 0.7526 - val_acc: 0.9100\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077234\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 0.7572 - acc: 0.9068\n",
      "\n",
      " End of Epoch Learning Rate = 0.076794\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.7633 - acc: 0.9004 - val_loss: 0.7572 - val_acc: 0.9068\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076794\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 0.7333 - acc: 0.9155\n",
      "\n",
      " End of Epoch Learning Rate = 0.076350\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7545 - acc: 0.9023 - val_loss: 0.7333 - val_acc: 0.9155\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076350\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.7800 - acc: 0.9015\n",
      "\n",
      " End of Epoch Learning Rate = 0.075904\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7489 - acc: 0.9022 - val_loss: 0.7800 - val_acc: 0.9015\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075904\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8017 - acc: 0.8966\n",
      "\n",
      " End of Epoch Learning Rate = 0.075455\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7425 - acc: 0.9046 - val_loss: 0.8017 - val_acc: 0.8966\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075455\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7305 - acc: 0.9120\n",
      "\n",
      " End of Epoch Learning Rate = 0.075002\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.7501 - acc: 0.9010 - val_loss: 0.7305 - val_acc: 0.9120\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075002\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.7484 - acc: 0.9101\n",
      "\n",
      " End of Epoch Learning Rate = 0.074548\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7489 - acc: 0.9027 - val_loss: 0.7484 - val_acc: 0.9101\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074548\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.7600 - acc: 0.9047\n",
      "\n",
      " End of Epoch Learning Rate = 0.074090\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7355 - acc: 0.9062 - val_loss: 0.7600 - val_acc: 0.9047\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074090\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.7303 - acc: 0.9134\n",
      "\n",
      " End of Epoch Learning Rate = 0.073630\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7361 - acc: 0.9061 - val_loss: 0.7303 - val_acc: 0.9134\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073630\n",
      "Epoch 104/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 0.7562 - acc: 0.9056\n",
      "\n",
      " End of Epoch Learning Rate = 0.073167\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.7435 - acc: 0.9019 - val_loss: 0.7562 - val_acc: 0.9056\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073167\n",
      "Epoch 105/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.7020 - acc: 0.9230\n",
      "\n",
      " End of Epoch Learning Rate = 0.072702\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.7333 - acc: 0.9060 - val_loss: 0.7020 - val_acc: 0.9230\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072702\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.7053 - acc: 0.9223\n",
      "\n",
      " End of Epoch Learning Rate = 0.072235\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.7361 - acc: 0.9049 - val_loss: 0.7053 - val_acc: 0.9223\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072235\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.7237 - acc: 0.9145\n",
      "\n",
      " End of Epoch Learning Rate = 0.071764\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.7320 - acc: 0.9074 - val_loss: 0.7237 - val_acc: 0.9145\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071764\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 0.6999 - acc: 0.9197\n",
      "\n",
      " End of Epoch Learning Rate = 0.071292\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.7323 - acc: 0.9064 - val_loss: 0.6999 - val_acc: 0.9197\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071292\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.7335 - acc: 0.9094\n",
      "\n",
      " End of Epoch Learning Rate = 0.070817\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7286 - acc: 0.9065 - val_loss: 0.7335 - val_acc: 0.9094\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070817\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.7696 - acc: 0.9028\n",
      "\n",
      " End of Epoch Learning Rate = 0.070340\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.7215 - acc: 0.9081 - val_loss: 0.7696 - val_acc: 0.9028\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070340\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.6853 - acc: 0.9250\n",
      "\n",
      " End of Epoch Learning Rate = 0.069860\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7191 - acc: 0.9092 - val_loss: 0.6853 - val_acc: 0.9250\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069860\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 0.7465 - acc: 0.9060\n",
      "\n",
      " End of Epoch Learning Rate = 0.069379\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7224 - acc: 0.9062 - val_loss: 0.7465 - val_acc: 0.9060\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069379\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7031 - acc: 0.9218\n",
      "\n",
      " End of Epoch Learning Rate = 0.068895\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7236 - acc: 0.9090 - val_loss: 0.7031 - val_acc: 0.9218\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068895\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.7066 - acc: 0.9206\n",
      "\n",
      " End of Epoch Learning Rate = 0.068409\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7079 - acc: 0.9109 - val_loss: 0.7066 - val_acc: 0.9206\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068409\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.6798 - acc: 0.9239\n",
      "\n",
      " End of Epoch Learning Rate = 0.067922\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7039 - acc: 0.9112 - val_loss: 0.6798 - val_acc: 0.9239\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067922\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.6830 - acc: 0.9240\n",
      "\n",
      " End of Epoch Learning Rate = 0.067432\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.7046 - acc: 0.9109 - val_loss: 0.6830 - val_acc: 0.9240\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067432\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 0.6864 - acc: 0.9249\n",
      "\n",
      " End of Epoch Learning Rate = 0.066940\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.6931 - acc: 0.9139 - val_loss: 0.6864 - val_acc: 0.9249\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066940\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.7440 - acc: 0.9104\n",
      "\n",
      " End of Epoch Learning Rate = 0.066447\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6969 - acc: 0.9129 - val_loss: 0.7440 - val_acc: 0.9104\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066447\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.6934 - acc: 0.9192\n",
      "\n",
      " End of Epoch Learning Rate = 0.065951\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6999 - acc: 0.9127 - val_loss: 0.6934 - val_acc: 0.9192\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065951\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.6780 - acc: 0.9204\n",
      "\n",
      " End of Epoch Learning Rate = 0.065454\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6967 - acc: 0.9112 - val_loss: 0.6780 - val_acc: 0.9204\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065454\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.6937 - acc: 0.9187\n",
      "\n",
      " End of Epoch Learning Rate = 0.064956\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6933 - acc: 0.9127 - val_loss: 0.6937 - val_acc: 0.9187\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064956\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.6851 - acc: 0.9152\n",
      "\n",
      " End of Epoch Learning Rate = 0.064455\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6875 - acc: 0.9143 - val_loss: 0.6851 - val_acc: 0.9152\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064455\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.7113 - acc: 0.9137\n",
      "\n",
      " End of Epoch Learning Rate = 0.063953\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6770 - acc: 0.9162 - val_loss: 0.7113 - val_acc: 0.9137\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063953\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.6874 - acc: 0.9165\n",
      "\n",
      " End of Epoch Learning Rate = 0.063450\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.6777 - acc: 0.9155 - val_loss: 0.6874 - val_acc: 0.9165\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063450\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.7653 - acc: 0.9038\n",
      "\n",
      " End of Epoch Learning Rate = 0.062945\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6771 - acc: 0.9146 - val_loss: 0.7653 - val_acc: 0.9038\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062945\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.6623 - acc: 0.9262\n",
      "\n",
      " End of Epoch Learning Rate = 0.062438\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6769 - acc: 0.9142 - val_loss: 0.6623 - val_acc: 0.9262\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062438\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.6839 - acc: 0.9234\n",
      "\n",
      " End of Epoch Learning Rate = 0.061930\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6729 - acc: 0.9167 - val_loss: 0.6839 - val_acc: 0.9234\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061930\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.6534 - acc: 0.9261\n",
      "\n",
      " End of Epoch Learning Rate = 0.061421\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6677 - acc: 0.9177 - val_loss: 0.6534 - val_acc: 0.9261\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061421\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.6715 - acc: 0.9220\n",
      "\n",
      " End of Epoch Learning Rate = 0.060911\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6592 - acc: 0.9191 - val_loss: 0.6715 - val_acc: 0.9220\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060911\n",
      "Epoch 130/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.6692 - acc: 0.9223\n",
      "\n",
      " End of Epoch Learning Rate = 0.060400\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.6526 - acc: 0.9212 - val_loss: 0.6692 - val_acc: 0.9223\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060400\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.6935 - acc: 0.9159\n",
      "\n",
      " End of Epoch Learning Rate = 0.059887\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.6496 - acc: 0.9212 - val_loss: 0.6935 - val_acc: 0.9159\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059887\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.6583 - acc: 0.9231\n",
      "\n",
      " End of Epoch Learning Rate = 0.059373\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6570 - acc: 0.9184 - val_loss: 0.6583 - val_acc: 0.9231\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059373\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.7369 - acc: 0.9050\n",
      "\n",
      " End of Epoch Learning Rate = 0.058858\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6511 - acc: 0.9190 - val_loss: 0.7369 - val_acc: 0.9050\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058858\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.6658 - acc: 0.9238\n",
      "\n",
      " End of Epoch Learning Rate = 0.058343\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6472 - acc: 0.9203 - val_loss: 0.6658 - val_acc: 0.9238\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058343\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.6859 - acc: 0.9131\n",
      "\n",
      " End of Epoch Learning Rate = 0.057826\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.6455 - acc: 0.9213 - val_loss: 0.6859 - val_acc: 0.9131\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057826\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.6421 - acc: 0.9265\n",
      "\n",
      " End of Epoch Learning Rate = 0.057308\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6452 - acc: 0.9197 - val_loss: 0.6421 - val_acc: 0.9265\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057308\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.6671 - acc: 0.9212\n",
      "\n",
      " End of Epoch Learning Rate = 0.056790\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6293 - acc: 0.9246 - val_loss: 0.6671 - val_acc: 0.9212\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056790\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.6633 - acc: 0.9190\n",
      "\n",
      " End of Epoch Learning Rate = 0.056271\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.6350 - acc: 0.9226 - val_loss: 0.6633 - val_acc: 0.9190\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056271\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.6710 - acc: 0.9190\n",
      "\n",
      " End of Epoch Learning Rate = 0.055751\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.6306 - acc: 0.9238 - val_loss: 0.6710 - val_acc: 0.9190\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055751\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.6709 - acc: 0.9201\n",
      "\n",
      " End of Epoch Learning Rate = 0.055231\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.6316 - acc: 0.9212 - val_loss: 0.6709 - val_acc: 0.9201\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055231\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.6384 - acc: 0.9300\n",
      "\n",
      " End of Epoch Learning Rate = 0.054710\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6219 - acc: 0.9241 - val_loss: 0.6384 - val_acc: 0.9300\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054710\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.6329 - acc: 0.9258\n",
      "\n",
      " End of Epoch Learning Rate = 0.054188\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6087 - acc: 0.9275 - val_loss: 0.6329 - val_acc: 0.9258\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054188\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.6171 - acc: 0.9297\n",
      "\n",
      " End of Epoch Learning Rate = 0.053667\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6110 - acc: 0.9260 - val_loss: 0.6171 - val_acc: 0.9297\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053667\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 7s 692us/sample - loss: 0.6623 - acc: 0.9174\n",
      "\n",
      " End of Epoch Learning Rate = 0.053144\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.6118 - acc: 0.9249 - val_loss: 0.6623 - val_acc: 0.9174\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053144\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.6273 - acc: 0.9266\n",
      "\n",
      " End of Epoch Learning Rate = 0.052622\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.6029 - acc: 0.9276 - val_loss: 0.6273 - val_acc: 0.9266\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052622\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.6078 - acc: 0.9329\n",
      "\n",
      " End of Epoch Learning Rate = 0.052099\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.6050 - acc: 0.9276 - val_loss: 0.6078 - val_acc: 0.9329\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052099\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.6477 - acc: 0.9185\n",
      "\n",
      " End of Epoch Learning Rate = 0.051575\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5922 - acc: 0.9302 - val_loss: 0.6477 - val_acc: 0.9185\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051575\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.6662 - acc: 0.9195\n",
      "\n",
      " End of Epoch Learning Rate = 0.051052\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5943 - acc: 0.9288 - val_loss: 0.6662 - val_acc: 0.9195\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051052\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.6387 - acc: 0.9210\n",
      "\n",
      " End of Epoch Learning Rate = 0.050529\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.6003 - acc: 0.9266 - val_loss: 0.6387 - val_acc: 0.9210\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050529\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.6216 - acc: 0.9269\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5868 - acc: 0.9309 - val_loss: 0.6216 - val_acc: 0.9269\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.6411 - acc: 0.9227\n",
      "\n",
      " End of Epoch Learning Rate = 0.049481\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5793 - acc: 0.9327 - val_loss: 0.6411 - val_acc: 0.9227\n",
      "\n",
      " Start of Epoch Learning Rate = 0.049481\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.6128 - acc: 0.9280\n",
      "\n",
      " End of Epoch Learning Rate = 0.048958\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5850 - acc: 0.9298 - val_loss: 0.6128 - val_acc: 0.9280\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048958\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.6396 - acc: 0.9177\n",
      "\n",
      " End of Epoch Learning Rate = 0.048435\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5857 - acc: 0.9284 - val_loss: 0.6396 - val_acc: 0.9177\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048435\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.6388 - acc: 0.9221\n",
      "\n",
      " End of Epoch Learning Rate = 0.047911\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5805 - acc: 0.9324 - val_loss: 0.6388 - val_acc: 0.9221\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047911\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.5967 - acc: 0.9332\n",
      "\n",
      " End of Epoch Learning Rate = 0.047388\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5677 - acc: 0.9353 - val_loss: 0.5967 - val_acc: 0.9332\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047388\n",
      "Epoch 156/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.6113 - acc: 0.9275\n",
      "\n",
      " End of Epoch Learning Rate = 0.046866\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.5705 - acc: 0.9316 - val_loss: 0.6113 - val_acc: 0.9275\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046866\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.6201 - acc: 0.9228\n",
      "\n",
      " End of Epoch Learning Rate = 0.046343\n",
      "400/400 [==============================] - 114s 284ms/step - loss: 0.5621 - acc: 0.9351 - val_loss: 0.6201 - val_acc: 0.9228\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046343\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 7s 694us/sample - loss: 0.6161 - acc: 0.9217\n",
      "\n",
      " End of Epoch Learning Rate = 0.045822\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5581 - acc: 0.9358 - val_loss: 0.6161 - val_acc: 0.9217\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045822\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.5951 - acc: 0.9259\n",
      "\n",
      " End of Epoch Learning Rate = 0.045300\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5493 - acc: 0.9375 - val_loss: 0.5951 - val_acc: 0.9259\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045300\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.5791 - acc: 0.9341\n",
      "\n",
      " End of Epoch Learning Rate = 0.044779\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5565 - acc: 0.9338 - val_loss: 0.5791 - val_acc: 0.9341\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044779\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 0.5784 - acc: 0.9323\n",
      "\n",
      " End of Epoch Learning Rate = 0.044259\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5544 - acc: 0.9350 - val_loss: 0.5784 - val_acc: 0.9323\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044259\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.6028 - acc: 0.9252\n",
      "\n",
      " End of Epoch Learning Rate = 0.043739\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5513 - acc: 0.9353 - val_loss: 0.6028 - val_acc: 0.9252\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043739\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.5832 - acc: 0.9298\n",
      "\n",
      " End of Epoch Learning Rate = 0.043220\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5449 - acc: 0.9370 - val_loss: 0.5832 - val_acc: 0.9298\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043220\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 7s 694us/sample - loss: 0.5722 - acc: 0.9321\n",
      "\n",
      " End of Epoch Learning Rate = 0.042702\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.5429 - acc: 0.9373 - val_loss: 0.5722 - val_acc: 0.9321\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042702\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 7s 693us/sample - loss: 0.5789 - acc: 0.9315\n",
      "\n",
      " End of Epoch Learning Rate = 0.042184\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5365 - acc: 0.9386 - val_loss: 0.5789 - val_acc: 0.9315\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042184\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.5751 - acc: 0.9342\n",
      "\n",
      " End of Epoch Learning Rate = 0.041667\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5368 - acc: 0.9381 - val_loss: 0.5751 - val_acc: 0.9342\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041667\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.5795 - acc: 0.9338\n",
      "\n",
      " End of Epoch Learning Rate = 0.041152\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.5257 - acc: 0.9412 - val_loss: 0.5795 - val_acc: 0.9338\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041152\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.5995 - acc: 0.9259\n",
      "\n",
      " End of Epoch Learning Rate = 0.040637\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5264 - acc: 0.9397 - val_loss: 0.5995 - val_acc: 0.9259\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040637\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.6048 - acc: 0.9205\n",
      "\n",
      " End of Epoch Learning Rate = 0.040123\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.5215 - acc: 0.9413 - val_loss: 0.6048 - val_acc: 0.9205\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040123\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 0.5987 - acc: 0.9270\n",
      "\n",
      " End of Epoch Learning Rate = 0.039610\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.5309 - acc: 0.9353 - val_loss: 0.5987 - val_acc: 0.9270\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039610\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.5850 - acc: 0.9310\n",
      "\n",
      " End of Epoch Learning Rate = 0.039099\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5143 - acc: 0.9431 - val_loss: 0.5850 - val_acc: 0.9310\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039099\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 0.5406 - acc: 0.9407\n",
      "\n",
      " End of Epoch Learning Rate = 0.038589\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.5165 - acc: 0.9415 - val_loss: 0.5406 - val_acc: 0.9407\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038589\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.5854 - acc: 0.9275\n",
      "\n",
      " End of Epoch Learning Rate = 0.038080\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.5075 - acc: 0.9440 - val_loss: 0.5854 - val_acc: 0.9275\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038080\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.5535 - acc: 0.9360\n",
      "\n",
      " End of Epoch Learning Rate = 0.037572\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.5068 - acc: 0.9432 - val_loss: 0.5535 - val_acc: 0.9360\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037572\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.5521 - acc: 0.9354\n",
      "\n",
      " End of Epoch Learning Rate = 0.037065\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.4980 - acc: 0.9453 - val_loss: 0.5521 - val_acc: 0.9354\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037065\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.5259 - acc: 0.9383\n",
      "\n",
      " End of Epoch Learning Rate = 0.036560\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.5007 - acc: 0.9438 - val_loss: 0.5259 - val_acc: 0.9383\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036560\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 0.5633 - acc: 0.9316\n",
      "\n",
      " End of Epoch Learning Rate = 0.036057\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4962 - acc: 0.9450 - val_loss: 0.5633 - val_acc: 0.9316\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036057\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.5717 - acc: 0.9294\n",
      "\n",
      " End of Epoch Learning Rate = 0.035555\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4906 - acc: 0.9448 - val_loss: 0.5717 - val_acc: 0.9294\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035555\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.5868 - acc: 0.9253\n",
      "\n",
      " End of Epoch Learning Rate = 0.035054\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4877 - acc: 0.9447 - val_loss: 0.5868 - val_acc: 0.9253\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035054\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.5417 - acc: 0.9342\n",
      "\n",
      " End of Epoch Learning Rate = 0.034556\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.4800 - acc: 0.9474 - val_loss: 0.5417 - val_acc: 0.9342\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034556\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.5372 - acc: 0.9373\n",
      "\n",
      " End of Epoch Learning Rate = 0.034059\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.4749 - acc: 0.9487 - val_loss: 0.5372 - val_acc: 0.9373\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034059\n",
      "Epoch 182/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 0.5332 - acc: 0.9366\n",
      "\n",
      " End of Epoch Learning Rate = 0.033563\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.4777 - acc: 0.9473 - val_loss: 0.5332 - val_acc: 0.9366\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033563\n",
      "Epoch 183/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.5210 - acc: 0.9397\n",
      "\n",
      " End of Epoch Learning Rate = 0.033070\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4698 - acc: 0.9486 - val_loss: 0.5210 - val_acc: 0.9397\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033070\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.5450 - acc: 0.9333\n",
      "\n",
      " End of Epoch Learning Rate = 0.032578\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.4644 - acc: 0.9495 - val_loss: 0.5450 - val_acc: 0.9333\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032578\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.5313 - acc: 0.9379\n",
      "\n",
      " End of Epoch Learning Rate = 0.032088\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4639 - acc: 0.9494 - val_loss: 0.5313 - val_acc: 0.9379\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032088\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.5330 - acc: 0.9360\n",
      "\n",
      " End of Epoch Learning Rate = 0.031601\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4591 - acc: 0.9499 - val_loss: 0.5330 - val_acc: 0.9360\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031601\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.5103 - acc: 0.9410\n",
      "\n",
      " End of Epoch Learning Rate = 0.031115\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4589 - acc: 0.9492 - val_loss: 0.5103 - val_acc: 0.9410\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031115\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 0.5287 - acc: 0.9409\n",
      "\n",
      " End of Epoch Learning Rate = 0.030631\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4474 - acc: 0.9528 - val_loss: 0.5287 - val_acc: 0.9409\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030631\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.5173 - acc: 0.9359\n",
      "\n",
      " End of Epoch Learning Rate = 0.030150\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4470 - acc: 0.9519 - val_loss: 0.5173 - val_acc: 0.9359\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030150\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.5215 - acc: 0.9357\n",
      "\n",
      " End of Epoch Learning Rate = 0.029670\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4437 - acc: 0.9536 - val_loss: 0.5215 - val_acc: 0.9357\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029670\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.5082 - acc: 0.9420\n",
      "\n",
      " End of Epoch Learning Rate = 0.029193\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.4367 - acc: 0.9538 - val_loss: 0.5082 - val_acc: 0.9420\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029193\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.5178 - acc: 0.9360\n",
      "\n",
      " End of Epoch Learning Rate = 0.028718\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.4420 - acc: 0.9523 - val_loss: 0.5178 - val_acc: 0.9360\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028718\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.5224 - acc: 0.9355\n",
      "\n",
      " End of Epoch Learning Rate = 0.028246\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.4358 - acc: 0.9526 - val_loss: 0.5224 - val_acc: 0.9355\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028246\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.4870 - acc: 0.9426\n",
      "\n",
      " End of Epoch Learning Rate = 0.027775\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.4325 - acc: 0.9541 - val_loss: 0.4870 - val_acc: 0.9426\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027775\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.5015 - acc: 0.9413\n",
      "\n",
      " End of Epoch Learning Rate = 0.027308\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.4247 - acc: 0.9560 - val_loss: 0.5015 - val_acc: 0.9413\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027308\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.5118 - acc: 0.9377\n",
      "\n",
      " End of Epoch Learning Rate = 0.026843\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4236 - acc: 0.9552 - val_loss: 0.5118 - val_acc: 0.9377\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026843\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.4980 - acc: 0.9397\n",
      "\n",
      " End of Epoch Learning Rate = 0.026380\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4195 - acc: 0.9562 - val_loss: 0.4980 - val_acc: 0.9397\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026380\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.5460 - acc: 0.9295\n",
      "\n",
      " End of Epoch Learning Rate = 0.025920\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.4131 - acc: 0.9581 - val_loss: 0.5460 - val_acc: 0.9295\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025920\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.5175 - acc: 0.9330\n",
      "\n",
      " End of Epoch Learning Rate = 0.025462\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.4108 - acc: 0.9577 - val_loss: 0.5175 - val_acc: 0.9330\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025462\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.4878 - acc: 0.9415\n",
      "\n",
      " End of Epoch Learning Rate = 0.025007\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4080 - acc: 0.9589 - val_loss: 0.4878 - val_acc: 0.9415\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025007\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.4842 - acc: 0.9403\n",
      "\n",
      " End of Epoch Learning Rate = 0.024555\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.4045 - acc: 0.9592 - val_loss: 0.4842 - val_acc: 0.9403\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024555\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.5230 - acc: 0.9329\n",
      "\n",
      " End of Epoch Learning Rate = 0.024106\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.4024 - acc: 0.9584 - val_loss: 0.5230 - val_acc: 0.9329\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024106\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.4794 - acc: 0.9432\n",
      "\n",
      " End of Epoch Learning Rate = 0.023660\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3987 - acc: 0.9598 - val_loss: 0.4794 - val_acc: 0.9432\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023660\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 0.4949 - acc: 0.9354\n",
      "\n",
      " End of Epoch Learning Rate = 0.023216\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3933 - acc: 0.9604 - val_loss: 0.4949 - val_acc: 0.9354\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023216\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.4897 - acc: 0.9390\n",
      "\n",
      " End of Epoch Learning Rate = 0.022776\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3919 - acc: 0.9607 - val_loss: 0.4897 - val_acc: 0.9390\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022776\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.4816 - acc: 0.9420\n",
      "\n",
      " End of Epoch Learning Rate = 0.022338\n",
      "400/400 [==============================] - 115s 289ms/step - loss: 0.3884 - acc: 0.9613 - val_loss: 0.4816 - val_acc: 0.9420\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022338\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.5027 - acc: 0.9359\n",
      "\n",
      " End of Epoch Learning Rate = 0.021904\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3814 - acc: 0.9625 - val_loss: 0.5027 - val_acc: 0.9359\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021904\n",
      "Epoch 208/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.4583 - acc: 0.9461\n",
      "\n",
      " End of Epoch Learning Rate = 0.021472\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.3836 - acc: 0.9615 - val_loss: 0.4583 - val_acc: 0.9461\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021472\n",
      "Epoch 209/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 717us/sample - loss: 0.4675 - acc: 0.9423\n",
      "\n",
      " End of Epoch Learning Rate = 0.021044\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3818 - acc: 0.9611 - val_loss: 0.4675 - val_acc: 0.9423\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021044\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.4654 - acc: 0.9431\n",
      "\n",
      " End of Epoch Learning Rate = 0.020619\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3740 - acc: 0.9623 - val_loss: 0.4654 - val_acc: 0.9431\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020619\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 0.4631 - acc: 0.9431\n",
      "\n",
      " End of Epoch Learning Rate = 0.020197\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3739 - acc: 0.9612 - val_loss: 0.4631 - val_acc: 0.9431\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020197\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.4565 - acc: 0.9439\n",
      "\n",
      " End of Epoch Learning Rate = 0.019778\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3676 - acc: 0.9637 - val_loss: 0.4565 - val_acc: 0.9439\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019778\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.4808 - acc: 0.9383\n",
      "\n",
      " End of Epoch Learning Rate = 0.019363\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3627 - acc: 0.9647 - val_loss: 0.4808 - val_acc: 0.9383\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019363\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.4620 - acc: 0.9440\n",
      "\n",
      " End of Epoch Learning Rate = 0.018951\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3655 - acc: 0.9628 - val_loss: 0.4620 - val_acc: 0.9440\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018951\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.4401 - acc: 0.9454\n",
      "\n",
      " End of Epoch Learning Rate = 0.018542\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3579 - acc: 0.9655 - val_loss: 0.4401 - val_acc: 0.9454\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018542\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.4564 - acc: 0.9442\n",
      "\n",
      " End of Epoch Learning Rate = 0.018137\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3568 - acc: 0.9657 - val_loss: 0.4564 - val_acc: 0.9442\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018137\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 0.4409 - acc: 0.9482\n",
      "\n",
      " End of Epoch Learning Rate = 0.017735\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3489 - acc: 0.9670 - val_loss: 0.4409 - val_acc: 0.9482\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017735\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.4328 - acc: 0.9499\n",
      "\n",
      " End of Epoch Learning Rate = 0.017337\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3486 - acc: 0.9664 - val_loss: 0.4328 - val_acc: 0.9499\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017337\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.4461 - acc: 0.9439\n",
      "\n",
      " End of Epoch Learning Rate = 0.016943\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3427 - acc: 0.9680 - val_loss: 0.4461 - val_acc: 0.9439\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016943\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.4384 - acc: 0.9465\n",
      "\n",
      " End of Epoch Learning Rate = 0.016552\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3414 - acc: 0.9685 - val_loss: 0.4384 - val_acc: 0.9465\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016552\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.4278 - acc: 0.9490\n",
      "\n",
      " End of Epoch Learning Rate = 0.016165\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3387 - acc: 0.9686 - val_loss: 0.4278 - val_acc: 0.9490\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016165\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.4499 - acc: 0.9421\n",
      "\n",
      " End of Epoch Learning Rate = 0.015781\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3333 - acc: 0.9699 - val_loss: 0.4499 - val_acc: 0.9421\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015781\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.4507 - acc: 0.9420\n",
      "\n",
      " End of Epoch Learning Rate = 0.015401\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3318 - acc: 0.9698 - val_loss: 0.4507 - val_acc: 0.9420\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015401\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.4349 - acc: 0.9466\n",
      "\n",
      " End of Epoch Learning Rate = 0.015025\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3280 - acc: 0.9700 - val_loss: 0.4349 - val_acc: 0.9466\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015025\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.4182 - acc: 0.9473\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3246 - acc: 0.9713 - val_loss: 0.4182 - val_acc: 0.9473\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.4258 - acc: 0.9479\n",
      "\n",
      " End of Epoch Learning Rate = 0.014285\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3248 - acc: 0.9701 - val_loss: 0.4258 - val_acc: 0.9479\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014285\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 7s 694us/sample - loss: 0.4241 - acc: 0.9470\n",
      "\n",
      " End of Epoch Learning Rate = 0.013921\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.3167 - acc: 0.9720 - val_loss: 0.4241 - val_acc: 0.9470\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013921\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.4137 - acc: 0.9488\n",
      "\n",
      " End of Epoch Learning Rate = 0.013560\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3151 - acc: 0.9722 - val_loss: 0.4137 - val_acc: 0.9488\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013560\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.4160 - acc: 0.9478\n",
      "\n",
      " End of Epoch Learning Rate = 0.013204\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3162 - acc: 0.9711 - val_loss: 0.4160 - val_acc: 0.9478\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013204\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.4228 - acc: 0.9459\n",
      "\n",
      " End of Epoch Learning Rate = 0.012851\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3085 - acc: 0.9741 - val_loss: 0.4228 - val_acc: 0.9459\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012851\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.4086 - acc: 0.9491\n",
      "\n",
      " End of Epoch Learning Rate = 0.012503\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3066 - acc: 0.9739 - val_loss: 0.4086 - val_acc: 0.9491\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012503\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.4321 - acc: 0.9436\n",
      "\n",
      " End of Epoch Learning Rate = 0.012159\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.3030 - acc: 0.9741 - val_loss: 0.4321 - val_acc: 0.9436\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012159\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.4244 - acc: 0.9482\n",
      "\n",
      " End of Epoch Learning Rate = 0.011819\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.3027 - acc: 0.9738 - val_loss: 0.4244 - val_acc: 0.9482\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011819\n",
      "Epoch 234/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.4161 - acc: 0.9477\n",
      "\n",
      " End of Epoch Learning Rate = 0.011483\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2974 - acc: 0.9748 - val_loss: 0.4161 - val_acc: 0.9477\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011483\n",
      "Epoch 235/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.4209 - acc: 0.9479\n",
      "\n",
      " End of Epoch Learning Rate = 0.011152\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2979 - acc: 0.9742 - val_loss: 0.4209 - val_acc: 0.9479\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011152\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.4140 - acc: 0.9488\n",
      "\n",
      " End of Epoch Learning Rate = 0.010824\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2927 - acc: 0.9752 - val_loss: 0.4140 - val_acc: 0.9488\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010824\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.4092 - acc: 0.9483\n",
      "\n",
      " End of Epoch Learning Rate = 0.010501\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2923 - acc: 0.9751 - val_loss: 0.4092 - val_acc: 0.9483\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010501\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.3964 - acc: 0.9504\n",
      "\n",
      " End of Epoch Learning Rate = 0.010182\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2838 - acc: 0.9768 - val_loss: 0.3964 - val_acc: 0.9504\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010182\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.4119 - acc: 0.9501\n",
      "\n",
      " End of Epoch Learning Rate = 0.009868\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2836 - acc: 0.9770 - val_loss: 0.4119 - val_acc: 0.9501\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009868\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.3987 - acc: 0.9489\n",
      "\n",
      " End of Epoch Learning Rate = 0.009558\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2860 - acc: 0.9756 - val_loss: 0.3987 - val_acc: 0.9489\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009558\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.4059 - acc: 0.9474\n",
      "\n",
      " End of Epoch Learning Rate = 0.009253\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2796 - acc: 0.9771 - val_loss: 0.4059 - val_acc: 0.9474\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009253\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 0.4015 - acc: 0.9499\n",
      "\n",
      " End of Epoch Learning Rate = 0.008952\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2763 - acc: 0.9776 - val_loss: 0.4015 - val_acc: 0.9499\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008952\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.3891 - acc: 0.9517\n",
      "\n",
      " End of Epoch Learning Rate = 0.008655\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2735 - acc: 0.9780 - val_loss: 0.3891 - val_acc: 0.9517\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008655\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.4107 - acc: 0.9476\n",
      "\n",
      " End of Epoch Learning Rate = 0.008363\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2693 - acc: 0.9789 - val_loss: 0.4107 - val_acc: 0.9476\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008363\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 0.3913 - acc: 0.9502\n",
      "\n",
      " End of Epoch Learning Rate = 0.008076\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2748 - acc: 0.9775 - val_loss: 0.3913 - val_acc: 0.9502\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008076\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.3891 - acc: 0.9506\n",
      "\n",
      " End of Epoch Learning Rate = 0.007793\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2689 - acc: 0.9776 - val_loss: 0.3891 - val_acc: 0.9506\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007793\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.3814 - acc: 0.9523\n",
      "\n",
      " End of Epoch Learning Rate = 0.007515\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2665 - acc: 0.9784 - val_loss: 0.3814 - val_acc: 0.9523\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007515\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.3835 - acc: 0.9524\n",
      "\n",
      " End of Epoch Learning Rate = 0.007241\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.2651 - acc: 0.9788 - val_loss: 0.3835 - val_acc: 0.9524\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007241\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 0.3761 - acc: 0.9521\n",
      "\n",
      " End of Epoch Learning Rate = 0.006972\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2618 - acc: 0.9792 - val_loss: 0.3761 - val_acc: 0.9521\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006972\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.3944 - acc: 0.9478\n",
      "\n",
      " End of Epoch Learning Rate = 0.006708\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.2560 - acc: 0.9819 - val_loss: 0.3944 - val_acc: 0.9478\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006708\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 0.3884 - acc: 0.9477\n",
      "\n",
      " End of Epoch Learning Rate = 0.006449\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2574 - acc: 0.9799 - val_loss: 0.3884 - val_acc: 0.9477\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006449\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.3955 - acc: 0.9487\n",
      "\n",
      " End of Epoch Learning Rate = 0.006194\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2545 - acc: 0.9802 - val_loss: 0.3955 - val_acc: 0.9487\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006194\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.3911 - acc: 0.9478\n",
      "\n",
      " End of Epoch Learning Rate = 0.005944\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2524 - acc: 0.9812 - val_loss: 0.3911 - val_acc: 0.9478\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005944\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.3981 - acc: 0.9482\n",
      "\n",
      " End of Epoch Learning Rate = 0.005699\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2483 - acc: 0.9821 - val_loss: 0.3981 - val_acc: 0.9482\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005699\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.3861 - acc: 0.9485\n",
      "\n",
      " End of Epoch Learning Rate = 0.005459\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2463 - acc: 0.9826 - val_loss: 0.3861 - val_acc: 0.9485\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005459\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.3751 - acc: 0.9519\n",
      "\n",
      " End of Epoch Learning Rate = 0.005224\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2451 - acc: 0.9826 - val_loss: 0.3751 - val_acc: 0.9519\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005224\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 0.3682 - acc: 0.9520\n",
      "\n",
      " End of Epoch Learning Rate = 0.004994\n",
      "400/400 [==============================] - 115s 288ms/step - loss: 0.2450 - acc: 0.9831 - val_loss: 0.3682 - val_acc: 0.9520\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004994\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.3797 - acc: 0.9520\n",
      "\n",
      " End of Epoch Learning Rate = 0.004768\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2419 - acc: 0.9825 - val_loss: 0.3797 - val_acc: 0.9520\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004768\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 7s 696us/sample - loss: 0.3652 - acc: 0.9547\n",
      "\n",
      " End of Epoch Learning Rate = 0.004548\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2395 - acc: 0.9828 - val_loss: 0.3652 - val_acc: 0.9547\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004548\n",
      "Epoch 260/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.3703 - acc: 0.9536\n",
      "\n",
      " End of Epoch Learning Rate = 0.004332\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2375 - acc: 0.9840 - val_loss: 0.3703 - val_acc: 0.9536\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004332\n",
      "Epoch 261/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 720us/sample - loss: 0.3735 - acc: 0.9522\n",
      "\n",
      " End of Epoch Learning Rate = 0.004122\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2380 - acc: 0.9825 - val_loss: 0.3735 - val_acc: 0.9522\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004122\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.3771 - acc: 0.9529\n",
      "\n",
      " End of Epoch Learning Rate = 0.003916\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2356 - acc: 0.9834 - val_loss: 0.3771 - val_acc: 0.9529\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003916\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 7s 691us/sample - loss: 0.3614 - acc: 0.9526\n",
      "\n",
      " End of Epoch Learning Rate = 0.003716\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2322 - acc: 0.9843 - val_loss: 0.3614 - val_acc: 0.9526\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003716\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.3528 - acc: 0.9546\n",
      "\n",
      " End of Epoch Learning Rate = 0.003521\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2313 - acc: 0.9844 - val_loss: 0.3528 - val_acc: 0.9546\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003521\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 0.3782 - acc: 0.9506\n",
      "\n",
      " End of Epoch Learning Rate = 0.003331\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2324 - acc: 0.9842 - val_loss: 0.3782 - val_acc: 0.9506\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003331\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.3694 - acc: 0.9525\n",
      "\n",
      " End of Epoch Learning Rate = 0.003146\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2269 - acc: 0.9857 - val_loss: 0.3694 - val_acc: 0.9525\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003146\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 0.3628 - acc: 0.9523\n",
      "\n",
      " End of Epoch Learning Rate = 0.002966\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2280 - acc: 0.9845 - val_loss: 0.3628 - val_acc: 0.9523\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002966\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.3637 - acc: 0.9531\n",
      "\n",
      " End of Epoch Learning Rate = 0.002791\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2245 - acc: 0.9856 - val_loss: 0.3637 - val_acc: 0.9531\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002791\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 0.3664 - acc: 0.9530\n",
      "\n",
      " End of Epoch Learning Rate = 0.002621\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2246 - acc: 0.9859 - val_loss: 0.3664 - val_acc: 0.9530\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002621\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 0.3666 - acc: 0.9531\n",
      "\n",
      " End of Epoch Learning Rate = 0.002457\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2208 - acc: 0.9866 - val_loss: 0.3666 - val_acc: 0.9531\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002457\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.3592 - acc: 0.9542\n",
      "\n",
      " End of Epoch Learning Rate = 0.002298\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2220 - acc: 0.9858 - val_loss: 0.3592 - val_acc: 0.9542\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002298\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.3683 - acc: 0.9517\n",
      "\n",
      " End of Epoch Learning Rate = 0.001995\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2193 - acc: 0.9868 - val_loss: 0.3683 - val_acc: 0.9517\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001995\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 0.3629 - acc: 0.9528\n",
      "\n",
      " End of Epoch Learning Rate = 0.001852\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2177 - acc: 0.9868 - val_loss: 0.3629 - val_acc: 0.9528\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001852\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 7s 691us/sample - loss: 0.3535 - acc: 0.9553\n",
      "\n",
      " End of Epoch Learning Rate = 0.001714\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2162 - acc: 0.9867 - val_loss: 0.3535 - val_acc: 0.9553\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001714\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.3530 - acc: 0.9564\n",
      "\n",
      " End of Epoch Learning Rate = 0.001581\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2174 - acc: 0.9861 - val_loss: 0.3530 - val_acc: 0.9564\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001581\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.3502 - acc: 0.9570\n",
      "\n",
      " End of Epoch Learning Rate = 0.001453\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2135 - acc: 0.9879 - val_loss: 0.3502 - val_acc: 0.9570\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001453\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.3525 - acc: 0.9549\n",
      "\n",
      " End of Epoch Learning Rate = 0.001331\n",
      "400/400 [==============================] - 115s 287ms/step - loss: 0.2145 - acc: 0.9871 - val_loss: 0.3525 - val_acc: 0.9549\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001331\n",
      "Epoch 279/300\n",
      "194/400 [=============>................] - ETA: 55s - loss: 0.2139 - acc: 0.9871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 694us/sample - loss: 0.3445 - acc: 0.9552\n",
      "\n",
      " End of Epoch Learning Rate = 0.000341\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2082 - acc: 0.9879 - val_loss: 0.3445 - val_acc: 0.9552\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000341\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 0.3496 - acc: 0.9552\n",
      "\n",
      " End of Epoch Learning Rate = 0.000284\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2057 - acc: 0.9892 - val_loss: 0.3496 - val_acc: 0.9552\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000284\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 0.3493 - acc: 0.9554\n",
      "\n",
      " End of Epoch Learning Rate = 0.000232\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.2051 - acc: 0.9898 - val_loss: 0.3493 - val_acc: 0.9554\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000232\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.3465 - acc: 0.9562\n",
      "\n",
      " End of Epoch Learning Rate = 0.000185\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.2057 - acc: 0.9892 - val_loss: 0.3465 - val_acc: 0.9562\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000185\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.3457 - acc: 0.9566\n",
      "\n",
      " End of Epoch Learning Rate = 0.000144\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2055 - acc: 0.9893 - val_loss: 0.3457 - val_acc: 0.9566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000144\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 0.3447 - acc: 0.9562\n",
      "\n",
      " End of Epoch Learning Rate = 0.000109\n",
      "400/400 [==============================] - 114s 286ms/step - loss: 0.2048 - acc: 0.9892 - val_loss: 0.3447 - val_acc: 0.9562\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000109\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 0.3480 - acc: 0.9560\n",
      "\n",
      " End of Epoch Learning Rate = 0.000079\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.2040 - acc: 0.9893 - val_loss: 0.3480 - val_acc: 0.9560\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000079\n",
      "Epoch 296/300\n",
      " 47/400 [==>...........................] - ETA: 1:35 - loss: 0.2074 - acc: 0.9862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(dataGenerator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch =steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (%): 95.66\n"
     ]
    }
   ],
   "source": [
    "#get final performance\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print('Test accuracy (%):', 100*sum(np.argmax(y_pred,-1)==np.argmax(y_test,-1))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss and accuracy\n",
    "plot_history(model.history.history)\n",
    "\n",
    "#plot learning rate schedule\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.arange(0,len(lr_scheduler.lr_used))/steps_per_epoch,lr_scheduler.lr_used)\n",
    "plt.xlabel('epoch number')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weigts used for updating\n",
    "model.save_weights(ModelsPath+'Final_weights_'+WhichDataSet+'_32bit_model_sReLU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
