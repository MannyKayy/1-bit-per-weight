{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow.keras implementation of one-bit-per-weight shifted ReLU CNN for CIFAR 100 \n",
    "##  https://arxiv.org/abs/1907.06916\n",
    "## Mark D. McDonnell, Hesham Mostafa, Runchun Wang, Andre van Schaik,\n",
    "## Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  1.13.1\n"
     ]
    }
   ],
   "source": [
    "# select a GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import savemat,loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import tensorflow\n",
    "print('Tensorflow version = ',tensorflow.__version__)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, History\n",
    "\n",
    "#from tensorflow.keras import backend as K\n",
    "\n",
    "from ResNetModel import resnet_srelu\n",
    "from Utils import cutout,LR_WarmRestart,GetDataGen,plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#params\n",
    "#WhichDataSet = 'CIFAR10'\n",
    "WhichDataSet = 'CIFAR100'\n",
    "init_lr = 0.1\n",
    "epochs = 300\n",
    "batch_size = 125\n",
    "My_wd=5e-4/2\n",
    "resnet_width = 10\n",
    "resnet_depth = 20\n",
    "UseBinary=True\n",
    "UseCutout=True\n",
    "Loss = 'categorical_crossentropy'\n",
    "Optimizer = SGD(lr=init_lr,decay=0.0, momentum=0.9, nesterov=False)\n",
    "Metrics = ['accuracy']\n",
    "ModelsPath = 'TrainedModels/Tensorflow.keras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare data\n",
    "if WhichDataSet == 'CIFAR10':\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()\n",
    "else:\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar100.load_data()\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "K_train = x_train.shape[0]\n",
    "input_shape = x_train.shape[1:]\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')#/255.0\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_entropy_logits_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        return tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d (BinaryConv2D)    (None, 32, 32, 160)  4320        batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 32, 32, 160)  0           binary_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 32, 32, 160)  0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 32, 160)  0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_1 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 32, 32, 160)  0           binary_conv2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 32, 32, 160)  0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 32, 32, 160)  0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_2 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 160)  0           binary_conv2d_2[0][0]            \n",
      "                                                                 binary_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 32, 32, 160)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32, 32, 160)  0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 32, 32, 160)  0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_3 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 32, 32, 160)  0           binary_conv2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 32, 32, 160)  0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 32, 32, 160)  0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_4 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 160)  0           binary_conv2d_4[0][0]            \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 32, 32, 160)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 160)  0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 32, 32, 160)  0           re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_5 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 32, 32, 160)  0           binary_conv2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32, 32, 160)  0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 32, 32, 160)  0           re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_6 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 160)  0           binary_conv2d_6[0][0]            \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 32, 32, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 32, 32, 160)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 32, 32, 160)  0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_7 (BinaryConv2D)  (None, 16, 16, 320)  460800      lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 16, 16, 320)  0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 16, 16, 320)  0           re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 16, 16, 160)  0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_8 (BinaryConv2D)  (None, 16, 16, 320)  921600      lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 320)  0           average_pooling2d[0][0]          \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_8[0][0]            \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 16, 16, 320)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 16, 16, 320)  0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 16, 16, 320)  0           re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_9 (BinaryConv2D)  (None, 16, 16, 320)  921600      lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 16, 16, 320)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 16, 16, 320)  0           re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_10 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_10[0][0]           \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 16, 16, 320)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 16, 16, 320)  0           lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 16, 16, 320)  0           re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_11 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 16, 16, 320)  0           lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 16, 16, 320)  0           re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_12 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_12[0][0]           \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 16, 16, 320)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 16, 16, 320)  0           lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 16, 16, 320)  0           re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_13 (BinaryConv2D) (None, 8, 8, 640)    1843200     lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 8, 8, 640)    0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 320)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 8, 8, 640)    0           re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 8, 320)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_14 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 640)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_14[0][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 8, 640)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 8, 640)    0           lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 8, 640)    0           re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_15 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 8, 8, 640)    0           lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 8, 640)    0           re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_16 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_16[0][0]           \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 8, 640)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 8, 8, 640)    0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 8, 8, 640)    0           re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_17 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 8, 8, 640)    0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 8, 640)    0           re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_18 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_18[0][0]           \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 8, 640)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 8, 8, 640)    0           lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 8, 640)    0           re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_19 (BinaryConv2D) (None, 8, 8, 100)    64000       lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 8, 100)    0           binary_conv2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 100)          0           lambda_40[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,794,732\n",
      "Trainable params: 26,794,726\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define a datagen or generating training samples with flip and pad/crop augmentation, and if set to True, with cutout augmentation\n",
    "dataGenerator = GetDataGen(UseCutout)\n",
    "\n",
    "#define and compile the model\n",
    "Temperature=25.0\n",
    "model = resnet_srelu(Temperature,UseBinary,input_shape=input_shape, depth=resnet_depth, num_classes=num_classes,\n",
    "                     wd=My_wd,width=resnet_width)\n",
    "model.compile(loss=catcross_entropy_logits_loss() ,optimizer = Optimizer, metrics = Metrics)\n",
    "\n",
    "#print  the model\n",
    "model.summary()\n",
    "\n",
    "#define the learnng rate schedule\n",
    "steps_per_epoch = int(np.floor(K_train / batch_size))\n",
    "lr_scheduler = LR_WarmRestart(nbatch=steps_per_epoch,\n",
    "                              initial_lr=init_lr, min_lr=init_lr*1e-4,\n",
    "                              epochs_restart = [],\n",
    "                              Tmult=300.0) \n",
    "\n",
    "#define callbacks\n",
    "history = History()\n",
    "callbacks = [lr_scheduler,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 8s 786us/sample - loss: 6.6323 - acc: 0.0539\n",
      "\n",
      " End of Epoch Learning Rate = 0.099997\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 7.4148 - acc: 0.0305 - val_loss: 6.6323 - val_acc: 0.0539\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099997\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 5.5967 - acc: 0.1182\n",
      "\n",
      " End of Epoch Learning Rate = 0.099989\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 6.1285 - acc: 0.0797 - val_loss: 5.5967 - val_acc: 0.1182\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099989\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 4.8241 - acc: 0.1859\n",
      "\n",
      " End of Epoch Learning Rate = 0.099975\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 5.2921 - acc: 0.1383 - val_loss: 4.8241 - val_acc: 0.1859\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099975\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 8s 750us/sample - loss: 4.3621 - acc: 0.2410\n",
      "\n",
      " End of Epoch Learning Rate = 0.099956\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 4.6892 - acc: 0.1973 - val_loss: 4.3621 - val_acc: 0.2410\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099956\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 4.0340 - acc: 0.2970\n",
      "\n",
      " End of Epoch Learning Rate = 0.099931\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 4.2702 - acc: 0.2520 - val_loss: 4.0340 - val_acc: 0.2970\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099931\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 3.6408 - acc: 0.3737\n",
      "\n",
      " End of Epoch Learning Rate = 0.099901\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.9608 - acc: 0.3105 - val_loss: 3.6408 - val_acc: 0.3737\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099901\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.4678 - acc: 0.4346\n",
      "\n",
      " End of Epoch Learning Rate = 0.099866\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.7346 - acc: 0.3673 - val_loss: 3.4678 - val_acc: 0.4346\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099866\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.3952 - acc: 0.4576\n",
      "\n",
      " End of Epoch Learning Rate = 0.099825\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.6168 - acc: 0.4045 - val_loss: 3.3952 - val_acc: 0.4576\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099825\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.4009 - acc: 0.4864\n",
      "\n",
      " End of Epoch Learning Rate = 0.099778\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.5382 - acc: 0.4424 - val_loss: 3.4009 - val_acc: 0.4864\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099778\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.3727 - acc: 0.5095\n",
      "\n",
      " End of Epoch Learning Rate = 0.099726\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.4906 - acc: 0.4674 - val_loss: 3.3727 - val_acc: 0.5095\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099726\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.4205 - acc: 0.5153\n",
      "\n",
      " End of Epoch Learning Rate = 0.099669\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.4656 - acc: 0.4880 - val_loss: 3.4205 - val_acc: 0.5153\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099669\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 8s 755us/sample - loss: 3.2849 - acc: 0.5548\n",
      "\n",
      " End of Epoch Learning Rate = 0.099606\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.4342 - acc: 0.5115 - val_loss: 3.2849 - val_acc: 0.5548\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099606\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.2707 - acc: 0.5721\n",
      "\n",
      " End of Epoch Learning Rate = 0.099537\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.4365 - acc: 0.5264 - val_loss: 3.2707 - val_acc: 0.5721\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099537\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.3899 - acc: 0.5661\n",
      "\n",
      " End of Epoch Learning Rate = 0.099464\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.4155 - acc: 0.5409 - val_loss: 3.3899 - val_acc: 0.5661\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099464\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.3030 - acc: 0.5733\n",
      "\n",
      " End of Epoch Learning Rate = 0.099384\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.4069 - acc: 0.5532 - val_loss: 3.3030 - val_acc: 0.5733\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099384\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.4432 - acc: 0.5642\n",
      "\n",
      " End of Epoch Learning Rate = 0.099300\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.4130 - acc: 0.5579 - val_loss: 3.4432 - val_acc: 0.5642\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099300\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 3.2995 - acc: 0.5991\n",
      "\n",
      " End of Epoch Learning Rate = 0.099210\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3991 - acc: 0.5737 - val_loss: 3.2995 - val_acc: 0.5991\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099210\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 3.4137 - acc: 0.5855\n",
      "\n",
      " End of Epoch Learning Rate = 0.099114\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.3947 - acc: 0.5781 - val_loss: 3.4137 - val_acc: 0.5855\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099114\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 3.3844 - acc: 0.6012\n",
      "\n",
      " End of Epoch Learning Rate = 0.099014\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3917 - acc: 0.5866 - val_loss: 3.3844 - val_acc: 0.6012\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099014\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.4575 - acc: 0.5846\n",
      "\n",
      " End of Epoch Learning Rate = 0.098907\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3859 - acc: 0.5949 - val_loss: 3.4575 - val_acc: 0.5846\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098907\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 3.4381 - acc: 0.5994\n",
      "\n",
      " End of Epoch Learning Rate = 0.098796\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3956 - acc: 0.5949 - val_loss: 3.4381 - val_acc: 0.5994\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098796\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.4715 - acc: 0.6054\n",
      "\n",
      " End of Epoch Learning Rate = 0.098679\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3961 - acc: 0.6007 - val_loss: 3.4715 - val_acc: 0.6054\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098679\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.3442 - acc: 0.6283\n",
      "\n",
      " End of Epoch Learning Rate = 0.098557\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3870 - acc: 0.6060 - val_loss: 3.3442 - val_acc: 0.6283\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098557\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.4251 - acc: 0.6071\n",
      "\n",
      " End of Epoch Learning Rate = 0.098429\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.3871 - acc: 0.6086 - val_loss: 3.4251 - val_acc: 0.6071\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098429\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 3.4248 - acc: 0.6093\n",
      "\n",
      " End of Epoch Learning Rate = 0.098296\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3710 - acc: 0.6169 - val_loss: 3.4248 - val_acc: 0.6093\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098296\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.4361 - acc: 0.6238\n",
      "\n",
      " End of Epoch Learning Rate = 0.098158\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.3589 - acc: 0.6207 - val_loss: 3.4361 - val_acc: 0.6238\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098158\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.4073 - acc: 0.6141\n",
      "\n",
      " End of Epoch Learning Rate = 0.098015\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.3616 - acc: 0.6199 - val_loss: 3.4073 - val_acc: 0.6141\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098015\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.3748 - acc: 0.6289\n",
      "\n",
      " End of Epoch Learning Rate = 0.097866\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3470 - acc: 0.6274 - val_loss: 3.3748 - val_acc: 0.6289\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097866\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.4781 - acc: 0.6136\n",
      "\n",
      " End of Epoch Learning Rate = 0.097712\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 3.3436 - acc: 0.6306 - val_loss: 3.4781 - val_acc: 0.6136\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097712\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 3.4384 - acc: 0.6239\n",
      "\n",
      " End of Epoch Learning Rate = 0.097553\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3501 - acc: 0.6303 - val_loss: 3.4384 - val_acc: 0.6239\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097553\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 3.3700 - acc: 0.6332\n",
      "\n",
      " End of Epoch Learning Rate = 0.097389\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3450 - acc: 0.6336 - val_loss: 3.3700 - val_acc: 0.6332\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097389\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 3.4372 - acc: 0.6303\n",
      "\n",
      " End of Epoch Learning Rate = 0.097219\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3894 - acc: 0.6315 - val_loss: 3.4372 - val_acc: 0.6303\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097219\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.4228 - acc: 0.6273\n",
      "\n",
      " End of Epoch Learning Rate = 0.097044\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3625 - acc: 0.6377 - val_loss: 3.4228 - val_acc: 0.6273\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097044\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 3.4985 - acc: 0.6229\n",
      "\n",
      " End of Epoch Learning Rate = 0.096864\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3399 - acc: 0.6397 - val_loss: 3.4985 - val_acc: 0.6229\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096864\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.4065 - acc: 0.6377\n",
      "\n",
      " End of Epoch Learning Rate = 0.096679\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 3.3451 - acc: 0.6406 - val_loss: 3.4065 - val_acc: 0.6377\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096679\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.3724 - acc: 0.6394\n",
      "\n",
      " End of Epoch Learning Rate = 0.096489\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3126 - acc: 0.6476 - val_loss: 3.3724 - val_acc: 0.6394\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096489\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.3835 - acc: 0.6491\n",
      "\n",
      " End of Epoch Learning Rate = 0.096294\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.3038 - acc: 0.6488 - val_loss: 3.3835 - val_acc: 0.6491\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096294\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 3.4434 - acc: 0.6331\n",
      "\n",
      " End of Epoch Learning Rate = 0.096094\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3218 - acc: 0.6466 - val_loss: 3.4434 - val_acc: 0.6331\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096094\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.4722 - acc: 0.6271\n",
      "\n",
      " End of Epoch Learning Rate = 0.095888\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.3180 - acc: 0.6474 - val_loss: 3.4722 - val_acc: 0.6271\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095888\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.8398 - acc: 0.6292\n",
      "\n",
      " End of Epoch Learning Rate = 0.095678\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 3.4703 - acc: 0.6304 - val_loss: 3.8398 - val_acc: 0.6292\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095678\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.7482 - acc: 0.6336\n",
      "\n",
      " End of Epoch Learning Rate = 0.095462\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.7199 - acc: 0.6408 - val_loss: 3.7482 - val_acc: 0.6336\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095462\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.5719 - acc: 0.6544\n",
      "\n",
      " End of Epoch Learning Rate = 0.095242\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.5833 - acc: 0.6488 - val_loss: 3.5719 - val_acc: 0.6544\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095242\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.6243 - acc: 0.6277\n",
      "\n",
      " End of Epoch Learning Rate = 0.095016\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 3.5467 - acc: 0.6462 - val_loss: 3.6243 - val_acc: 0.6277\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095016\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 3.5095 - acc: 0.6440\n",
      "\n",
      " End of Epoch Learning Rate = 0.094786\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.4636 - acc: 0.6543 - val_loss: 3.5095 - val_acc: 0.6440\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094786\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 3.4908 - acc: 0.6451\n",
      "\n",
      " End of Epoch Learning Rate = 0.094551\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.4049 - acc: 0.6571 - val_loss: 3.4908 - val_acc: 0.6451\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094551\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.5079 - acc: 0.6447\n",
      "\n",
      " End of Epoch Learning Rate = 0.094311\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3578 - acc: 0.6595 - val_loss: 3.5079 - val_acc: 0.6447\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094311\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.4774 - acc: 0.6387\n",
      "\n",
      " End of Epoch Learning Rate = 0.094066\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.3309 - acc: 0.6622 - val_loss: 3.4774 - val_acc: 0.6387\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094066\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.5384 - acc: 0.6314\n",
      "\n",
      " End of Epoch Learning Rate = 0.093816\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.3133 - acc: 0.6625 - val_loss: 3.5384 - val_acc: 0.6314\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093816\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.4506 - acc: 0.6486\n",
      "\n",
      " End of Epoch Learning Rate = 0.093561\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.2947 - acc: 0.6636 - val_loss: 3.4506 - val_acc: 0.6486\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093561\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 3.4373 - acc: 0.6430\n",
      "\n",
      " End of Epoch Learning Rate = 0.093302\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2835 - acc: 0.6649 - val_loss: 3.4373 - val_acc: 0.6430\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093302\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 3.4679 - acc: 0.6387\n",
      "\n",
      " End of Epoch Learning Rate = 0.093038\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2873 - acc: 0.6652 - val_loss: 3.4679 - val_acc: 0.6387\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093038\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.4140 - acc: 0.6484\n",
      "\n",
      " End of Epoch Learning Rate = 0.092769\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2638 - acc: 0.6678 - val_loss: 3.4140 - val_acc: 0.6484\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092769\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 3.3670 - acc: 0.6483\n",
      "\n",
      " End of Epoch Learning Rate = 0.092495\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.2610 - acc: 0.6701 - val_loss: 3.3670 - val_acc: 0.6483\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092495\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.4430 - acc: 0.6431\n",
      "\n",
      " End of Epoch Learning Rate = 0.092217\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.2385 - acc: 0.6737 - val_loss: 3.4430 - val_acc: 0.6431\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092217\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.3953 - acc: 0.6417\n",
      "\n",
      " End of Epoch Learning Rate = 0.091934\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2370 - acc: 0.6694 - val_loss: 3.3953 - val_acc: 0.6417\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091934\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.3655 - acc: 0.6578\n",
      "\n",
      " End of Epoch Learning Rate = 0.091647\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2266 - acc: 0.6735 - val_loss: 3.3655 - val_acc: 0.6578\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091647\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.3721 - acc: 0.6539\n",
      "\n",
      " End of Epoch Learning Rate = 0.091355\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.2366 - acc: 0.6721 - val_loss: 3.3721 - val_acc: 0.6539\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091355\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.3755 - acc: 0.6527\n",
      "\n",
      " End of Epoch Learning Rate = 0.091058\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.2268 - acc: 0.6775 - val_loss: 3.3755 - val_acc: 0.6527\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091058\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.4319 - acc: 0.6500\n",
      "\n",
      " End of Epoch Learning Rate = 0.090757\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.2020 - acc: 0.6773 - val_loss: 3.4319 - val_acc: 0.6500\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090757\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 3.3981 - acc: 0.6440\n",
      "\n",
      " End of Epoch Learning Rate = 0.090452\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2324 - acc: 0.6745 - val_loss: 3.3981 - val_acc: 0.6440\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090452\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 3.3957 - acc: 0.6550\n",
      "\n",
      " End of Epoch Learning Rate = 0.090142\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.2206 - acc: 0.6774 - val_loss: 3.3957 - val_acc: 0.6550\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090142\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.3760 - acc: 0.6553\n",
      "\n",
      " End of Epoch Learning Rate = 0.089828\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.2083 - acc: 0.6773 - val_loss: 3.3760 - val_acc: 0.6553\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089828\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.4305 - acc: 0.6481\n",
      "\n",
      " End of Epoch Learning Rate = 0.089509\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.2048 - acc: 0.6778 - val_loss: 3.4305 - val_acc: 0.6481\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089509\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 3.2808 - acc: 0.6620\n",
      "\n",
      " End of Epoch Learning Rate = 0.089186\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.1811 - acc: 0.6817 - val_loss: 3.2808 - val_acc: 0.6620\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089186\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 3.3835 - acc: 0.6511\n",
      "\n",
      " End of Epoch Learning Rate = 0.088858\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1785 - acc: 0.6820 - val_loss: 3.3835 - val_acc: 0.6511\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088858\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 3.2942 - acc: 0.6674\n",
      "\n",
      " End of Epoch Learning Rate = 0.088527\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.1742 - acc: 0.6817 - val_loss: 3.2942 - val_acc: 0.6674\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088527\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 3.2820 - acc: 0.6692\n",
      "\n",
      " End of Epoch Learning Rate = 0.088191\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.1616 - acc: 0.6870 - val_loss: 3.2820 - val_acc: 0.6692\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088191\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 3.2635 - acc: 0.6657\n",
      "\n",
      " End of Epoch Learning Rate = 0.087851\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1522 - acc: 0.6858 - val_loss: 3.2635 - val_acc: 0.6657\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087851\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 3.3192 - acc: 0.6577\n",
      "\n",
      " End of Epoch Learning Rate = 0.087507\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1501 - acc: 0.6867 - val_loss: 3.3192 - val_acc: 0.6577\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087507\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.3349 - acc: 0.6554\n",
      "\n",
      " End of Epoch Learning Rate = 0.087159\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.1440 - acc: 0.6886 - val_loss: 3.3349 - val_acc: 0.6554\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087159\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.3975 - acc: 0.6371\n",
      "\n",
      " End of Epoch Learning Rate = 0.086806\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.1197 - acc: 0.6918 - val_loss: 3.3975 - val_acc: 0.6371\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086806\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 3.3352 - acc: 0.6583\n",
      "\n",
      " End of Epoch Learning Rate = 0.086450\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1212 - acc: 0.6896 - val_loss: 3.3352 - val_acc: 0.6583\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086450\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 3.3360 - acc: 0.6491\n",
      "\n",
      " End of Epoch Learning Rate = 0.086089\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.1143 - acc: 0.6930 - val_loss: 3.3360 - val_acc: 0.6491\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086089\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 3.2943 - acc: 0.6666\n",
      "\n",
      " End of Epoch Learning Rate = 0.085725\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1030 - acc: 0.6934 - val_loss: 3.2943 - val_acc: 0.6666\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085725\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.3087 - acc: 0.6566\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 3.1058 - acc: 0.6924 - val_loss: 3.3087 - val_acc: 0.6566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.2275 - acc: 0.6758\n",
      "\n",
      " End of Epoch Learning Rate = 0.084985\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.1001 - acc: 0.6967 - val_loss: 3.2275 - val_acc: 0.6758\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084985\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.2738 - acc: 0.6670\n",
      "\n",
      " End of Epoch Learning Rate = 0.084609\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.0848 - acc: 0.6976 - val_loss: 3.2738 - val_acc: 0.6670\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084609\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 742us/sample - loss: 3.2640 - acc: 0.6732\n",
      "\n",
      " End of Epoch Learning Rate = 0.084229\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.0829 - acc: 0.6997 - val_loss: 3.2640 - val_acc: 0.6732\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084229\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.2581 - acc: 0.6634\n",
      "\n",
      " End of Epoch Learning Rate = 0.083845\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.0687 - acc: 0.6988 - val_loss: 3.2581 - val_acc: 0.6634\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083845\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 3.7810 - acc: 0.6435\n",
      "\n",
      " End of Epoch Learning Rate = 0.083458\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 3.1106 - acc: 0.6969 - val_loss: 3.7810 - val_acc: 0.6435\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083458\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.6467 - acc: 0.6480\n",
      "\n",
      " End of Epoch Learning Rate = 0.083067\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.4697 - acc: 0.6848 - val_loss: 3.6467 - val_acc: 0.6480\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083067\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.5092 - acc: 0.6633\n",
      "\n",
      " End of Epoch Learning Rate = 0.082673\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.3316 - acc: 0.6977 - val_loss: 3.5092 - val_acc: 0.6633\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082673\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.4403 - acc: 0.6674\n",
      "\n",
      " End of Epoch Learning Rate = 0.082275\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.2389 - acc: 0.7004 - val_loss: 3.4403 - val_acc: 0.6674\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082275\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 3.4430 - acc: 0.6542\n",
      "\n",
      " End of Epoch Learning Rate = 0.081873\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.1778 - acc: 0.7026 - val_loss: 3.4430 - val_acc: 0.6542\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081873\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 3.3123 - acc: 0.6756\n",
      "\n",
      " End of Epoch Learning Rate = 0.081468\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.1328 - acc: 0.7052 - val_loss: 3.3123 - val_acc: 0.6756\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081468\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.3245 - acc: 0.6593\n",
      "\n",
      " End of Epoch Learning Rate = 0.081059\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.0951 - acc: 0.7038 - val_loss: 3.3245 - val_acc: 0.6593\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081059\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 3.3979 - acc: 0.6516\n",
      "\n",
      " End of Epoch Learning Rate = 0.080647\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.0652 - acc: 0.7080 - val_loss: 3.3979 - val_acc: 0.6516\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080647\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.2435 - acc: 0.6678\n",
      "\n",
      " End of Epoch Learning Rate = 0.080232\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.0343 - acc: 0.7097 - val_loss: 3.2435 - val_acc: 0.6678\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080232\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.2143 - acc: 0.6735\n",
      "\n",
      " End of Epoch Learning Rate = 0.079813\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.0179 - acc: 0.7099 - val_loss: 3.2143 - val_acc: 0.6735\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079813\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.2757 - acc: 0.6621\n",
      "\n",
      " End of Epoch Learning Rate = 0.079391\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.9992 - acc: 0.7104 - val_loss: 3.2757 - val_acc: 0.6621\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079391\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.2266 - acc: 0.6763\n",
      "\n",
      " End of Epoch Learning Rate = 0.078966\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 3.0013 - acc: 0.7104 - val_loss: 3.2266 - val_acc: 0.6763\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078966\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 3.1860 - acc: 0.6730\n",
      "\n",
      " End of Epoch Learning Rate = 0.078538\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9748 - acc: 0.7137 - val_loss: 3.1860 - val_acc: 0.6730\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078538\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.3296 - acc: 0.6559\n",
      "\n",
      " End of Epoch Learning Rate = 0.078106\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.9795 - acc: 0.7127 - val_loss: 3.3296 - val_acc: 0.6559\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078106\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 3.2232 - acc: 0.6669\n",
      "\n",
      " End of Epoch Learning Rate = 0.077672\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.9747 - acc: 0.7131 - val_loss: 3.2232 - val_acc: 0.6669\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077672\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 8s 760us/sample - loss: 3.1674 - acc: 0.6804\n",
      "\n",
      " End of Epoch Learning Rate = 0.077234\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9604 - acc: 0.7147 - val_loss: 3.1674 - val_acc: 0.6804\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077234\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.2503 - acc: 0.6553\n",
      "\n",
      " End of Epoch Learning Rate = 0.076794\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9443 - acc: 0.7170 - val_loss: 3.2503 - val_acc: 0.6553\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076794\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 3.2991 - acc: 0.6524\n",
      "\n",
      " End of Epoch Learning Rate = 0.076350\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.9247 - acc: 0.7193 - val_loss: 3.2991 - val_acc: 0.6524\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076350\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 3.1742 - acc: 0.6718\n",
      "\n",
      " End of Epoch Learning Rate = 0.075904\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9241 - acc: 0.7183 - val_loss: 3.1742 - val_acc: 0.6718\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075904\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 3.1754 - acc: 0.6727\n",
      "\n",
      " End of Epoch Learning Rate = 0.075455\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9037 - acc: 0.7209 - val_loss: 3.1754 - val_acc: 0.6727\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075455\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 3.2166 - acc: 0.6647\n",
      "\n",
      " End of Epoch Learning Rate = 0.075002\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.9003 - acc: 0.7211 - val_loss: 3.2166 - val_acc: 0.6647\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075002\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 3.1474 - acc: 0.6802\n",
      "\n",
      " End of Epoch Learning Rate = 0.074548\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.8914 - acc: 0.7223 - val_loss: 3.1474 - val_acc: 0.6802\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074548\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.1799 - acc: 0.6720\n",
      "\n",
      " End of Epoch Learning Rate = 0.074090\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.8696 - acc: 0.7260 - val_loss: 3.1799 - val_acc: 0.6720\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074090\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 3.1911 - acc: 0.6648\n",
      "\n",
      " End of Epoch Learning Rate = 0.073630\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.8565 - acc: 0.7262 - val_loss: 3.1911 - val_acc: 0.6648\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073630\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.1747 - acc: 0.6674\n",
      "\n",
      " End of Epoch Learning Rate = 0.073167\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.8474 - acc: 0.7285 - val_loss: 3.1747 - val_acc: 0.6674\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073167\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.1784 - acc: 0.6669\n",
      "\n",
      " End of Epoch Learning Rate = 0.072702\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.8567 - acc: 0.7244 - val_loss: 3.1784 - val_acc: 0.6669\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072702\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.1622 - acc: 0.6717\n",
      "\n",
      " End of Epoch Learning Rate = 0.072235\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.8394 - acc: 0.7313 - val_loss: 3.1622 - val_acc: 0.6717\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072235\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 3.1303 - acc: 0.6717\n",
      "\n",
      " End of Epoch Learning Rate = 0.071764\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 2.8365 - acc: 0.7314 - val_loss: 3.1303 - val_acc: 0.6717\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071764\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 8s 755us/sample - loss: 3.1131 - acc: 0.6936\n",
      "\n",
      " End of Epoch Learning Rate = 0.071292\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.8258 - acc: 0.7326 - val_loss: 3.1131 - val_acc: 0.6936\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071292\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.1963 - acc: 0.6738\n",
      "\n",
      " End of Epoch Learning Rate = 0.070817\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.8238 - acc: 0.7316 - val_loss: 3.1963 - val_acc: 0.6738\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070817\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.1802 - acc: 0.6613\n",
      "\n",
      " End of Epoch Learning Rate = 0.070340\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.8039 - acc: 0.7372 - val_loss: 3.1802 - val_acc: 0.6613\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070340\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.1678 - acc: 0.6568\n",
      "\n",
      " End of Epoch Learning Rate = 0.069860\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.7930 - acc: 0.7361 - val_loss: 3.1678 - val_acc: 0.6568\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069860\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 3.3628 - acc: 0.6752\n",
      "\n",
      " End of Epoch Learning Rate = 0.069379\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 3.1223 - acc: 0.7167 - val_loss: 3.3628 - val_acc: 0.6752\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069379\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.2677 - acc: 0.6886\n",
      "\n",
      " End of Epoch Learning Rate = 0.068895\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.0129 - acc: 0.7307 - val_loss: 3.2677 - val_acc: 0.6886\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068895\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 3.2609 - acc: 0.6773\n",
      "\n",
      " End of Epoch Learning Rate = 0.068409\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.9407 - acc: 0.7371 - val_loss: 3.2609 - val_acc: 0.6773\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068409\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.3410 - acc: 0.6659\n",
      "\n",
      " End of Epoch Learning Rate = 0.067922\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.8986 - acc: 0.7364 - val_loss: 3.3410 - val_acc: 0.6659\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067922\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 3.1744 - acc: 0.6839\n",
      "\n",
      " End of Epoch Learning Rate = 0.067432\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.8557 - acc: 0.7394 - val_loss: 3.1744 - val_acc: 0.6839\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067432\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 3.1772 - acc: 0.6810\n",
      "\n",
      " End of Epoch Learning Rate = 0.066940\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.8148 - acc: 0.7419 - val_loss: 3.1772 - val_acc: 0.6810\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066940\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 3.1742 - acc: 0.6732\n",
      "\n",
      " End of Epoch Learning Rate = 0.066447\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.7809 - acc: 0.7450 - val_loss: 3.1742 - val_acc: 0.6732\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066447\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 3.0631 - acc: 0.6793\n",
      "\n",
      " End of Epoch Learning Rate = 0.065951\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.7567 - acc: 0.7450 - val_loss: 3.0631 - val_acc: 0.6793\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065951\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 3.1243 - acc: 0.6728\n",
      "\n",
      " End of Epoch Learning Rate = 0.065454\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.7266 - acc: 0.7480 - val_loss: 3.1243 - val_acc: 0.6728\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065454\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 3.1937 - acc: 0.6712\n",
      "\n",
      " End of Epoch Learning Rate = 0.064956\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.7002 - acc: 0.7482 - val_loss: 3.1937 - val_acc: 0.6712\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064956\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 3.1742 - acc: 0.6684\n",
      "\n",
      " End of Epoch Learning Rate = 0.064455\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.6928 - acc: 0.7491 - val_loss: 3.1742 - val_acc: 0.6684\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064455\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 3.1946 - acc: 0.6730\n",
      "\n",
      " End of Epoch Learning Rate = 0.063953\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.7131 - acc: 0.7481 - val_loss: 3.1946 - val_acc: 0.6730\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063953\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 8s 762us/sample - loss: 3.1757 - acc: 0.6625\n",
      "\n",
      " End of Epoch Learning Rate = 0.063450\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.6795 - acc: 0.7519 - val_loss: 3.1757 - val_acc: 0.6625\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063450\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 3.1055 - acc: 0.6757\n",
      "\n",
      " End of Epoch Learning Rate = 0.062945\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.6538 - acc: 0.7536 - val_loss: 3.1055 - val_acc: 0.6757\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062945\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 3.2045 - acc: 0.6616\n",
      "\n",
      " End of Epoch Learning Rate = 0.062438\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.6634 - acc: 0.7506 - val_loss: 3.2045 - val_acc: 0.6616\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062438\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.9397 - acc: 0.6994\n",
      "\n",
      " End of Epoch Learning Rate = 0.061930\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.6390 - acc: 0.7548 - val_loss: 2.9397 - val_acc: 0.6994\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061930\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 2.9504 - acc: 0.6915\n",
      "\n",
      " End of Epoch Learning Rate = 0.061421\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.6002 - acc: 0.7613 - val_loss: 2.9504 - val_acc: 0.6915\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061421\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.9563 - acc: 0.7016\n",
      "\n",
      " End of Epoch Learning Rate = 0.060911\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.6016 - acc: 0.7573 - val_loss: 2.9563 - val_acc: 0.7016\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060911\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 732us/sample - loss: 3.0720 - acc: 0.6884\n",
      "\n",
      " End of Epoch Learning Rate = 0.060400\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.6086 - acc: 0.7584 - val_loss: 3.0720 - val_acc: 0.6884\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060400\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 3.0159 - acc: 0.6880\n",
      "\n",
      " End of Epoch Learning Rate = 0.059887\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.5840 - acc: 0.7618 - val_loss: 3.0159 - val_acc: 0.6880\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059887\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 2.9719 - acc: 0.6964\n",
      "\n",
      " End of Epoch Learning Rate = 0.059373\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.5557 - acc: 0.7657 - val_loss: 2.9719 - val_acc: 0.6964\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059373\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 3.1029 - acc: 0.6646\n",
      "\n",
      " End of Epoch Learning Rate = 0.058858\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.5591 - acc: 0.7611 - val_loss: 3.1029 - val_acc: 0.6646\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058858\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 3.0763 - acc: 0.6885\n",
      "\n",
      " End of Epoch Learning Rate = 0.058343\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.5377 - acc: 0.7662 - val_loss: 3.0763 - val_acc: 0.6885\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058343\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 2.9428 - acc: 0.6888\n",
      "\n",
      " End of Epoch Learning Rate = 0.057826\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.5194 - acc: 0.7697 - val_loss: 2.9428 - val_acc: 0.6888\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057826\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 2.9811 - acc: 0.6851\n",
      "\n",
      " End of Epoch Learning Rate = 0.057308\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.4893 - acc: 0.7713 - val_loss: 2.9811 - val_acc: 0.6851\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057308\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 2.9882 - acc: 0.6951\n",
      "\n",
      " End of Epoch Learning Rate = 0.056790\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.4671 - acc: 0.7736 - val_loss: 2.9882 - val_acc: 0.6951\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056790\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 3.0044 - acc: 0.6799\n",
      "\n",
      " End of Epoch Learning Rate = 0.056271\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.4548 - acc: 0.7737 - val_loss: 3.0044 - val_acc: 0.6799\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056271\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.9264 - acc: 0.6845\n",
      "\n",
      " End of Epoch Learning Rate = 0.055751\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.4399 - acc: 0.7764 - val_loss: 2.9264 - val_acc: 0.6845\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055751\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.8961 - acc: 0.6950\n",
      "\n",
      " End of Epoch Learning Rate = 0.055231\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.4289 - acc: 0.7771 - val_loss: 2.8961 - val_acc: 0.6950\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055231\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.9962 - acc: 0.6863\n",
      "\n",
      " End of Epoch Learning Rate = 0.054710\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.4138 - acc: 0.7764 - val_loss: 2.9962 - val_acc: 0.6863\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054710\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.8922 - acc: 0.6991\n",
      "\n",
      " End of Epoch Learning Rate = 0.054188\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.4140 - acc: 0.7758 - val_loss: 2.8922 - val_acc: 0.6991\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054188\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 2.9402 - acc: 0.6859\n",
      "\n",
      " End of Epoch Learning Rate = 0.053667\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.3862 - acc: 0.7831 - val_loss: 2.9402 - val_acc: 0.6859\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053667\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.8172 - acc: 0.6984\n",
      "\n",
      " End of Epoch Learning Rate = 0.053144\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3881 - acc: 0.7801 - val_loss: 2.8172 - val_acc: 0.6984\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053144\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.8961 - acc: 0.6942\n",
      "\n",
      " End of Epoch Learning Rate = 0.052622\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3682 - acc: 0.7834 - val_loss: 2.8961 - val_acc: 0.6942\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052622\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.9438 - acc: 0.6838\n",
      "\n",
      " End of Epoch Learning Rate = 0.052099\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3529 - acc: 0.7876 - val_loss: 2.9438 - val_acc: 0.6838\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052099\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.8815 - acc: 0.6870\n",
      "\n",
      " End of Epoch Learning Rate = 0.051575\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3353 - acc: 0.7874 - val_loss: 2.8815 - val_acc: 0.6870\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051575\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.8399 - acc: 0.7059\n",
      "\n",
      " End of Epoch Learning Rate = 0.051052\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3140 - acc: 0.7914 - val_loss: 2.8399 - val_acc: 0.7059\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051052\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.9124 - acc: 0.6861\n",
      "\n",
      " End of Epoch Learning Rate = 0.050529\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3096 - acc: 0.7874 - val_loss: 2.9124 - val_acc: 0.6861\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050529\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.9893 - acc: 0.6999\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.2956 - acc: 0.7900 - val_loss: 2.9893 - val_acc: 0.6999\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.8720 - acc: 0.6957\n",
      "\n",
      " End of Epoch Learning Rate = 0.049481\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2849 - acc: 0.7927 - val_loss: 2.8720 - val_acc: 0.6957\n",
      "\n",
      " Start of Epoch Learning Rate = 0.049481\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.8208 - acc: 0.6981\n",
      "\n",
      " End of Epoch Learning Rate = 0.048958\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.2488 - acc: 0.7978 - val_loss: 2.8208 - val_acc: 0.6981\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048958\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.8257 - acc: 0.6936\n",
      "\n",
      " End of Epoch Learning Rate = 0.048435\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2413 - acc: 0.7981 - val_loss: 2.8257 - val_acc: 0.6936\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048435\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.7961 - acc: 0.6974\n",
      "\n",
      " End of Epoch Learning Rate = 0.047911\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.2375 - acc: 0.7989 - val_loss: 2.7961 - val_acc: 0.6974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047911\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.8853 - acc: 0.6855\n",
      "\n",
      " End of Epoch Learning Rate = 0.047388\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2240 - acc: 0.7995 - val_loss: 2.8853 - val_acc: 0.6855\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047388\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.8593 - acc: 0.6908\n",
      "\n",
      " End of Epoch Learning Rate = 0.046866\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.2200 - acc: 0.7981 - val_loss: 2.8593 - val_acc: 0.6908\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046866\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 2.9120 - acc: 0.6845\n",
      "\n",
      " End of Epoch Learning Rate = 0.046343\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.1881 - acc: 0.8055 - val_loss: 2.9120 - val_acc: 0.6845\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046343\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.8033 - acc: 0.7020\n",
      "\n",
      " End of Epoch Learning Rate = 0.045822\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.1603 - acc: 0.8093 - val_loss: 2.8033 - val_acc: 0.7020\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045822\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.8072 - acc: 0.6989\n",
      "\n",
      " End of Epoch Learning Rate = 0.045300\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.1503 - acc: 0.8090 - val_loss: 2.8072 - val_acc: 0.6989\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045300\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 2.8382 - acc: 0.6945\n",
      "\n",
      " End of Epoch Learning Rate = 0.044779\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.1399 - acc: 0.8098 - val_loss: 2.8382 - val_acc: 0.6945\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044779\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 2.8159 - acc: 0.6922\n",
      "\n",
      " End of Epoch Learning Rate = 0.044259\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.1231 - acc: 0.8117 - val_loss: 2.8159 - val_acc: 0.6922\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044259\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.7215 - acc: 0.7059\n",
      "\n",
      " End of Epoch Learning Rate = 0.043739\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.1103 - acc: 0.8134 - val_loss: 2.7215 - val_acc: 0.7059\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043739\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.7265 - acc: 0.6974\n",
      "\n",
      " End of Epoch Learning Rate = 0.043220\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.0934 - acc: 0.8144 - val_loss: 2.7265 - val_acc: 0.6974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043220\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 2.9094 - acc: 0.6954\n",
      "\n",
      " End of Epoch Learning Rate = 0.042702\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.0800 - acc: 0.8138 - val_loss: 2.9094 - val_acc: 0.6954\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042702\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.7539 - acc: 0.7008\n",
      "\n",
      " End of Epoch Learning Rate = 0.042184\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.0674 - acc: 0.8176 - val_loss: 2.7539 - val_acc: 0.7008\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042184\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.7083 - acc: 0.6973\n",
      "\n",
      " End of Epoch Learning Rate = 0.041667\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.0611 - acc: 0.8156 - val_loss: 2.7083 - val_acc: 0.6973\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041667\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.7445 - acc: 0.6939\n",
      "\n",
      " End of Epoch Learning Rate = 0.041152\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.0217 - acc: 0.8225 - val_loss: 2.7445 - val_acc: 0.6939\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041152\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 2.6406 - acc: 0.7127\n",
      "\n",
      " End of Epoch Learning Rate = 0.040637\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 2.0058 - acc: 0.8242 - val_loss: 2.6406 - val_acc: 0.7127\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040637\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 2.5957 - acc: 0.7161\n",
      "\n",
      " End of Epoch Learning Rate = 0.040123\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.9879 - acc: 0.8249 - val_loss: 2.5957 - val_acc: 0.7161\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040123\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.6461 - acc: 0.7097\n",
      "\n",
      " End of Epoch Learning Rate = 0.039610\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.9729 - acc: 0.8273 - val_loss: 2.6461 - val_acc: 0.7097\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039610\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 2.6243 - acc: 0.6953\n",
      "\n",
      " End of Epoch Learning Rate = 0.039099\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9519 - acc: 0.8304 - val_loss: 2.6243 - val_acc: 0.6953\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039099\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 2.8277 - acc: 0.6802\n",
      "\n",
      " End of Epoch Learning Rate = 0.038589\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.9237 - acc: 0.8343 - val_loss: 2.8277 - val_acc: 0.6802\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038589\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.6031 - acc: 0.7063\n",
      "\n",
      " End of Epoch Learning Rate = 0.038080\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.9056 - acc: 0.8347 - val_loss: 2.6031 - val_acc: 0.7063\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038080\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 2.7606 - acc: 0.7071\n",
      "\n",
      " End of Epoch Learning Rate = 0.037572\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8937 - acc: 0.8352 - val_loss: 2.7606 - val_acc: 0.7071\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037572\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.6542 - acc: 0.6964\n",
      "\n",
      " End of Epoch Learning Rate = 0.037065\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.9029 - acc: 0.8305 - val_loss: 2.6542 - val_acc: 0.6964\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037065\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.7018 - acc: 0.7105\n",
      "\n",
      " End of Epoch Learning Rate = 0.036560\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.8602 - acc: 0.8402 - val_loss: 2.7018 - val_acc: 0.7105\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036560\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.5434 - acc: 0.7070\n",
      "\n",
      " End of Epoch Learning Rate = 0.036057\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.8484 - acc: 0.8380 - val_loss: 2.5434 - val_acc: 0.7070\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036057\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 2.6111 - acc: 0.7044\n",
      "\n",
      " End of Epoch Learning Rate = 0.035555\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.8387 - acc: 0.8393 - val_loss: 2.6111 - val_acc: 0.7044\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035555\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.5732 - acc: 0.7051\n",
      "\n",
      " End of Epoch Learning Rate = 0.035054\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.8166 - acc: 0.8437 - val_loss: 2.5732 - val_acc: 0.7051\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035054\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 2.5861 - acc: 0.7201\n",
      "\n",
      " End of Epoch Learning Rate = 0.034556\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.7892 - acc: 0.8475 - val_loss: 2.5861 - val_acc: 0.7201\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034556\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.4572 - acc: 0.7145\n",
      "\n",
      " End of Epoch Learning Rate = 0.034059\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.7725 - acc: 0.8460 - val_loss: 2.4572 - val_acc: 0.7145\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034059\n",
      "Epoch 182/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 728us/sample - loss: 2.5944 - acc: 0.7073\n",
      "\n",
      " End of Epoch Learning Rate = 0.033563\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.7349 - acc: 0.8541 - val_loss: 2.5944 - val_acc: 0.7073\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033563\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 2.6267 - acc: 0.7017\n",
      "\n",
      " End of Epoch Learning Rate = 0.033070\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.7187 - acc: 0.8556 - val_loss: 2.6267 - val_acc: 0.7017\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033070\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.6231 - acc: 0.6985\n",
      "\n",
      " End of Epoch Learning Rate = 0.032578\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.7137 - acc: 0.8526 - val_loss: 2.6231 - val_acc: 0.6985\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032578\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.4731 - acc: 0.7132\n",
      "\n",
      " End of Epoch Learning Rate = 0.032088\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.6975 - acc: 0.8537 - val_loss: 2.4731 - val_acc: 0.7132\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032088\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.5469 - acc: 0.7068\n",
      "\n",
      " End of Epoch Learning Rate = 0.031601\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.6694 - acc: 0.8571 - val_loss: 2.5469 - val_acc: 0.7068\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031601\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.5171 - acc: 0.7092\n",
      "\n",
      " End of Epoch Learning Rate = 0.031115\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.6495 - acc: 0.8597 - val_loss: 2.5171 - val_acc: 0.7092\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031115\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.5870 - acc: 0.7089\n",
      "\n",
      " End of Epoch Learning Rate = 0.030631\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.6283 - acc: 0.8615 - val_loss: 2.5870 - val_acc: 0.7089\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030631\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.5868 - acc: 0.7152\n",
      "\n",
      " End of Epoch Learning Rate = 0.030150\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.6157 - acc: 0.8614 - val_loss: 2.5868 - val_acc: 0.7152\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030150\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 2.4496 - acc: 0.7170\n",
      "\n",
      " End of Epoch Learning Rate = 0.029670\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5947 - acc: 0.8639 - val_loss: 2.4496 - val_acc: 0.7170\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029670\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.3994 - acc: 0.7170\n",
      "\n",
      " End of Epoch Learning Rate = 0.029193\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5759 - acc: 0.8652 - val_loss: 2.3994 - val_acc: 0.7170\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029193\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.4476 - acc: 0.7141\n",
      "\n",
      " End of Epoch Learning Rate = 0.028718\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5441 - acc: 0.8719 - val_loss: 2.4476 - val_acc: 0.7141\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028718\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 2.4247 - acc: 0.7139\n",
      "\n",
      " End of Epoch Learning Rate = 0.028246\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.5252 - acc: 0.8711 - val_loss: 2.4247 - val_acc: 0.7139\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028246\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 2.4491 - acc: 0.7213\n",
      "\n",
      " End of Epoch Learning Rate = 0.027775\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.5172 - acc: 0.8713 - val_loss: 2.4491 - val_acc: 0.7213\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027775\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.4600 - acc: 0.7164\n",
      "\n",
      " End of Epoch Learning Rate = 0.027308\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.4883 - acc: 0.8739 - val_loss: 2.4600 - val_acc: 0.7164\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027308\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.4111 - acc: 0.7146\n",
      "\n",
      " End of Epoch Learning Rate = 0.026843\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.4783 - acc: 0.8733 - val_loss: 2.4111 - val_acc: 0.7146\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026843\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.3661 - acc: 0.7162\n",
      "\n",
      " End of Epoch Learning Rate = 0.026380\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.4597 - acc: 0.8752 - val_loss: 2.3661 - val_acc: 0.7162\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026380\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 2.3500 - acc: 0.7180\n",
      "\n",
      " End of Epoch Learning Rate = 0.025920\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4249 - acc: 0.8810 - val_loss: 2.3500 - val_acc: 0.7180\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025920\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.3978 - acc: 0.7118\n",
      "\n",
      " End of Epoch Learning Rate = 0.025462\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.4079 - acc: 0.8823 - val_loss: 2.3978 - val_acc: 0.7118\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025462\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.5174 - acc: 0.6943\n",
      "\n",
      " End of Epoch Learning Rate = 0.025007\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.3952 - acc: 0.8824 - val_loss: 2.5174 - val_acc: 0.6943\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025007\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.4317 - acc: 0.7222\n",
      "\n",
      " End of Epoch Learning Rate = 0.024555\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.3665 - acc: 0.8858 - val_loss: 2.4317 - val_acc: 0.7222\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024555\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 2.3235 - acc: 0.7182\n",
      "\n",
      " End of Epoch Learning Rate = 0.024106\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3521 - acc: 0.8870 - val_loss: 2.3235 - val_acc: 0.7182\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024106\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 2.3253 - acc: 0.7230\n",
      "\n",
      " End of Epoch Learning Rate = 0.023660\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.3389 - acc: 0.8865 - val_loss: 2.3253 - val_acc: 0.7230\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023660\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 2.3601 - acc: 0.7227\n",
      "\n",
      " End of Epoch Learning Rate = 0.023216\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.3190 - acc: 0.8871 - val_loss: 2.3601 - val_acc: 0.7227\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023216\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.3961 - acc: 0.7147\n",
      "\n",
      " End of Epoch Learning Rate = 0.022776\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.2917 - acc: 0.8929 - val_loss: 2.3961 - val_acc: 0.7147\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022776\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.3497 - acc: 0.7167\n",
      "\n",
      " End of Epoch Learning Rate = 0.022338\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.2703 - acc: 0.8945 - val_loss: 2.3497 - val_acc: 0.7167\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022338\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.3211 - acc: 0.7241\n",
      "\n",
      " End of Epoch Learning Rate = 0.021904\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.2548 - acc: 0.8945 - val_loss: 2.3211 - val_acc: 0.7241\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021904\n",
      "Epoch 208/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.2865 - acc: 0.7215\n",
      "\n",
      " End of Epoch Learning Rate = 0.021472\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2284 - acc: 0.8996 - val_loss: 2.2865 - val_acc: 0.7215\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021472\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.3469 - acc: 0.7212\n",
      "\n",
      " End of Epoch Learning Rate = 0.021044\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2184 - acc: 0.8995 - val_loss: 2.3469 - val_acc: 0.7212\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021044\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 2.0876 - acc: 0.7250\n",
      "\n",
      " End of Epoch Learning Rate = 0.020619\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2009 - acc: 0.8995 - val_loss: 2.0876 - val_acc: 0.7250\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020619\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 2.2716 - acc: 0.7266\n",
      "\n",
      " End of Epoch Learning Rate = 0.020197\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.1723 - acc: 0.9035 - val_loss: 2.2716 - val_acc: 0.7266\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020197\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.1944 - acc: 0.7299\n",
      "\n",
      " End of Epoch Learning Rate = 0.019778\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1589 - acc: 0.9034 - val_loss: 2.1944 - val_acc: 0.7299\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019778\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.2357 - acc: 0.7252\n",
      "\n",
      " End of Epoch Learning Rate = 0.019363\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 1.1358 - acc: 0.9058 - val_loss: 2.2357 - val_acc: 0.7252\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019363\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.2043 - acc: 0.7193\n",
      "\n",
      " End of Epoch Learning Rate = 0.018951\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.1231 - acc: 0.9066 - val_loss: 2.2043 - val_acc: 0.7193\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018951\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.1425 - acc: 0.7338\n",
      "\n",
      " End of Epoch Learning Rate = 0.018542\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0987 - acc: 0.9095 - val_loss: 2.1425 - val_acc: 0.7338\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018542\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 2.2353 - acc: 0.7226\n",
      "\n",
      " End of Epoch Learning Rate = 0.018137\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.0742 - acc: 0.9137 - val_loss: 2.2353 - val_acc: 0.7226\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018137\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 2.3028 - acc: 0.7203\n",
      "\n",
      " End of Epoch Learning Rate = 0.017735\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0533 - acc: 0.9153 - val_loss: 2.3028 - val_acc: 0.7203\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017735\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.2788 - acc: 0.7178\n",
      "\n",
      " End of Epoch Learning Rate = 0.017337\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0362 - acc: 0.9167 - val_loss: 2.2788 - val_acc: 0.7178\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017337\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 2.1468 - acc: 0.7270\n",
      "\n",
      " End of Epoch Learning Rate = 0.016943\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0189 - acc: 0.9172 - val_loss: 2.1468 - val_acc: 0.7270\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016943\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.1385 - acc: 0.7318\n",
      "\n",
      " End of Epoch Learning Rate = 0.016552\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.9982 - acc: 0.9185 - val_loss: 2.1385 - val_acc: 0.7318\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016552\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.1540 - acc: 0.7220\n",
      "\n",
      " End of Epoch Learning Rate = 0.016165\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.9951 - acc: 0.9168 - val_loss: 2.1540 - val_acc: 0.7220\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016165\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.1261 - acc: 0.7358\n",
      "\n",
      " End of Epoch Learning Rate = 0.015781\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.9602 - acc: 0.9230 - val_loss: 2.1261 - val_acc: 0.7358\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015781\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.1987 - acc: 0.7247\n",
      "\n",
      " End of Epoch Learning Rate = 0.015401\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9442 - acc: 0.9240 - val_loss: 2.1987 - val_acc: 0.7247\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015401\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0657 - acc: 0.7340\n",
      "\n",
      " End of Epoch Learning Rate = 0.015025\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.9304 - acc: 0.9256 - val_loss: 2.0657 - val_acc: 0.7340\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015025\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 2.0254 - acc: 0.7387\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.9172 - acc: 0.9250 - val_loss: 2.0254 - val_acc: 0.7387\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 2.0569 - acc: 0.7332\n",
      "\n",
      " End of Epoch Learning Rate = 0.014285\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8956 - acc: 0.9296 - val_loss: 2.0569 - val_acc: 0.7332\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014285\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 2.0619 - acc: 0.7357\n",
      "\n",
      " End of Epoch Learning Rate = 0.013921\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8762 - acc: 0.9307 - val_loss: 2.0619 - val_acc: 0.7357\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013921\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 2.1146 - acc: 0.7398\n",
      "\n",
      " End of Epoch Learning Rate = 0.013560\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.8583 - acc: 0.9320 - val_loss: 2.1146 - val_acc: 0.7398\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013560\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.1495 - acc: 0.7323\n",
      "\n",
      " End of Epoch Learning Rate = 0.013204\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8423 - acc: 0.9344 - val_loss: 2.1495 - val_acc: 0.7323\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013204\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.1130 - acc: 0.7381\n",
      "\n",
      " End of Epoch Learning Rate = 0.012851\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8340 - acc: 0.9324 - val_loss: 2.1130 - val_acc: 0.7381\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012851\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.0915 - acc: 0.7413\n",
      "\n",
      " End of Epoch Learning Rate = 0.012503\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8048 - acc: 0.9395 - val_loss: 2.0915 - val_acc: 0.7413\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012503\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.1666 - acc: 0.7369\n",
      "\n",
      " End of Epoch Learning Rate = 0.012159\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7892 - acc: 0.9393 - val_loss: 2.1666 - val_acc: 0.7369\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012159\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.9610 - acc: 0.7429\n",
      "\n",
      " End of Epoch Learning Rate = 0.011819\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7754 - acc: 0.9396 - val_loss: 1.9610 - val_acc: 0.7429\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011819\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 746us/sample - loss: 2.1123 - acc: 0.7346\n",
      "\n",
      " End of Epoch Learning Rate = 0.011483\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.7621 - acc: 0.9411 - val_loss: 2.1123 - val_acc: 0.7346\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011483\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 2.1304 - acc: 0.7371\n",
      "\n",
      " End of Epoch Learning Rate = 0.011152\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7465 - acc: 0.9421 - val_loss: 2.1304 - val_acc: 0.7371\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011152\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 2.0054 - acc: 0.7353\n",
      "\n",
      " End of Epoch Learning Rate = 0.010824\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7280 - acc: 0.9453 - val_loss: 2.0054 - val_acc: 0.7353\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010824\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 2.0408 - acc: 0.7389\n",
      "\n",
      " End of Epoch Learning Rate = 0.010501\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.7150 - acc: 0.9454 - val_loss: 2.0408 - val_acc: 0.7389\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010501\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.0188 - acc: 0.7447\n",
      "\n",
      " End of Epoch Learning Rate = 0.010182\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6952 - acc: 0.9490 - val_loss: 2.0188 - val_acc: 0.7447\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010182\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.0731 - acc: 0.7428\n",
      "\n",
      " End of Epoch Learning Rate = 0.009868\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6825 - acc: 0.9496 - val_loss: 2.0731 - val_acc: 0.7428\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009868\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 2.0235 - acc: 0.7428\n",
      "\n",
      " End of Epoch Learning Rate = 0.009558\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.6772 - acc: 0.9497 - val_loss: 2.0235 - val_acc: 0.7428\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009558\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.9344 - acc: 0.7511\n",
      "\n",
      " End of Epoch Learning Rate = 0.009253\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6621 - acc: 0.9497 - val_loss: 1.9344 - val_acc: 0.7511\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009253\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.9109 - acc: 0.7453\n",
      "\n",
      " End of Epoch Learning Rate = 0.008952\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6480 - acc: 0.9515 - val_loss: 1.9109 - val_acc: 0.7453\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008952\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 1.9440 - acc: 0.7458\n",
      "\n",
      " End of Epoch Learning Rate = 0.008655\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6308 - acc: 0.9551 - val_loss: 1.9440 - val_acc: 0.7458\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008655\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.8990 - acc: 0.7537\n",
      "\n",
      " End of Epoch Learning Rate = 0.008363\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6244 - acc: 0.9543 - val_loss: 1.8990 - val_acc: 0.7537\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008363\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 2.1053 - acc: 0.7500\n",
      "\n",
      " End of Epoch Learning Rate = 0.008076\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6054 - acc: 0.9573 - val_loss: 2.1053 - val_acc: 0.7500\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008076\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.9011 - acc: 0.7537\n",
      "\n",
      " End of Epoch Learning Rate = 0.007793\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5966 - acc: 0.9570 - val_loss: 1.9011 - val_acc: 0.7537\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007793\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0023 - acc: 0.7534\n",
      "\n",
      " End of Epoch Learning Rate = 0.007515\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5765 - acc: 0.9611 - val_loss: 2.0023 - val_acc: 0.7534\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007515\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 8s 750us/sample - loss: 1.8974 - acc: 0.7615\n",
      "\n",
      " End of Epoch Learning Rate = 0.007241\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5715 - acc: 0.9606 - val_loss: 1.8974 - val_acc: 0.7615\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007241\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 1.9391 - acc: 0.7576\n",
      "\n",
      " End of Epoch Learning Rate = 0.006972\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5514 - acc: 0.9632 - val_loss: 1.9391 - val_acc: 0.7576\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006972\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 1.9638 - acc: 0.7526\n",
      "\n",
      " End of Epoch Learning Rate = 0.006708\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5478 - acc: 0.9629 - val_loss: 1.9638 - val_acc: 0.7526\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006708\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 8s 751us/sample - loss: 2.0341 - acc: 0.7536\n",
      "\n",
      " End of Epoch Learning Rate = 0.006449\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5295 - acc: 0.9660 - val_loss: 2.0341 - val_acc: 0.7536\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006449\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.8212 - acc: 0.7595\n",
      "\n",
      " End of Epoch Learning Rate = 0.006194\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5295 - acc: 0.9642 - val_loss: 1.8212 - val_acc: 0.7595\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006194\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 2.0333 - acc: 0.7508\n",
      "\n",
      " End of Epoch Learning Rate = 0.005944\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5134 - acc: 0.9670 - val_loss: 2.0333 - val_acc: 0.7508\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005944\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0110 - acc: 0.7598\n",
      "\n",
      " End of Epoch Learning Rate = 0.005699\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5080 - acc: 0.9664 - val_loss: 2.0110 - val_acc: 0.7598\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005699\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 2.0385 - acc: 0.7510\n",
      "\n",
      " End of Epoch Learning Rate = 0.005459\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4923 - acc: 0.9699 - val_loss: 2.0385 - val_acc: 0.7510\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005459\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.9934 - acc: 0.7603\n",
      "\n",
      " End of Epoch Learning Rate = 0.005224\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.4884 - acc: 0.9688 - val_loss: 1.9934 - val_acc: 0.7603\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005224\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 1.8836 - acc: 0.7559\n",
      "\n",
      " End of Epoch Learning Rate = 0.004994\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.4754 - acc: 0.9710 - val_loss: 1.8836 - val_acc: 0.7559\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004994\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.8865 - acc: 0.7566\n",
      "\n",
      " End of Epoch Learning Rate = 0.004768\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.4727 - acc: 0.9706 - val_loss: 1.8865 - val_acc: 0.7566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004768\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 8s 755us/sample - loss: 1.8988 - acc: 0.7561\n",
      "\n",
      " End of Epoch Learning Rate = 0.004548\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4618 - acc: 0.9730 - val_loss: 1.8988 - val_acc: 0.7561\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004548\n",
      "Epoch 260/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.9071 - acc: 0.7653\n",
      "\n",
      " End of Epoch Learning Rate = 0.004332\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4497 - acc: 0.9745 - val_loss: 1.9071 - val_acc: 0.7653\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004332\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 1.8125 - acc: 0.7619\n",
      "\n",
      " End of Epoch Learning Rate = 0.004122\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4442 - acc: 0.9752 - val_loss: 1.8125 - val_acc: 0.7619\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004122\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 1.9245 - acc: 0.7694\n",
      "\n",
      " End of Epoch Learning Rate = 0.003916\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4377 - acc: 0.9753 - val_loss: 1.9245 - val_acc: 0.7694\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003916\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 1.9422 - acc: 0.7672\n",
      "\n",
      " End of Epoch Learning Rate = 0.003716\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4326 - acc: 0.9759 - val_loss: 1.9422 - val_acc: 0.7672\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003716\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 1.9665 - acc: 0.7695\n",
      "\n",
      " End of Epoch Learning Rate = 0.003521\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4247 - acc: 0.9770 - val_loss: 1.9665 - val_acc: 0.7695\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003521\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 2.0218 - acc: 0.7628\n",
      "\n",
      " End of Epoch Learning Rate = 0.003331\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4183 - acc: 0.9782 - val_loss: 2.0218 - val_acc: 0.7628\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003331\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 1.9458 - acc: 0.7674\n",
      "\n",
      " End of Epoch Learning Rate = 0.003146\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4134 - acc: 0.9776 - val_loss: 1.9458 - val_acc: 0.7674\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003146\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9577 - acc: 0.7712\n",
      "\n",
      " End of Epoch Learning Rate = 0.002966\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4023 - acc: 0.9806 - val_loss: 1.9577 - val_acc: 0.7712\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002966\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.0313 - acc: 0.7674\n",
      "\n",
      " End of Epoch Learning Rate = 0.002791\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.4019 - acc: 0.9796 - val_loss: 2.0313 - val_acc: 0.7674\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002791\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.9629 - acc: 0.7627\n",
      "\n",
      " End of Epoch Learning Rate = 0.002621\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3931 - acc: 0.9812 - val_loss: 1.9629 - val_acc: 0.7627\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002621\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 1.9650 - acc: 0.7714\n",
      "\n",
      " End of Epoch Learning Rate = 0.002457\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3876 - acc: 0.9823 - val_loss: 1.9650 - val_acc: 0.7714\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002457\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.9910 - acc: 0.7748\n",
      "\n",
      " End of Epoch Learning Rate = 0.002298\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.3871 - acc: 0.9810 - val_loss: 1.9910 - val_acc: 0.7748\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002298\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9242 - acc: 0.7695\n",
      "\n",
      " End of Epoch Learning Rate = 0.002144\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3825 - acc: 0.9816 - val_loss: 1.9242 - val_acc: 0.7695\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002144\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9792 - acc: 0.7715\n",
      "\n",
      " End of Epoch Learning Rate = 0.001995\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3766 - acc: 0.9827 - val_loss: 1.9792 - val_acc: 0.7715\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001995\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 1.8937 - acc: 0.7685\n",
      "\n",
      " End of Epoch Learning Rate = 0.001852\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3743 - acc: 0.9833 - val_loss: 1.8937 - val_acc: 0.7685\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001852\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.9445 - acc: 0.7723\n",
      "\n",
      " End of Epoch Learning Rate = 0.001714\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3685 - acc: 0.9838 - val_loss: 1.9445 - val_acc: 0.7723\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001714\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9569 - acc: 0.7736\n",
      "\n",
      " End of Epoch Learning Rate = 0.001581\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3642 - acc: 0.9843 - val_loss: 1.9569 - val_acc: 0.7736\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001581\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 2.0091 - acc: 0.7748\n",
      "\n",
      " End of Epoch Learning Rate = 0.001453\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3623 - acc: 0.9842 - val_loss: 2.0091 - val_acc: 0.7748\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001453\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.0022 - acc: 0.7759\n",
      "\n",
      " End of Epoch Learning Rate = 0.001331\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3584 - acc: 0.9856 - val_loss: 2.0022 - val_acc: 0.7759\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001331\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.0060 - acc: 0.7762\n",
      "\n",
      " End of Epoch Learning Rate = 0.001214\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3555 - acc: 0.9856 - val_loss: 2.0060 - val_acc: 0.7762\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001214\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 1.9962 - acc: 0.7776\n",
      "\n",
      " End of Epoch Learning Rate = 0.001103\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3501 - acc: 0.9869 - val_loss: 1.9962 - val_acc: 0.7776\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001103\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 2.0024 - acc: 0.7791\n",
      "\n",
      " End of Epoch Learning Rate = 0.000996\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.3485 - acc: 0.9870 - val_loss: 2.0024 - val_acc: 0.7791\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000996\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 2.0239 - acc: 0.7750\n",
      "\n",
      " End of Epoch Learning Rate = 0.000896\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3481 - acc: 0.9868 - val_loss: 2.0239 - val_acc: 0.7750\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000896\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 1.9556 - acc: 0.7801\n",
      "\n",
      " End of Epoch Learning Rate = 0.000800\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3450 - acc: 0.9878 - val_loss: 1.9556 - val_acc: 0.7801\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000800\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 8s 757us/sample - loss: 1.9302 - acc: 0.7794\n",
      "\n",
      " End of Epoch Learning Rate = 0.000710\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3461 - acc: 0.9875 - val_loss: 1.9302 - val_acc: 0.7794\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000710\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 8s 750us/sample - loss: 1.9762 - acc: 0.7764\n",
      "\n",
      " End of Epoch Learning Rate = 0.000626\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3428 - acc: 0.9873 - val_loss: 1.9762 - val_acc: 0.7764\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000626\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 740us/sample - loss: 1.9996 - acc: 0.7796\n",
      "\n",
      " End of Epoch Learning Rate = 0.000546\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3426 - acc: 0.9877 - val_loss: 1.9996 - val_acc: 0.7796\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000546\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9661 - acc: 0.7789\n",
      "\n",
      " End of Epoch Learning Rate = 0.000473\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.3419 - acc: 0.9882 - val_loss: 1.9661 - val_acc: 0.7789\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000473\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 2.0152 - acc: 0.7825\n",
      "\n",
      " End of Epoch Learning Rate = 0.000404\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3382 - acc: 0.9889 - val_loss: 2.0152 - val_acc: 0.7825\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000404\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.0272 - acc: 0.7804\n",
      "\n",
      " End of Epoch Learning Rate = 0.000341\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3377 - acc: 0.9885 - val_loss: 2.0272 - val_acc: 0.7804\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000341\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.0052 - acc: 0.7841\n",
      "\n",
      " End of Epoch Learning Rate = 0.000284\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.3350 - acc: 0.9894 - val_loss: 2.0052 - val_acc: 0.7841\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000284\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 1.9919 - acc: 0.7825\n",
      "\n",
      " End of Epoch Learning Rate = 0.000232\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.3362 - acc: 0.9888 - val_loss: 1.9919 - val_acc: 0.7825\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000232\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0077 - acc: 0.7838\n",
      "\n",
      " End of Epoch Learning Rate = 0.000185\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3364 - acc: 0.9888 - val_loss: 2.0077 - val_acc: 0.7838\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000185\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 2.0003 - acc: 0.7817\n",
      "\n",
      " End of Epoch Learning Rate = 0.000144\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3346 - acc: 0.9896 - val_loss: 2.0003 - val_acc: 0.7817\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000144\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 1.9827 - acc: 0.7818\n",
      "\n",
      " End of Epoch Learning Rate = 0.000109\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3332 - acc: 0.9894 - val_loss: 1.9827 - val_acc: 0.7818\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000109\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0008 - acc: 0.7849\n",
      "\n",
      " End of Epoch Learning Rate = 0.000079\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3324 - acc: 0.9895 - val_loss: 2.0008 - val_acc: 0.7849\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000079\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 1.9862 - acc: 0.7848\n",
      "\n",
      " End of Epoch Learning Rate = 0.000054\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3330 - acc: 0.9896 - val_loss: 1.9862 - val_acc: 0.7848\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000054\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.9951 - acc: 0.7864\n",
      "\n",
      " End of Epoch Learning Rate = 0.000035\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3316 - acc: 0.9895 - val_loss: 1.9951 - val_acc: 0.7864\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000035\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.9909 - acc: 0.7846\n",
      "\n",
      " End of Epoch Learning Rate = 0.000021\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3335 - acc: 0.9896 - val_loss: 1.9909 - val_acc: 0.7846\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000021\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.9859 - acc: 0.7858\n",
      "\n",
      " End of Epoch Learning Rate = 0.000013\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3329 - acc: 0.9894 - val_loss: 1.9859 - val_acc: 0.7858\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000013\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.9834 - acc: 0.7841\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.3306 - acc: 0.9905 - val_loss: 1.9834 - val_acc: 0.7841\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(dataGenerator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch =steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (%): 78.41\n"
     ]
    }
   ],
   "source": [
    "#get final performance\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print('Test accuracy (%):', 100*sum(np.argmax(y_pred,-1)==np.argmax(y_test,-1))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss and accuracy\n",
    "plot_history(model.history.history)\n",
    "\n",
    "#plot learning rate schedule\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.arange(0,len(lr_scheduler.lr_used))/steps_per_epoch,lr_scheduler.lr_used)\n",
    "plt.xlabel('epoch number')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weigts used for updating\n",
    "model.save_weights(ModelsPath+'Final_weights_'+WhichDataSet+'_OneBitPerWeight_model_sReLU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
