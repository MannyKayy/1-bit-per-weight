{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow.keras implementation of full precision shifted ReLU CNN for CIFAR 100 \n",
    "##  https://arxiv.org/abs/1907.06916\n",
    "## Mark D. McDonnell, Hesham Mostafa, Runchun Wang, Andre van Schaik,\n",
    "## Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  1.13.1\n"
     ]
    }
   ],
   "source": [
    "# select a GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import savemat,loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import tensorflow\n",
    "print('Tensorflow version = ',tensorflow.__version__)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, History\n",
    "\n",
    "#from tensorflow.keras import backend as K\n",
    "\n",
    "from ResNetModel import resnet_srelu\n",
    "from Utils import cutout,LR_WarmRestart,GetDataGen,plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#params\n",
    "#WhichDataSet = 'CIFAR10'\n",
    "WhichDataSet = 'CIFAR100'\n",
    "init_lr = 0.1\n",
    "epochs = 300\n",
    "batch_size = 125\n",
    "My_wd=5e-4/2\n",
    "resnet_width = 10\n",
    "resnet_depth = 20\n",
    "UseBinary=False\n",
    "UseCutout=True\n",
    "Loss = 'categorical_crossentropy'\n",
    "Optimizer = SGD(lr=init_lr,decay=0.0, momentum=0.9, nesterov=False)\n",
    "Metrics = ['accuracy']\n",
    "ModelsPath = 'TrainedModels/Tensorflow.keras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare data\n",
    "if WhichDataSet == 'CIFAR10':\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()\n",
    "else:\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar100.load_data()\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "K_train = x_train.shape[0]\n",
    "input_shape = x_train.shape[1:]\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')#/255.0\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_entropy_logits_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        return tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 160)  4320        batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 32, 32, 160)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 32, 32, 160)  0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 32, 160)  0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 160)  230400      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 32, 32, 160)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 32, 32, 160)  0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 32, 32, 160)  0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 160)  230400      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 160)  0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 32, 32, 160)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32, 32, 160)  0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 32, 32, 160)  0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 160)  230400      lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 32, 32, 160)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 32, 32, 160)  0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 32, 32, 160)  0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 160)  230400      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 32, 32, 160)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 160)  0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 32, 32, 160)  0           re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 160)  230400      lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 32, 32, 160)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32, 32, 160)  0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 32, 32, 160)  0           re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 160)  230400      lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 160)  0           conv2d_6[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 32, 32, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 32, 32, 160)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 32, 32, 160)  0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 320)  460800      lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 16, 16, 320)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 16, 16, 320)  0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 16, 16, 320)  0           re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 16, 16, 160)  0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 320)  921600      lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 320)  0           average_pooling2d[0][0]          \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 320)  0           conv2d_8[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 16, 16, 320)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 16, 16, 320)  0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 16, 16, 320)  0           re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 320)  921600      lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 16, 16, 320)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 16, 16, 320)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 16, 16, 320)  0           re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 320)  921600      lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 320)  0           conv2d_10[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 16, 16, 320)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 16, 16, 320)  0           lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 16, 16, 320)  0           re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 320)  921600      lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 16, 16, 320)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 16, 16, 320)  0           lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 16, 16, 320)  0           re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 320)  921600      lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_12[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 16, 16, 320)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 16, 16, 320)  0           lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 16, 16, 320)  0           re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 640)    1843200     lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 8, 640)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 8, 8, 640)    0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 320)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 8, 8, 640)    0           re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 8, 320)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 640)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 640)    0           conv2d_14[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 8, 640)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 8, 640)    0           lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 8, 640)    0           re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 8, 640)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 8, 8, 640)    0           lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 8, 640)    0           re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 640)    0           conv2d_16[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 8, 640)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 8, 8, 640)    0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 8, 8, 640)    0           re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 8, 640)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 8, 8, 640)    0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 8, 640)    0           re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 640)    3686400     lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 640)    0           conv2d_18[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 8, 640)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 8, 8, 640)    0           lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 8, 640)    0           re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 100)    64000       lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 8, 100)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 100)          0           lambda_40[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,794,732\n",
      "Trainable params: 26,794,726\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define a datagen or generating training samples with flip and pad/crop augmentation, and if set to True, with cutout augmentation\n",
    "dataGenerator = GetDataGen(UseCutout)\n",
    "\n",
    "#define and compile the model\n",
    "Temperature=25.0\n",
    "model = resnet_srelu(Temperature,UseBinary,input_shape=input_shape, depth=resnet_depth, num_classes=num_classes,\n",
    "                     wd=My_wd,width=resnet_width)\n",
    "model.compile(loss=catcross_entropy_logits_loss() ,optimizer = Optimizer, metrics = Metrics)\n",
    "\n",
    "#print  the model\n",
    "model.summary()\n",
    "\n",
    "#define the learnng rate schedule\n",
    "steps_per_epoch = int(np.floor(K_train / batch_size))\n",
    "lr_scheduler = LR_WarmRestart(nbatch=steps_per_epoch,\n",
    "                              initial_lr=init_lr, min_lr=init_lr*1e-4,\n",
    "                              epochs_restart = [],\n",
    "                              Tmult=300.0) \n",
    "\n",
    "#define callbacks\n",
    "history = History()\n",
    "callbacks = [lr_scheduler,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 6.5205 - acc: 0.0612\n",
      "\n",
      " End of Epoch Learning Rate = 0.099997\n",
      "400/400 [==============================] - 124s 311ms/step - loss: 7.4122 - acc: 0.0264 - val_loss: 6.5205 - val_acc: 0.0612\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099997\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 5.3521 - acc: 0.1359\n",
      "\n",
      " End of Epoch Learning Rate = 0.099989\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 5.9849 - acc: 0.0857 - val_loss: 5.3521 - val_acc: 0.1359\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099989\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 4.5927 - acc: 0.1946\n",
      "\n",
      " End of Epoch Learning Rate = 0.099975\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 5.0053 - acc: 0.1563 - val_loss: 4.5927 - val_acc: 0.1946\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099975\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 3.8572 - acc: 0.2798\n",
      "\n",
      " End of Epoch Learning Rate = 0.099956\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 4.3110 - acc: 0.2173 - val_loss: 3.8572 - val_acc: 0.2798\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099956\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 3.4895 - acc: 0.3290\n",
      "\n",
      " End of Epoch Learning Rate = 0.099931\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 3.7948 - acc: 0.2769 - val_loss: 3.4895 - val_acc: 0.3290\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099931\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 3.1944 - acc: 0.3908\n",
      "\n",
      " End of Epoch Learning Rate = 0.099901\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 3.4035 - acc: 0.3357 - val_loss: 3.1944 - val_acc: 0.3908\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099901\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 8s 770us/sample - loss: 2.8284 - acc: 0.4517\n",
      "\n",
      " End of Epoch Learning Rate = 0.099866\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 3.0977 - acc: 0.3946 - val_loss: 2.8284 - val_acc: 0.4517\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099866\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.7231 - acc: 0.4954\n",
      "\n",
      " End of Epoch Learning Rate = 0.099825\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.8865 - acc: 0.4424 - val_loss: 2.7231 - val_acc: 0.4954\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099825\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 2.6239 - acc: 0.5082\n",
      "\n",
      " End of Epoch Learning Rate = 0.099778\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.7562 - acc: 0.4727 - val_loss: 2.6239 - val_acc: 0.5082\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099778\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.5434 - acc: 0.5370\n",
      "\n",
      " End of Epoch Learning Rate = 0.099726\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.6572 - acc: 0.4982 - val_loss: 2.5434 - val_acc: 0.5370\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099726\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.5175 - acc: 0.5468\n",
      "\n",
      " End of Epoch Learning Rate = 0.099669\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.5896 - acc: 0.5226 - val_loss: 2.5175 - val_acc: 0.5468\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099669\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.3935 - acc: 0.5735\n",
      "\n",
      " End of Epoch Learning Rate = 0.099606\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.5438 - acc: 0.5370 - val_loss: 2.3935 - val_acc: 0.5735\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099606\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.4521 - acc: 0.5746\n",
      "\n",
      " End of Epoch Learning Rate = 0.099537\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.4968 - acc: 0.5567 - val_loss: 2.4521 - val_acc: 0.5746\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099537\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.4228 - acc: 0.5878\n",
      "\n",
      " End of Epoch Learning Rate = 0.099464\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.4736 - acc: 0.5665 - val_loss: 2.4228 - val_acc: 0.5878\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099464\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.4493 - acc: 0.5967\n",
      "\n",
      " End of Epoch Learning Rate = 0.099384\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.4581 - acc: 0.5754 - val_loss: 2.4493 - val_acc: 0.5967\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099384\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4780 - acc: 0.5884\n",
      "\n",
      " End of Epoch Learning Rate = 0.099300\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.4264 - acc: 0.5924 - val_loss: 2.4780 - val_acc: 0.5884\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099300\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.4299 - acc: 0.6195\n",
      "\n",
      " End of Epoch Learning Rate = 0.099210\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.4098 - acc: 0.6007 - val_loss: 2.4299 - val_acc: 0.6195\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099210\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.4482 - acc: 0.6128\n",
      "\n",
      " End of Epoch Learning Rate = 0.099114\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.4152 - acc: 0.6046 - val_loss: 2.4482 - val_acc: 0.6128\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099114\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.5036 - acc: 0.6003\n",
      "\n",
      " End of Epoch Learning Rate = 0.099014\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3933 - acc: 0.6128 - val_loss: 2.5036 - val_acc: 0.6003\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099014\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.4917 - acc: 0.6169\n",
      "\n",
      " End of Epoch Learning Rate = 0.098907\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3885 - acc: 0.6193 - val_loss: 2.4917 - val_acc: 0.6169\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098907\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.4277 - acc: 0.6202\n",
      "\n",
      " End of Epoch Learning Rate = 0.098796\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3698 - acc: 0.6285 - val_loss: 2.4277 - val_acc: 0.6202\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098796\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 2.4257 - acc: 0.6266\n",
      "\n",
      " End of Epoch Learning Rate = 0.098679\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3716 - acc: 0.6328 - val_loss: 2.4257 - val_acc: 0.6266\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098679\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4366 - acc: 0.6256\n",
      "\n",
      " End of Epoch Learning Rate = 0.098557\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3630 - acc: 0.6351 - val_loss: 2.4366 - val_acc: 0.6256\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098557\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.4200 - acc: 0.6357\n",
      "\n",
      " End of Epoch Learning Rate = 0.098429\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3557 - acc: 0.6397 - val_loss: 2.4200 - val_acc: 0.6357\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098429\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.4256 - acc: 0.6352\n",
      "\n",
      " End of Epoch Learning Rate = 0.098296\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3529 - acc: 0.6463 - val_loss: 2.4256 - val_acc: 0.6352\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098296\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4559 - acc: 0.6430\n",
      "\n",
      " End of Epoch Learning Rate = 0.098158\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3305 - acc: 0.6546 - val_loss: 2.4559 - val_acc: 0.6430\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098158\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.4193 - acc: 0.6440\n",
      "\n",
      " End of Epoch Learning Rate = 0.098015\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3439 - acc: 0.6512 - val_loss: 2.4193 - val_acc: 0.6440\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098015\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.4082 - acc: 0.6419\n",
      "\n",
      " End of Epoch Learning Rate = 0.097866\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3394 - acc: 0.6547 - val_loss: 2.4082 - val_acc: 0.6419\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097866\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4507 - acc: 0.6384\n",
      "\n",
      " End of Epoch Learning Rate = 0.097712\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3369 - acc: 0.6585 - val_loss: 2.4507 - val_acc: 0.6384\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097712\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.5186 - acc: 0.6386\n",
      "\n",
      " End of Epoch Learning Rate = 0.097553\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3357 - acc: 0.6600 - val_loss: 2.5186 - val_acc: 0.6386\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097553\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.4285 - acc: 0.6512\n",
      "\n",
      " End of Epoch Learning Rate = 0.097389\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3373 - acc: 0.6635 - val_loss: 2.4285 - val_acc: 0.6512\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097389\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4696 - acc: 0.6478\n",
      "\n",
      " End of Epoch Learning Rate = 0.097219\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3166 - acc: 0.6678 - val_loss: 2.4696 - val_acc: 0.6478\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097219\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.4122 - acc: 0.6578\n",
      "\n",
      " End of Epoch Learning Rate = 0.097044\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3150 - acc: 0.6699 - val_loss: 2.4122 - val_acc: 0.6578\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097044\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 2.4384 - acc: 0.6511\n",
      "\n",
      " End of Epoch Learning Rate = 0.096864\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3127 - acc: 0.6696 - val_loss: 2.4384 - val_acc: 0.6511\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096864\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.4911 - acc: 0.6436\n",
      "\n",
      " End of Epoch Learning Rate = 0.096679\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3076 - acc: 0.6732 - val_loss: 2.4911 - val_acc: 0.6436\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096679\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.3982 - acc: 0.6638\n",
      "\n",
      " End of Epoch Learning Rate = 0.096489\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.3034 - acc: 0.6759 - val_loss: 2.3982 - val_acc: 0.6638\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096489\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.4055 - acc: 0.6676\n",
      "\n",
      " End of Epoch Learning Rate = 0.096294\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3071 - acc: 0.6774 - val_loss: 2.4055 - val_acc: 0.6676\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096294\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.5387 - acc: 0.6489\n",
      "\n",
      " End of Epoch Learning Rate = 0.096094\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2919 - acc: 0.6815 - val_loss: 2.5387 - val_acc: 0.6489\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096094\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.5223 - acc: 0.6468\n",
      "\n",
      " End of Epoch Learning Rate = 0.095888\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.3003 - acc: 0.6807 - val_loss: 2.5223 - val_acc: 0.6468\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095888\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.5057 - acc: 0.6493\n",
      "\n",
      " End of Epoch Learning Rate = 0.095678\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.3039 - acc: 0.6808 - val_loss: 2.5057 - val_acc: 0.6493\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095678\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 2.4265 - acc: 0.6667\n",
      "\n",
      " End of Epoch Learning Rate = 0.095462\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.2913 - acc: 0.6836 - val_loss: 2.4265 - val_acc: 0.6667\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095462\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 2.4084 - acc: 0.6725\n",
      "\n",
      " End of Epoch Learning Rate = 0.095242\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.2710 - acc: 0.6892 - val_loss: 2.4084 - val_acc: 0.6725\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095242\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 2.4277 - acc: 0.6593\n",
      "\n",
      " End of Epoch Learning Rate = 0.095016\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2643 - acc: 0.6932 - val_loss: 2.4277 - val_acc: 0.6593\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095016\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.4852 - acc: 0.6528\n",
      "\n",
      " End of Epoch Learning Rate = 0.094786\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2814 - acc: 0.6897 - val_loss: 2.4852 - val_acc: 0.6528\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094786\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.4941 - acc: 0.6491\n",
      "\n",
      " End of Epoch Learning Rate = 0.094551\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2717 - acc: 0.6905 - val_loss: 2.4941 - val_acc: 0.6491\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094551\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.4542 - acc: 0.6643\n",
      "\n",
      " End of Epoch Learning Rate = 0.094311\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2688 - acc: 0.6939 - val_loss: 2.4542 - val_acc: 0.6643\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094311\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4893 - acc: 0.6642\n",
      "\n",
      " End of Epoch Learning Rate = 0.094066\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.2847 - acc: 0.6905 - val_loss: 2.4893 - val_acc: 0.6642\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094066\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.4269 - acc: 0.6761\n",
      "\n",
      " End of Epoch Learning Rate = 0.093816\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2789 - acc: 0.6959 - val_loss: 2.4269 - val_acc: 0.6761\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093816\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.3656 - acc: 0.6901\n",
      "\n",
      " End of Epoch Learning Rate = 0.093561\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.2664 - acc: 0.6978 - val_loss: 2.3656 - val_acc: 0.6901\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093561\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.3648 - acc: 0.6865\n",
      "\n",
      " End of Epoch Learning Rate = 0.093302\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.2522 - acc: 0.7001 - val_loss: 2.3648 - val_acc: 0.6865\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093302\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4329 - acc: 0.6631\n",
      "\n",
      " End of Epoch Learning Rate = 0.093038\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.2487 - acc: 0.7004 - val_loss: 2.4329 - val_acc: 0.6631\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093038\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.4788 - acc: 0.6619\n",
      "\n",
      " End of Epoch Learning Rate = 0.092769\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.2603 - acc: 0.6997 - val_loss: 2.4788 - val_acc: 0.6619\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092769\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.4347 - acc: 0.6707\n",
      "\n",
      " End of Epoch Learning Rate = 0.092495\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.2335 - acc: 0.7069 - val_loss: 2.4347 - val_acc: 0.6707\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092495\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4222 - acc: 0.6686\n",
      "\n",
      " End of Epoch Learning Rate = 0.092217\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2249 - acc: 0.7086 - val_loss: 2.4222 - val_acc: 0.6686\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092217\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.4293 - acc: 0.6774\n",
      "\n",
      " End of Epoch Learning Rate = 0.091934\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.2388 - acc: 0.7058 - val_loss: 2.4293 - val_acc: 0.6774\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091934\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.4045 - acc: 0.6790\n",
      "\n",
      " End of Epoch Learning Rate = 0.091647\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.2374 - acc: 0.7079 - val_loss: 2.4045 - val_acc: 0.6790\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091647\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.3959 - acc: 0.6790\n",
      "\n",
      " End of Epoch Learning Rate = 0.091355\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2175 - acc: 0.7114 - val_loss: 2.3959 - val_acc: 0.6790\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091355\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.4131 - acc: 0.6801\n",
      "\n",
      " End of Epoch Learning Rate = 0.091058\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.2197 - acc: 0.7099 - val_loss: 2.4131 - val_acc: 0.6801\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091058\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 2.4234 - acc: 0.6808\n",
      "\n",
      " End of Epoch Learning Rate = 0.090757\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2125 - acc: 0.7117 - val_loss: 2.4234 - val_acc: 0.6808\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090757\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.3486 - acc: 0.6878\n",
      "\n",
      " End of Epoch Learning Rate = 0.090452\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2290 - acc: 0.7098 - val_loss: 2.3486 - val_acc: 0.6878\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090452\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.4434 - acc: 0.6778\n",
      "\n",
      " End of Epoch Learning Rate = 0.090142\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.2032 - acc: 0.7154 - val_loss: 2.4434 - val_acc: 0.6778\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090142\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.4353 - acc: 0.6777\n",
      "\n",
      " End of Epoch Learning Rate = 0.089828\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.2046 - acc: 0.7156 - val_loss: 2.4353 - val_acc: 0.6777\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089828\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.3994 - acc: 0.6830\n",
      "\n",
      " End of Epoch Learning Rate = 0.089509\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1985 - acc: 0.7161 - val_loss: 2.3994 - val_acc: 0.6830\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089509\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4486 - acc: 0.6716\n",
      "\n",
      " End of Epoch Learning Rate = 0.089186\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.1922 - acc: 0.7195 - val_loss: 2.4486 - val_acc: 0.6716\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089186\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.4625 - acc: 0.6723\n",
      "\n",
      " End of Epoch Learning Rate = 0.088858\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1849 - acc: 0.7230 - val_loss: 2.4625 - val_acc: 0.6723\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088858\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.4185 - acc: 0.6820\n",
      "\n",
      " End of Epoch Learning Rate = 0.088527\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.1933 - acc: 0.7196 - val_loss: 2.4185 - val_acc: 0.6820\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088527\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.4538 - acc: 0.6722\n",
      "\n",
      " End of Epoch Learning Rate = 0.088191\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1786 - acc: 0.7239 - val_loss: 2.4538 - val_acc: 0.6722\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088191\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4445 - acc: 0.6755\n",
      "\n",
      " End of Epoch Learning Rate = 0.087851\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1777 - acc: 0.7246 - val_loss: 2.4445 - val_acc: 0.6755\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087851\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.3852 - acc: 0.6900\n",
      "\n",
      " End of Epoch Learning Rate = 0.087507\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1811 - acc: 0.7218 - val_loss: 2.3852 - val_acc: 0.6900\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087507\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.4323 - acc: 0.6879\n",
      "\n",
      " End of Epoch Learning Rate = 0.087159\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1668 - acc: 0.7260 - val_loss: 2.4323 - val_acc: 0.6879\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087159\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.4173 - acc: 0.6931\n",
      "\n",
      " End of Epoch Learning Rate = 0.086806\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1690 - acc: 0.7274 - val_loss: 2.4173 - val_acc: 0.6931\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086806\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.3699 - acc: 0.6969\n",
      "\n",
      " End of Epoch Learning Rate = 0.086450\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1622 - acc: 0.7249 - val_loss: 2.3699 - val_acc: 0.6969\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086450\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 2.4683 - acc: 0.6812\n",
      "\n",
      " End of Epoch Learning Rate = 0.086089\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1418 - acc: 0.7303 - val_loss: 2.4683 - val_acc: 0.6812\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086089\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4585 - acc: 0.6741\n",
      "\n",
      " End of Epoch Learning Rate = 0.085725\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.1481 - acc: 0.7297 - val_loss: 2.4585 - val_acc: 0.6741\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085725\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4145 - acc: 0.6787\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1376 - acc: 0.7348 - val_loss: 2.4145 - val_acc: 0.6787\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.4076 - acc: 0.6783\n",
      "\n",
      " End of Epoch Learning Rate = 0.084985\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1446 - acc: 0.7319 - val_loss: 2.4076 - val_acc: 0.6783\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084985\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.3559 - acc: 0.6971\n",
      "\n",
      " End of Epoch Learning Rate = 0.084609\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1321 - acc: 0.7322 - val_loss: 2.3559 - val_acc: 0.6971\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084609\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.4212 - acc: 0.6840\n",
      "\n",
      " End of Epoch Learning Rate = 0.084229\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1237 - acc: 0.7386 - val_loss: 2.4212 - val_acc: 0.6840\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084229\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4027 - acc: 0.6862\n",
      "\n",
      " End of Epoch Learning Rate = 0.083845\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1140 - acc: 0.7408 - val_loss: 2.4027 - val_acc: 0.6862\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083845\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 2.4427 - acc: 0.6857\n",
      "\n",
      " End of Epoch Learning Rate = 0.083458\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 2.1350 - acc: 0.7342 - val_loss: 2.4427 - val_acc: 0.6857\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083458\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.3917 - acc: 0.6955\n",
      "\n",
      " End of Epoch Learning Rate = 0.083067\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.1086 - acc: 0.7419 - val_loss: 2.3917 - val_acc: 0.6955\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083067\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.3340 - acc: 0.6931\n",
      "\n",
      " End of Epoch Learning Rate = 0.082673\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1141 - acc: 0.7386 - val_loss: 2.3340 - val_acc: 0.6931\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082673\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.4232 - acc: 0.6841\n",
      "\n",
      " End of Epoch Learning Rate = 0.082275\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.0992 - acc: 0.7431 - val_loss: 2.4232 - val_acc: 0.6841\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082275\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.3823 - acc: 0.6862\n",
      "\n",
      " End of Epoch Learning Rate = 0.081873\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.0907 - acc: 0.7441 - val_loss: 2.3823 - val_acc: 0.6862\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081873\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.3308 - acc: 0.7057\n",
      "\n",
      " End of Epoch Learning Rate = 0.081468\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.0970 - acc: 0.7432 - val_loss: 2.3308 - val_acc: 0.7057\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081468\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4377 - acc: 0.6873\n",
      "\n",
      " End of Epoch Learning Rate = 0.081059\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0854 - acc: 0.7447 - val_loss: 2.4377 - val_acc: 0.6873\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081059\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.3756 - acc: 0.6918\n",
      "\n",
      " End of Epoch Learning Rate = 0.080647\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.0746 - acc: 0.7491 - val_loss: 2.3756 - val_acc: 0.6918\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080647\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.3597 - acc: 0.7013\n",
      "\n",
      " End of Epoch Learning Rate = 0.080232\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.0768 - acc: 0.7459 - val_loss: 2.3597 - val_acc: 0.7013\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080232\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 2.3805 - acc: 0.6909\n",
      "\n",
      " End of Epoch Learning Rate = 0.079813\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.0741 - acc: 0.7473 - val_loss: 2.3805 - val_acc: 0.6909\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079813\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.3152 - acc: 0.6955\n",
      "\n",
      " End of Epoch Learning Rate = 0.079391\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0526 - acc: 0.7529 - val_loss: 2.3152 - val_acc: 0.6955\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079391\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.4024 - acc: 0.6789\n",
      "\n",
      " End of Epoch Learning Rate = 0.078966\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0606 - acc: 0.7510 - val_loss: 2.4024 - val_acc: 0.6789\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078966\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.3821 - acc: 0.6940\n",
      "\n",
      " End of Epoch Learning Rate = 0.078538\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0446 - acc: 0.7533 - val_loss: 2.3821 - val_acc: 0.6940\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078538\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.3853 - acc: 0.6941\n",
      "\n",
      " End of Epoch Learning Rate = 0.078106\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0452 - acc: 0.7550 - val_loss: 2.3853 - val_acc: 0.6941\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078106\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.4317 - acc: 0.6810\n",
      "\n",
      " End of Epoch Learning Rate = 0.077672\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0414 - acc: 0.7537 - val_loss: 2.4317 - val_acc: 0.6810\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077672\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.3926 - acc: 0.6934\n",
      "\n",
      " End of Epoch Learning Rate = 0.077234\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 2.0271 - acc: 0.7561 - val_loss: 2.3926 - val_acc: 0.6934\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077234\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.3934 - acc: 0.6830\n",
      "\n",
      " End of Epoch Learning Rate = 0.076794\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 2.0239 - acc: 0.7573 - val_loss: 2.3934 - val_acc: 0.6830\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076794\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.4058 - acc: 0.6843\n",
      "\n",
      " End of Epoch Learning Rate = 0.076350\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 2.0160 - acc: 0.7605 - val_loss: 2.4058 - val_acc: 0.6843\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076350\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.3758 - acc: 0.6989\n",
      "\n",
      " End of Epoch Learning Rate = 0.075904\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.0211 - acc: 0.7615 - val_loss: 2.3758 - val_acc: 0.6989\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075904\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.4266 - acc: 0.6738\n",
      "\n",
      " End of Epoch Learning Rate = 0.075455\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 2.0015 - acc: 0.7611 - val_loss: 2.4266 - val_acc: 0.6738\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075455\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.3650 - acc: 0.6982\n",
      "\n",
      " End of Epoch Learning Rate = 0.075002\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.0084 - acc: 0.7601 - val_loss: 2.3650 - val_acc: 0.6982\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075002\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 2.3499 - acc: 0.6968\n",
      "\n",
      " End of Epoch Learning Rate = 0.074548\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9826 - acc: 0.7664 - val_loss: 2.3499 - val_acc: 0.6968\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074548\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.4039 - acc: 0.6916\n",
      "\n",
      " End of Epoch Learning Rate = 0.074090\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.9821 - acc: 0.7649 - val_loss: 2.4039 - val_acc: 0.6916\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074090\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.3600 - acc: 0.6994\n",
      "\n",
      " End of Epoch Learning Rate = 0.073630\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9808 - acc: 0.7649 - val_loss: 2.3600 - val_acc: 0.6994\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073630\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.3126 - acc: 0.7002\n",
      "\n",
      " End of Epoch Learning Rate = 0.073167\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.9625 - acc: 0.7708 - val_loss: 2.3126 - val_acc: 0.7002\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073167\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.3974 - acc: 0.6985\n",
      "\n",
      " End of Epoch Learning Rate = 0.072702\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9619 - acc: 0.7714 - val_loss: 2.3974 - val_acc: 0.6985\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072702\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 2.3592 - acc: 0.6973\n",
      "\n",
      " End of Epoch Learning Rate = 0.072235\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9681 - acc: 0.7670 - val_loss: 2.3592 - val_acc: 0.6973\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072235\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.4162 - acc: 0.6859\n",
      "\n",
      " End of Epoch Learning Rate = 0.071764\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9506 - acc: 0.7734 - val_loss: 2.4162 - val_acc: 0.6859\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071764\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 2.3341 - acc: 0.6879\n",
      "\n",
      " End of Epoch Learning Rate = 0.071292\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9372 - acc: 0.7768 - val_loss: 2.3341 - val_acc: 0.6879\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071292\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 2.2993 - acc: 0.6974\n",
      "\n",
      " End of Epoch Learning Rate = 0.070817\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 1.9287 - acc: 0.7775 - val_loss: 2.2993 - val_acc: 0.6974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070817\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.3321 - acc: 0.6986\n",
      "\n",
      " End of Epoch Learning Rate = 0.070340\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.9303 - acc: 0.7756 - val_loss: 2.3321 - val_acc: 0.6986\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070340\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.3242 - acc: 0.6969\n",
      "\n",
      " End of Epoch Learning Rate = 0.069860\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.9152 - acc: 0.7809 - val_loss: 2.3242 - val_acc: 0.6969\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069860\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.2904 - acc: 0.7103\n",
      "\n",
      " End of Epoch Learning Rate = 0.069379\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.9321 - acc: 0.7736 - val_loss: 2.2904 - val_acc: 0.7103\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069379\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 2.3210 - acc: 0.6992\n",
      "\n",
      " End of Epoch Learning Rate = 0.068895\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.9108 - acc: 0.7780 - val_loss: 2.3210 - val_acc: 0.6992\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068895\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.3152 - acc: 0.7003\n",
      "\n",
      " End of Epoch Learning Rate = 0.068409\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.8934 - acc: 0.7854 - val_loss: 2.3152 - val_acc: 0.7003\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068409\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.3672 - acc: 0.6979\n",
      "\n",
      " End of Epoch Learning Rate = 0.067922\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.8882 - acc: 0.7839 - val_loss: 2.3672 - val_acc: 0.6979\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067922\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.3170 - acc: 0.6902\n",
      "\n",
      " End of Epoch Learning Rate = 0.067432\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8793 - acc: 0.7812 - val_loss: 2.3170 - val_acc: 0.6902\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067432\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.3687 - acc: 0.6948\n",
      "\n",
      " End of Epoch Learning Rate = 0.066940\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8723 - acc: 0.7855 - val_loss: 2.3687 - val_acc: 0.6948\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066940\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 2.3438 - acc: 0.7057\n",
      "\n",
      " End of Epoch Learning Rate = 0.066447\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8589 - acc: 0.7900 - val_loss: 2.3438 - val_acc: 0.7057\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066447\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 2.2948 - acc: 0.7056\n",
      "\n",
      " End of Epoch Learning Rate = 0.065951\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8638 - acc: 0.7874 - val_loss: 2.2948 - val_acc: 0.7056\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065951\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.2512 - acc: 0.7142\n",
      "\n",
      " End of Epoch Learning Rate = 0.065454\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.8477 - acc: 0.7918 - val_loss: 2.2512 - val_acc: 0.7142\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065454\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.2954 - acc: 0.7018\n",
      "\n",
      " End of Epoch Learning Rate = 0.064956\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8321 - acc: 0.7940 - val_loss: 2.2954 - val_acc: 0.7018\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064956\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.3561 - acc: 0.6981\n",
      "\n",
      " End of Epoch Learning Rate = 0.064455\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8330 - acc: 0.7945 - val_loss: 2.3561 - val_acc: 0.6981\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064455\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.3409 - acc: 0.6996\n",
      "\n",
      " End of Epoch Learning Rate = 0.063953\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.8260 - acc: 0.7957 - val_loss: 2.3409 - val_acc: 0.6996\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063953\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.3351 - acc: 0.7173\n",
      "\n",
      " End of Epoch Learning Rate = 0.063450\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.8120 - acc: 0.7979 - val_loss: 2.3351 - val_acc: 0.7173\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063450\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 2.3581 - acc: 0.6981\n",
      "\n",
      " End of Epoch Learning Rate = 0.062945\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.8142 - acc: 0.7976 - val_loss: 2.3581 - val_acc: 0.6981\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062945\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.2409 - acc: 0.7160\n",
      "\n",
      " End of Epoch Learning Rate = 0.062438\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.8132 - acc: 0.7975 - val_loss: 2.2409 - val_acc: 0.7160\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062438\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 2.2868 - acc: 0.6975\n",
      "\n",
      " End of Epoch Learning Rate = 0.061930\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.8003 - acc: 0.7994 - val_loss: 2.2868 - val_acc: 0.6975\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061930\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 7s 724us/sample - loss: 2.2658 - acc: 0.7086\n",
      "\n",
      " End of Epoch Learning Rate = 0.061421\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.7957 - acc: 0.8007 - val_loss: 2.2658 - val_acc: 0.7086\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061421\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.2641 - acc: 0.7111\n",
      "\n",
      " End of Epoch Learning Rate = 0.060911\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.7775 - acc: 0.8059 - val_loss: 2.2641 - val_acc: 0.7111\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060911\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 720us/sample - loss: 2.3208 - acc: 0.7046\n",
      "\n",
      " End of Epoch Learning Rate = 0.060400\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.7689 - acc: 0.8052 - val_loss: 2.3208 - val_acc: 0.7046\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060400\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.3281 - acc: 0.7166\n",
      "\n",
      " End of Epoch Learning Rate = 0.059887\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.7762 - acc: 0.8032 - val_loss: 2.3281 - val_acc: 0.7166\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059887\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.3540 - acc: 0.6971\n",
      "\n",
      " End of Epoch Learning Rate = 0.059373\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.7656 - acc: 0.8070 - val_loss: 2.3540 - val_acc: 0.6971\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059373\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.2594 - acc: 0.7095\n",
      "\n",
      " End of Epoch Learning Rate = 0.058858\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.7510 - acc: 0.8102 - val_loss: 2.2594 - val_acc: 0.7095\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058858\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.2667 - acc: 0.7120\n",
      "\n",
      " End of Epoch Learning Rate = 0.058343\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.7433 - acc: 0.8093 - val_loss: 2.2667 - val_acc: 0.7120\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058343\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.3221 - acc: 0.7072\n",
      "\n",
      " End of Epoch Learning Rate = 0.057826\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.7277 - acc: 0.8117 - val_loss: 2.3221 - val_acc: 0.7072\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057826\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.1763 - acc: 0.7179\n",
      "\n",
      " End of Epoch Learning Rate = 0.057308\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.7299 - acc: 0.8133 - val_loss: 2.1763 - val_acc: 0.7179\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057308\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.2972 - acc: 0.7028\n",
      "\n",
      " End of Epoch Learning Rate = 0.056790\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.7081 - acc: 0.8167 - val_loss: 2.2972 - val_acc: 0.7028\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056790\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.2982 - acc: 0.7058\n",
      "\n",
      " End of Epoch Learning Rate = 0.056271\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.7022 - acc: 0.8167 - val_loss: 2.2982 - val_acc: 0.7058\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056271\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.3734 - acc: 0.6952\n",
      "\n",
      " End of Epoch Learning Rate = 0.055751\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6938 - acc: 0.8202 - val_loss: 2.3734 - val_acc: 0.6952\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055751\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.2363 - acc: 0.7222\n",
      "\n",
      " End of Epoch Learning Rate = 0.055231\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6835 - acc: 0.8214 - val_loss: 2.2363 - val_acc: 0.7222\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055231\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 2.2599 - acc: 0.7106\n",
      "\n",
      " End of Epoch Learning Rate = 0.054710\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6734 - acc: 0.8230 - val_loss: 2.2599 - val_acc: 0.7106\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054710\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.1826 - acc: 0.7171\n",
      "\n",
      " End of Epoch Learning Rate = 0.054188\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6701 - acc: 0.8220 - val_loss: 2.1826 - val_acc: 0.7171\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054188\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.2792 - acc: 0.7238\n",
      "\n",
      " End of Epoch Learning Rate = 0.053667\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6624 - acc: 0.8234 - val_loss: 2.2792 - val_acc: 0.7238\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053667\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 2.2240 - acc: 0.7220\n",
      "\n",
      " End of Epoch Learning Rate = 0.053144\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.6528 - acc: 0.8255 - val_loss: 2.2240 - val_acc: 0.7220\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053144\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.2542 - acc: 0.7153\n",
      "\n",
      " End of Epoch Learning Rate = 0.052622\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6361 - acc: 0.8310 - val_loss: 2.2542 - val_acc: 0.7153\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052622\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.2356 - acc: 0.7166\n",
      "\n",
      " End of Epoch Learning Rate = 0.052099\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.6301 - acc: 0.8286 - val_loss: 2.2356 - val_acc: 0.7166\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052099\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.2146 - acc: 0.7234\n",
      "\n",
      " End of Epoch Learning Rate = 0.051575\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6141 - acc: 0.8323 - val_loss: 2.2146 - val_acc: 0.7234\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051575\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.1578 - acc: 0.7304\n",
      "\n",
      " End of Epoch Learning Rate = 0.051052\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.6142 - acc: 0.8309 - val_loss: 2.1578 - val_acc: 0.7304\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051052\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.1891 - acc: 0.7226\n",
      "\n",
      " End of Epoch Learning Rate = 0.050529\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.5955 - acc: 0.8352 - val_loss: 2.1891 - val_acc: 0.7226\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050529\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 2.2003 - acc: 0.7132\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.5972 - acc: 0.8345 - val_loss: 2.2003 - val_acc: 0.7132\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.1615 - acc: 0.7306\n",
      "\n",
      " End of Epoch Learning Rate = 0.049481\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.5892 - acc: 0.8353 - val_loss: 2.1615 - val_acc: 0.7306\n",
      "\n",
      " Start of Epoch Learning Rate = 0.049481\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 2.2026 - acc: 0.7128\n",
      "\n",
      " End of Epoch Learning Rate = 0.048958\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.5873 - acc: 0.8328 - val_loss: 2.2026 - val_acc: 0.7128\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048958\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.2139 - acc: 0.7128\n",
      "\n",
      " End of Epoch Learning Rate = 0.048435\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5712 - acc: 0.8375 - val_loss: 2.2139 - val_acc: 0.7128\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048435\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.2144 - acc: 0.7231\n",
      "\n",
      " End of Epoch Learning Rate = 0.047911\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.5557 - acc: 0.8431 - val_loss: 2.2144 - val_acc: 0.7231\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047911\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.2636 - acc: 0.7193\n",
      "\n",
      " End of Epoch Learning Rate = 0.047388\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.5493 - acc: 0.8431 - val_loss: 2.2636 - val_acc: 0.7193\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047388\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 699us/sample - loss: 2.1536 - acc: 0.7287\n",
      "\n",
      " End of Epoch Learning Rate = 0.046866\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.5361 - acc: 0.8460 - val_loss: 2.1536 - val_acc: 0.7287\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046866\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.1841 - acc: 0.7242\n",
      "\n",
      " End of Epoch Learning Rate = 0.046343\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5279 - acc: 0.8453 - val_loss: 2.1841 - val_acc: 0.7242\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046343\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.2185 - acc: 0.7155\n",
      "\n",
      " End of Epoch Learning Rate = 0.045822\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5228 - acc: 0.8464 - val_loss: 2.2185 - val_acc: 0.7155\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045822\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.2349 - acc: 0.7194\n",
      "\n",
      " End of Epoch Learning Rate = 0.045300\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.5093 - acc: 0.8488 - val_loss: 2.2349 - val_acc: 0.7194\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045300\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.1350 - acc: 0.7363\n",
      "\n",
      " End of Epoch Learning Rate = 0.044779\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4886 - acc: 0.8535 - val_loss: 2.1350 - val_acc: 0.7363\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044779\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.1821 - acc: 0.7275\n",
      "\n",
      " End of Epoch Learning Rate = 0.044259\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.4870 - acc: 0.8518 - val_loss: 2.1821 - val_acc: 0.7275\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044259\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 2.1331 - acc: 0.7231\n",
      "\n",
      " End of Epoch Learning Rate = 0.043739\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.4840 - acc: 0.8510 - val_loss: 2.1331 - val_acc: 0.7231\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043739\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.1666 - acc: 0.7238\n",
      "\n",
      " End of Epoch Learning Rate = 0.043220\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4812 - acc: 0.8520 - val_loss: 2.1666 - val_acc: 0.7238\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043220\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.2132 - acc: 0.7138\n",
      "\n",
      " End of Epoch Learning Rate = 0.042702\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4642 - acc: 0.8546 - val_loss: 2.2132 - val_acc: 0.7138\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042702\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.1846 - acc: 0.7261\n",
      "\n",
      " End of Epoch Learning Rate = 0.042184\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.4492 - acc: 0.8601 - val_loss: 2.1846 - val_acc: 0.7261\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042184\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.2223 - acc: 0.7238\n",
      "\n",
      " End of Epoch Learning Rate = 0.041667\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.4371 - acc: 0.8601 - val_loss: 2.2223 - val_acc: 0.7238\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041667\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.1092 - acc: 0.7258\n",
      "\n",
      " End of Epoch Learning Rate = 0.041152\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.4370 - acc: 0.8596 - val_loss: 2.1092 - val_acc: 0.7258\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041152\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.1949 - acc: 0.7298\n",
      "\n",
      " End of Epoch Learning Rate = 0.040637\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 1.4216 - acc: 0.8627 - val_loss: 2.1949 - val_acc: 0.7298\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040637\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.2111 - acc: 0.7273\n",
      "\n",
      " End of Epoch Learning Rate = 0.040123\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4028 - acc: 0.8667 - val_loss: 2.2111 - val_acc: 0.7273\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040123\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.1094 - acc: 0.7352\n",
      "\n",
      " End of Epoch Learning Rate = 0.039610\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.4043 - acc: 0.8650 - val_loss: 2.1094 - val_acc: 0.7352\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039610\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.0507 - acc: 0.7377\n",
      "\n",
      " End of Epoch Learning Rate = 0.039099\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.4082 - acc: 0.8648 - val_loss: 2.0507 - val_acc: 0.7377\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039099\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.1324 - acc: 0.7209\n",
      "\n",
      " End of Epoch Learning Rate = 0.038589\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.3887 - acc: 0.8674 - val_loss: 2.1324 - val_acc: 0.7209\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038589\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 2.1004 - acc: 0.7401\n",
      "\n",
      " End of Epoch Learning Rate = 0.038080\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3792 - acc: 0.8676 - val_loss: 2.1004 - val_acc: 0.7401\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038080\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 2.1212 - acc: 0.7308\n",
      "\n",
      " End of Epoch Learning Rate = 0.037572\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.3628 - acc: 0.8729 - val_loss: 2.1212 - val_acc: 0.7308\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037572\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 2.0865 - acc: 0.7298\n",
      "\n",
      " End of Epoch Learning Rate = 0.037065\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3479 - acc: 0.8738 - val_loss: 2.0865 - val_acc: 0.7298\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037065\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.1806 - acc: 0.7239\n",
      "\n",
      " End of Epoch Learning Rate = 0.036560\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.3348 - acc: 0.8758 - val_loss: 2.1806 - val_acc: 0.7239\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036560\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.1373 - acc: 0.7293\n",
      "\n",
      " End of Epoch Learning Rate = 0.036057\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3474 - acc: 0.8714 - val_loss: 2.1373 - val_acc: 0.7293\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036057\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 2.1761 - acc: 0.7112\n",
      "\n",
      " End of Epoch Learning Rate = 0.035555\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3258 - acc: 0.8752 - val_loss: 2.1761 - val_acc: 0.7112\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035555\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.0911 - acc: 0.7265\n",
      "\n",
      " End of Epoch Learning Rate = 0.035054\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3189 - acc: 0.8774 - val_loss: 2.0911 - val_acc: 0.7265\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035054\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.1188 - acc: 0.7285\n",
      "\n",
      " End of Epoch Learning Rate = 0.034556\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.3042 - acc: 0.8799 - val_loss: 2.1188 - val_acc: 0.7285\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034556\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.0432 - acc: 0.7397\n",
      "\n",
      " End of Epoch Learning Rate = 0.034059\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.2914 - acc: 0.8809 - val_loss: 2.0432 - val_acc: 0.7397\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034059\n",
      "Epoch 182/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.0753 - acc: 0.7294\n",
      "\n",
      " End of Epoch Learning Rate = 0.033563\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2831 - acc: 0.8818 - val_loss: 2.0753 - val_acc: 0.7294\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033563\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 2.1244 - acc: 0.7255\n",
      "\n",
      " End of Epoch Learning Rate = 0.033070\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.2921 - acc: 0.8805 - val_loss: 2.1244 - val_acc: 0.7255\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033070\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.0272 - acc: 0.7443\n",
      "\n",
      " End of Epoch Learning Rate = 0.032578\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2674 - acc: 0.8846 - val_loss: 2.0272 - val_acc: 0.7443\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032578\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 2.0048 - acc: 0.7395\n",
      "\n",
      " End of Epoch Learning Rate = 0.032088\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2496 - acc: 0.8898 - val_loss: 2.0048 - val_acc: 0.7395\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032088\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.1381 - acc: 0.7359\n",
      "\n",
      " End of Epoch Learning Rate = 0.031601\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2486 - acc: 0.8872 - val_loss: 2.1381 - val_acc: 0.7359\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031601\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.1568 - acc: 0.7235\n",
      "\n",
      " End of Epoch Learning Rate = 0.031115\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.2382 - acc: 0.8884 - val_loss: 2.1568 - val_acc: 0.7235\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031115\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 2.1334 - acc: 0.7395\n",
      "\n",
      " End of Epoch Learning Rate = 0.030631\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2230 - acc: 0.8911 - val_loss: 2.1334 - val_acc: 0.7395\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030631\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.0769 - acc: 0.7250\n",
      "\n",
      " End of Epoch Learning Rate = 0.030150\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.2085 - acc: 0.8923 - val_loss: 2.0769 - val_acc: 0.7250\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030150\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.0686 - acc: 0.7369\n",
      "\n",
      " End of Epoch Learning Rate = 0.029670\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2103 - acc: 0.8913 - val_loss: 2.0686 - val_acc: 0.7369\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029670\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.1010 - acc: 0.7325\n",
      "\n",
      " End of Epoch Learning Rate = 0.029193\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1852 - acc: 0.8960 - val_loss: 2.1010 - val_acc: 0.7325\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029193\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.0980 - acc: 0.7354\n",
      "\n",
      " End of Epoch Learning Rate = 0.028718\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1845 - acc: 0.8947 - val_loss: 2.0980 - val_acc: 0.7354\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028718\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.1014 - acc: 0.7316\n",
      "\n",
      " End of Epoch Learning Rate = 0.028246\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.1750 - acc: 0.8980 - val_loss: 2.1014 - val_acc: 0.7316\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028246\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.9995 - acc: 0.7360\n",
      "\n",
      " End of Epoch Learning Rate = 0.027775\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1618 - acc: 0.8985 - val_loss: 1.9995 - val_acc: 0.7360\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027775\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.0246 - acc: 0.7333\n",
      "\n",
      " End of Epoch Learning Rate = 0.027308\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1611 - acc: 0.8988 - val_loss: 2.0246 - val_acc: 0.7333\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027308\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.0436 - acc: 0.7345\n",
      "\n",
      " End of Epoch Learning Rate = 0.026843\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.1501 - acc: 0.8990 - val_loss: 2.0436 - val_acc: 0.7345\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026843\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.9820 - acc: 0.7355\n",
      "\n",
      " End of Epoch Learning Rate = 0.026380\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.1418 - acc: 0.9001 - val_loss: 1.9820 - val_acc: 0.7355\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026380\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.0067 - acc: 0.7394\n",
      "\n",
      " End of Epoch Learning Rate = 0.025920\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1384 - acc: 0.9001 - val_loss: 2.0067 - val_acc: 0.7394\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025920\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.0123 - acc: 0.7357\n",
      "\n",
      " End of Epoch Learning Rate = 0.025462\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.1097 - acc: 0.9087 - val_loss: 2.0123 - val_acc: 0.7357\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025462\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.9793 - acc: 0.7500\n",
      "\n",
      " End of Epoch Learning Rate = 0.025007\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.1086 - acc: 0.9059 - val_loss: 1.9793 - val_acc: 0.7500\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025007\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.0354 - acc: 0.7370\n",
      "\n",
      " End of Epoch Learning Rate = 0.024555\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.0957 - acc: 0.9071 - val_loss: 2.0354 - val_acc: 0.7370\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024555\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.9814 - acc: 0.7377\n",
      "\n",
      " End of Epoch Learning Rate = 0.024106\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.0889 - acc: 0.9085 - val_loss: 1.9814 - val_acc: 0.7377\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024106\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.0099 - acc: 0.7400\n",
      "\n",
      " End of Epoch Learning Rate = 0.023660\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0774 - acc: 0.9090 - val_loss: 2.0099 - val_acc: 0.7400\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023660\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 2.0424 - acc: 0.7396\n",
      "\n",
      " End of Epoch Learning Rate = 0.023216\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 1.0670 - acc: 0.9114 - val_loss: 2.0424 - val_acc: 0.7396\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023216\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.9689 - acc: 0.7368\n",
      "\n",
      " End of Epoch Learning Rate = 0.022776\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0590 - acc: 0.9116 - val_loss: 1.9689 - val_acc: 0.7368\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022776\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.9787 - acc: 0.7465\n",
      "\n",
      " End of Epoch Learning Rate = 0.022338\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0494 - acc: 0.9131 - val_loss: 1.9787 - val_acc: 0.7465\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022338\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.0384 - acc: 0.7454\n",
      "\n",
      " End of Epoch Learning Rate = 0.021904\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0310 - acc: 0.9168 - val_loss: 2.0384 - val_acc: 0.7454\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021904\n",
      "Epoch 208/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 715us/sample - loss: 1.9171 - acc: 0.7446\n",
      "\n",
      " End of Epoch Learning Rate = 0.021472\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 1.0226 - acc: 0.9164 - val_loss: 1.9171 - val_acc: 0.7446\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021472\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.9777 - acc: 0.7490\n",
      "\n",
      " End of Epoch Learning Rate = 0.021044\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0130 - acc: 0.9187 - val_loss: 1.9777 - val_acc: 0.7490\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021044\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.0992 - acc: 0.7475\n",
      "\n",
      " End of Epoch Learning Rate = 0.020619\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.9979 - acc: 0.9211 - val_loss: 2.0992 - val_acc: 0.7475\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020619\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.9383 - acc: 0.7512\n",
      "\n",
      " End of Epoch Learning Rate = 0.020197\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9925 - acc: 0.9210 - val_loss: 1.9383 - val_acc: 0.7512\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020197\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 2.0016 - acc: 0.7330\n",
      "\n",
      " End of Epoch Learning Rate = 0.019778\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.9798 - acc: 0.9242 - val_loss: 2.0016 - val_acc: 0.7330\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019778\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 1.9626 - acc: 0.7357\n",
      "\n",
      " End of Epoch Learning Rate = 0.019363\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9780 - acc: 0.9210 - val_loss: 1.9626 - val_acc: 0.7357\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019363\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.0934 - acc: 0.7332\n",
      "\n",
      " End of Epoch Learning Rate = 0.018951\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9644 - acc: 0.9229 - val_loss: 2.0934 - val_acc: 0.7332\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018951\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 1.9658 - acc: 0.7478\n",
      "\n",
      " End of Epoch Learning Rate = 0.018542\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9564 - acc: 0.9252 - val_loss: 1.9658 - val_acc: 0.7478\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018542\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 2.0382 - acc: 0.7389\n",
      "\n",
      " End of Epoch Learning Rate = 0.018137\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.9476 - acc: 0.9259 - val_loss: 2.0382 - val_acc: 0.7389\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018137\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.9619 - acc: 0.7465\n",
      "\n",
      " End of Epoch Learning Rate = 0.017735\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9358 - acc: 0.9275 - val_loss: 1.9619 - val_acc: 0.7465\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017735\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 1.9838 - acc: 0.7487\n",
      "\n",
      " End of Epoch Learning Rate = 0.017337\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.9215 - acc: 0.9297 - val_loss: 1.9838 - val_acc: 0.7487\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017337\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 2.0167 - acc: 0.7451\n",
      "\n",
      " End of Epoch Learning Rate = 0.016943\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9120 - acc: 0.9306 - val_loss: 2.0167 - val_acc: 0.7451\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016943\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.8771 - acc: 0.7502\n",
      "\n",
      " End of Epoch Learning Rate = 0.016552\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.9014 - acc: 0.9318 - val_loss: 1.8771 - val_acc: 0.7502\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016552\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.9559 - acc: 0.7546\n",
      "\n",
      " End of Epoch Learning Rate = 0.016165\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8950 - acc: 0.9318 - val_loss: 1.9559 - val_acc: 0.7546\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016165\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.0084 - acc: 0.7453\n",
      "\n",
      " End of Epoch Learning Rate = 0.015781\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8858 - acc: 0.9332 - val_loss: 2.0084 - val_acc: 0.7453\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015781\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.0131 - acc: 0.7488\n",
      "\n",
      " End of Epoch Learning Rate = 0.015401\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8790 - acc: 0.9338 - val_loss: 2.0131 - val_acc: 0.7488\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015401\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 1.8482 - acc: 0.7567\n",
      "\n",
      " End of Epoch Learning Rate = 0.015025\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8651 - acc: 0.9364 - val_loss: 1.8482 - val_acc: 0.7567\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015025\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 7s 720us/sample - loss: 1.9003 - acc: 0.7600\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8589 - acc: 0.9364 - val_loss: 1.9003 - val_acc: 0.7600\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 1.9744 - acc: 0.7492\n",
      "\n",
      " End of Epoch Learning Rate = 0.014285\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8400 - acc: 0.9403 - val_loss: 1.9744 - val_acc: 0.7492\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014285\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 1.9712 - acc: 0.7464\n",
      "\n",
      " End of Epoch Learning Rate = 0.013921\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8292 - acc: 0.9424 - val_loss: 1.9712 - val_acc: 0.7464\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013921\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 2.1369 - acc: 0.7425\n",
      "\n",
      " End of Epoch Learning Rate = 0.013560\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8248 - acc: 0.9403 - val_loss: 2.1369 - val_acc: 0.7425\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013560\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 7s 712us/sample - loss: 1.9158 - acc: 0.7532\n",
      "\n",
      " End of Epoch Learning Rate = 0.013204\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8185 - acc: 0.9415 - val_loss: 1.9158 - val_acc: 0.7532\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013204\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 1.9185 - acc: 0.7566\n",
      "\n",
      " End of Epoch Learning Rate = 0.012851\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8055 - acc: 0.9424 - val_loss: 1.9185 - val_acc: 0.7566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012851\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.8629 - acc: 0.7651\n",
      "\n",
      " End of Epoch Learning Rate = 0.012503\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.8000 - acc: 0.9428 - val_loss: 1.8629 - val_acc: 0.7651\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012503\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.9776 - acc: 0.7520\n",
      "\n",
      " End of Epoch Learning Rate = 0.012159\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7877 - acc: 0.9447 - val_loss: 1.9776 - val_acc: 0.7520\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012159\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.9500 - acc: 0.7505\n",
      "\n",
      " End of Epoch Learning Rate = 0.011819\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7784 - acc: 0.9460 - val_loss: 1.9500 - val_acc: 0.7505\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011819\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.8512 - acc: 0.7579\n",
      "\n",
      " End of Epoch Learning Rate = 0.011483\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7616 - acc: 0.9486 - val_loss: 1.8512 - val_acc: 0.7579\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011483\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 2.0608 - acc: 0.7492\n",
      "\n",
      " End of Epoch Learning Rate = 0.011152\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7533 - acc: 0.9505 - val_loss: 2.0608 - val_acc: 0.7492\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011152\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 7s 723us/sample - loss: 2.0377 - acc: 0.7538\n",
      "\n",
      " End of Epoch Learning Rate = 0.010824\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.7490 - acc: 0.9495 - val_loss: 2.0377 - val_acc: 0.7538\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010824\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.0410 - acc: 0.7533\n",
      "\n",
      " End of Epoch Learning Rate = 0.010501\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7453 - acc: 0.9487 - val_loss: 2.0410 - val_acc: 0.7533\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010501\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.8624 - acc: 0.7500\n",
      "\n",
      " End of Epoch Learning Rate = 0.010182\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7328 - acc: 0.9517 - val_loss: 1.8624 - val_acc: 0.7500\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010182\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.9150 - acc: 0.7515\n",
      "\n",
      " End of Epoch Learning Rate = 0.009868\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7193 - acc: 0.9545 - val_loss: 1.9150 - val_acc: 0.7515\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009868\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 2.0279 - acc: 0.7558\n",
      "\n",
      " End of Epoch Learning Rate = 0.009558\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.7088 - acc: 0.9548 - val_loss: 2.0279 - val_acc: 0.7558\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009558\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.9305 - acc: 0.7594\n",
      "\n",
      " End of Epoch Learning Rate = 0.009253\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7008 - acc: 0.9557 - val_loss: 1.9305 - val_acc: 0.7594\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009253\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 1.8263 - acc: 0.7645\n",
      "\n",
      " End of Epoch Learning Rate = 0.008952\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6956 - acc: 0.9558 - val_loss: 1.8263 - val_acc: 0.7645\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008952\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 1.8613 - acc: 0.7598\n",
      "\n",
      " End of Epoch Learning Rate = 0.008655\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6835 - acc: 0.9573 - val_loss: 1.8613 - val_acc: 0.7598\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008655\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.0134 - acc: 0.7553\n",
      "\n",
      " End of Epoch Learning Rate = 0.008363\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6748 - acc: 0.9585 - val_loss: 2.0134 - val_acc: 0.7553\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008363\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.9817 - acc: 0.7579\n",
      "\n",
      " End of Epoch Learning Rate = 0.008076\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.6681 - acc: 0.9591 - val_loss: 1.9817 - val_acc: 0.7579\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008076\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.9754 - acc: 0.7581\n",
      "\n",
      " End of Epoch Learning Rate = 0.007793\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.6549 - acc: 0.9616 - val_loss: 1.9754 - val_acc: 0.7581\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007793\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 1.8435 - acc: 0.7663\n",
      "\n",
      " End of Epoch Learning Rate = 0.007515\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6552 - acc: 0.9600 - val_loss: 1.8435 - val_acc: 0.7663\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007515\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 1.9323 - acc: 0.7615\n",
      "\n",
      " End of Epoch Learning Rate = 0.007241\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6476 - acc: 0.9605 - val_loss: 1.9323 - val_acc: 0.7615\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007241\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.8742 - acc: 0.7611\n",
      "\n",
      " End of Epoch Learning Rate = 0.006972\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6358 - acc: 0.9629 - val_loss: 1.8742 - val_acc: 0.7611\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006972\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 1.8097 - acc: 0.7635\n",
      "\n",
      " End of Epoch Learning Rate = 0.006708\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.6257 - acc: 0.9635 - val_loss: 1.8097 - val_acc: 0.7635\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006708\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.8632 - acc: 0.7610\n",
      "\n",
      " End of Epoch Learning Rate = 0.006449\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.6157 - acc: 0.9660 - val_loss: 1.8632 - val_acc: 0.7610\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006449\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.8844 - acc: 0.7651\n",
      "\n",
      " End of Epoch Learning Rate = 0.006194\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.6100 - acc: 0.9661 - val_loss: 1.8844 - val_acc: 0.7651\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006194\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.7847 - acc: 0.7724\n",
      "\n",
      " End of Epoch Learning Rate = 0.005944\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.6087 - acc: 0.9661 - val_loss: 1.7847 - val_acc: 0.7724\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005944\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 1.8115 - acc: 0.7675\n",
      "\n",
      " End of Epoch Learning Rate = 0.005699\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5973 - acc: 0.9670 - val_loss: 1.8115 - val_acc: 0.7675\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005699\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.9709 - acc: 0.7665\n",
      "\n",
      " End of Epoch Learning Rate = 0.005459\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5877 - acc: 0.9680 - val_loss: 1.9709 - val_acc: 0.7665\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005459\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.7899 - acc: 0.7686\n",
      "\n",
      " End of Epoch Learning Rate = 0.005224\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5857 - acc: 0.9680 - val_loss: 1.7899 - val_acc: 0.7686\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005224\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.8091 - acc: 0.7738\n",
      "\n",
      " End of Epoch Learning Rate = 0.004994\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.5762 - acc: 0.9702 - val_loss: 1.8091 - val_acc: 0.7738\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004994\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.8109 - acc: 0.7696\n",
      "\n",
      " End of Epoch Learning Rate = 0.004768\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5682 - acc: 0.9704 - val_loss: 1.8109 - val_acc: 0.7696\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004768\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.8416 - acc: 0.7702\n",
      "\n",
      " End of Epoch Learning Rate = 0.004548\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5612 - acc: 0.9718 - val_loss: 1.8416 - val_acc: 0.7702\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004548\n",
      "Epoch 260/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.7970 - acc: 0.7734\n",
      "\n",
      " End of Epoch Learning Rate = 0.004332\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5557 - acc: 0.9732 - val_loss: 1.7970 - val_acc: 0.7734\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004332\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.8567 - acc: 0.7739\n",
      "\n",
      " End of Epoch Learning Rate = 0.004122\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5495 - acc: 0.9733 - val_loss: 1.8567 - val_acc: 0.7739\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004122\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 1.8072 - acc: 0.7824\n",
      "\n",
      " End of Epoch Learning Rate = 0.003916\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5446 - acc: 0.9736 - val_loss: 1.8072 - val_acc: 0.7824\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003916\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.8620 - acc: 0.7778\n",
      "\n",
      " End of Epoch Learning Rate = 0.003716\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.5377 - acc: 0.9743 - val_loss: 1.8620 - val_acc: 0.7778\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003716\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.7956 - acc: 0.7794\n",
      "\n",
      " End of Epoch Learning Rate = 0.003521\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5303 - acc: 0.9753 - val_loss: 1.7956 - val_acc: 0.7794\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003521\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 1.8848 - acc: 0.7791\n",
      "\n",
      " End of Epoch Learning Rate = 0.003331\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5233 - acc: 0.9773 - val_loss: 1.8848 - val_acc: 0.7791\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003331\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.8455 - acc: 0.7696\n",
      "\n",
      " End of Epoch Learning Rate = 0.003146\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.5194 - acc: 0.9765 - val_loss: 1.8455 - val_acc: 0.7696\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003146\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 7s 699us/sample - loss: 1.8394 - acc: 0.7854\n",
      "\n",
      " End of Epoch Learning Rate = 0.002966\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5154 - acc: 0.9771 - val_loss: 1.8394 - val_acc: 0.7854\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002966\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 1.8549 - acc: 0.7823\n",
      "\n",
      " End of Epoch Learning Rate = 0.002791\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.5091 - acc: 0.9783 - val_loss: 1.8549 - val_acc: 0.7823\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002791\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 1.8343 - acc: 0.7802\n",
      "\n",
      " End of Epoch Learning Rate = 0.002621\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.5029 - acc: 0.9794 - val_loss: 1.8343 - val_acc: 0.7802\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002621\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.8307 - acc: 0.7836\n",
      "\n",
      " End of Epoch Learning Rate = 0.002457\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4987 - acc: 0.9793 - val_loss: 1.8307 - val_acc: 0.7836\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002457\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 7s 715us/sample - loss: 1.8198 - acc: 0.7805\n",
      "\n",
      " End of Epoch Learning Rate = 0.002298\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4968 - acc: 0.9796 - val_loss: 1.8198 - val_acc: 0.7805\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002298\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.7862 - acc: 0.7779\n",
      "\n",
      " End of Epoch Learning Rate = 0.002144\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4924 - acc: 0.9801 - val_loss: 1.7862 - val_acc: 0.7779\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002144\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.8436 - acc: 0.7819\n",
      "\n",
      " End of Epoch Learning Rate = 0.001995\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4865 - acc: 0.9818 - val_loss: 1.8436 - val_acc: 0.7819\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001995\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 7s 705us/sample - loss: 1.8418 - acc: 0.7817\n",
      "\n",
      " End of Epoch Learning Rate = 0.001852\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4796 - acc: 0.9828 - val_loss: 1.8418 - val_acc: 0.7817\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001852\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.7855 - acc: 0.7834\n",
      "\n",
      " End of Epoch Learning Rate = 0.001714\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4816 - acc: 0.9809 - val_loss: 1.7855 - val_acc: 0.7834\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001714\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 1.7814 - acc: 0.7892\n",
      "\n",
      " End of Epoch Learning Rate = 0.001581\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4757 - acc: 0.9820 - val_loss: 1.7814 - val_acc: 0.7892\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001581\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.8684 - acc: 0.7833\n",
      "\n",
      " End of Epoch Learning Rate = 0.001453\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4744 - acc: 0.9821 - val_loss: 1.8684 - val_acc: 0.7833\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001453\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.7846 - acc: 0.7871\n",
      "\n",
      " End of Epoch Learning Rate = 0.001331\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4705 - acc: 0.9827 - val_loss: 1.7846 - val_acc: 0.7871\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001331\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 1.8082 - acc: 0.7848\n",
      "\n",
      " End of Epoch Learning Rate = 0.001214\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4654 - acc: 0.9845 - val_loss: 1.8082 - val_acc: 0.7848\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001214\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 1.7914 - acc: 0.7891\n",
      "\n",
      " End of Epoch Learning Rate = 0.001103\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4660 - acc: 0.9836 - val_loss: 1.7914 - val_acc: 0.7891\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001103\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 1.8497 - acc: 0.7867\n",
      "\n",
      " End of Epoch Learning Rate = 0.000996\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4618 - acc: 0.9846 - val_loss: 1.8497 - val_acc: 0.7867\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000996\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 1.8162 - acc: 0.7868\n",
      "\n",
      " End of Epoch Learning Rate = 0.000896\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4594 - acc: 0.9853 - val_loss: 1.8162 - val_acc: 0.7868\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000896\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 1.7746 - acc: 0.7905\n",
      "\n",
      " End of Epoch Learning Rate = 0.000800\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4560 - acc: 0.9861 - val_loss: 1.7746 - val_acc: 0.7905\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000800\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.8087 - acc: 0.7859\n",
      "\n",
      " End of Epoch Learning Rate = 0.000710\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4551 - acc: 0.9857 - val_loss: 1.8087 - val_acc: 0.7859\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000710\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.8064 - acc: 0.7892\n",
      "\n",
      " End of Epoch Learning Rate = 0.000626\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4515 - acc: 0.9867 - val_loss: 1.8064 - val_acc: 0.7892\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000626\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.7990 - acc: 0.7908\n",
      "\n",
      " End of Epoch Learning Rate = 0.000546\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.4542 - acc: 0.9858 - val_loss: 1.7990 - val_acc: 0.7908\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000546\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 7s 701us/sample - loss: 1.7986 - acc: 0.7898\n",
      "\n",
      " End of Epoch Learning Rate = 0.000473\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.4519 - acc: 0.9859 - val_loss: 1.7986 - val_acc: 0.7898\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000473\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 7s 704us/sample - loss: 1.7816 - acc: 0.7893\n",
      "\n",
      " End of Epoch Learning Rate = 0.000404\n",
      "400/400 [==============================] - 116s 289ms/step - loss: 0.4491 - acc: 0.9863 - val_loss: 1.7816 - val_acc: 0.7893\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000404\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 7s 718us/sample - loss: 1.8380 - acc: 0.7892\n",
      "\n",
      " End of Epoch Learning Rate = 0.000341\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4490 - acc: 0.9867 - val_loss: 1.8380 - val_acc: 0.7892\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000341\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 1.7849 - acc: 0.7904\n",
      "\n",
      " End of Epoch Learning Rate = 0.000284\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4455 - acc: 0.9876 - val_loss: 1.7849 - val_acc: 0.7904\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000284\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.7832 - acc: 0.7907\n",
      "\n",
      " End of Epoch Learning Rate = 0.000232\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4424 - acc: 0.9882 - val_loss: 1.7832 - val_acc: 0.7907\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000232\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.8112 - acc: 0.7895\n",
      "\n",
      " End of Epoch Learning Rate = 0.000185\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4453 - acc: 0.9870 - val_loss: 1.8112 - val_acc: 0.7895\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000185\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 1.7996 - acc: 0.7919\n",
      "\n",
      " End of Epoch Learning Rate = 0.000144\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4437 - acc: 0.9875 - val_loss: 1.7996 - val_acc: 0.7919\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000144\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 7s 706us/sample - loss: 1.8096 - acc: 0.7913\n",
      "\n",
      " End of Epoch Learning Rate = 0.000109\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4420 - acc: 0.9882 - val_loss: 1.8096 - val_acc: 0.7913\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000109\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.7950 - acc: 0.7923\n",
      "\n",
      " End of Epoch Learning Rate = 0.000079\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.4417 - acc: 0.9883 - val_loss: 1.7950 - val_acc: 0.7923\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000079\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 1.7933 - acc: 0.7930\n",
      "\n",
      " End of Epoch Learning Rate = 0.000054\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4427 - acc: 0.9879 - val_loss: 1.7933 - val_acc: 0.7930\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000054\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.7971 - acc: 0.7940\n",
      "\n",
      " End of Epoch Learning Rate = 0.000035\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4416 - acc: 0.9887 - val_loss: 1.7971 - val_acc: 0.7940\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000035\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 1.7921 - acc: 0.7939\n",
      "\n",
      " End of Epoch Learning Rate = 0.000021\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4408 - acc: 0.9880 - val_loss: 1.7921 - val_acc: 0.7939\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000021\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 1.7956 - acc: 0.7938\n",
      "\n",
      " End of Epoch Learning Rate = 0.000013\n",
      "400/400 [==============================] - 116s 290ms/step - loss: 0.4397 - acc: 0.9891 - val_loss: 1.7956 - val_acc: 0.7938\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000013\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 1.7939 - acc: 0.7937\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "400/400 [==============================] - 116s 291ms/step - loss: 0.4435 - acc: 0.9876 - val_loss: 1.7939 - val_acc: 0.7937\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(dataGenerator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch =steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (%): 79.37\n"
     ]
    }
   ],
   "source": [
    "#get final performance\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print('Test accuracy (%):', 100*sum(np.argmax(y_pred,-1)==np.argmax(y_test,-1))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss and accuracy\n",
    "plot_history(model.history.history)\n",
    "\n",
    "#plot learning rate schedule\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.arange(0,len(lr_scheduler.lr_used))/steps_per_epoch,lr_scheduler.lr_used)\n",
    "plt.xlabel('epoch number')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weigts used for updating\n",
    "model.save_weights(ModelsPath+'Final_weights_'+WhichDataSet+'_32bit_model_sReLU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
